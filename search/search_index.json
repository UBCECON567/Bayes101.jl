{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bayes101.jl \u00b6 These notes and examples are meant to accompany the slides at https://faculty.arts.ubc.ca/pschrimpf/565/565.html MCMC sampling methods: ols.jmd Quasi-Bayesian: quasi.jmd","title":"Home"},{"location":"#bayes101jl","text":"These notes and examples are meant to accompany the slides at https://faculty.arts.ubc.ca/pschrimpf/565/565.html MCMC sampling methods: ols.jmd Quasi-Bayesian: quasi.jmd <!--` \u2013>","title":"Bayes101.jl"},{"location":"license/","text":"These notes are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf. BibTeX citation.","title":"License"},{"location":"ols/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License using Plots, Distributions, StatsPlots, LinearAlgebra Plots.pyplot() Plots.PyPlotBackend() Linear Regression \u00b6 Let\u2019s simulate a simple linear regression, and then sample from the posterior using a few methods. The model is y_i = x_i \\beta + \\epsilon_i with $\\epsilon_i \\sim N(0, \\sigma^2)$ Simulation \u00b6 function simulateOLS(;N::Integer = Int(1e4), \u03b2 = ones(4), \u03c3 = 1) x = randn(N,length(\u03b2)) y = x*\u03b2 + randn(N)*\u03c3 return(x, y) end x, y = simulateOLS(N=Int(1e2), \u03b2=ones(2), \u03c3=1); Computing Posteriors \u00b6 To calculate the posterior distribution of $\\beta$ given the $x$ and $y$, we use a conditional version of Bayes\u2019 rule. p(\\beta|x, y, \\sigma) = \\frac{p(y|x, \\beta, \\sigma) p(\\beta|\\sigma, x) } {p(y |x,\\sigma)} To keep the calculations as simple as possible, we will treat $\\sigma$ as known for now. Since a regression model is agnostic about the distribution of $x$, it is standard to assume that p(\\beta|\\sigma, x) = p(\\beta|\\sigma) In other words, the distribution of $\\beta$ is independent of $x$. With normal errors, the log likelihood, i.e. the log conditional density of $y$ given the parameters , or, \\log(p(y|x, \\beta,\\sigma)) is: function loglike(\u03b2, \u03c3, x, y) return(-length(y)/2*log(2\u03c0) - 1/2*sum(((y .- x*\u03b2)./\u03c3).^2)) end loglike (generic function with 1 method) Exact Posterior \u00b6 If we assume that the prior for $\\beta$ is normal, \\beta|\\sigma \\sim N(0, \\tau^2 I_k) then the posterior can be computed exactly. It is: \\beta|x, y, \\sigma \\sim N\\left( (x'x/\\sigma^2 + I/\\tau^2)^{-1} x'y/\\sigma^2, (x'x/\\sigma^2 + I/\\tau^2)^{-1}\\right) function posterior(\u03b2, \u03c3, x, y, \u03c4) \u03a3 = inv(x'*x/\u03c3^2 + I/\u03c4^2) \u03bc = \u03a3*x'*y/\u03c3^2 return(pdf(MvNormal(\u03bc, \u03a3), \u03b2)) end posterior (generic function with 1 method) Metropolis Hastings \u00b6 If we didn\u2019t know the exact posterior, we could sample from it in a number of ways. Metropolis Hastings is a simple general purpose method. function mhstep(\u03b8, likelihood::Function, prior::Function, proposal::Function) pold = likelihood(\u03b8)*prior(\u03b8) \u03b8new = rand(proposal(\u03b8)) pnew = likelihood(\u03b8new)*prior(\u03b8new) \u03b1 = pnew*pdf(proposal(\u03b8new), \u03b8)/(pold *pdf(proposal(\u03b8), \u03b8new)) u = rand() return( (u < \u03b1) ? \u03b8new : \u03b8) end function mhchain(\u03b80, iterations, likelihood, prior, proposal) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b8[:,i] .= mhstep(\u03b8[:,i-1], likelihood, prior, proposal) end return(\u03b8) end mhchain (generic function with 1 method) When using Metropolis-Hastings, the proposal density plays a key role. The close the proposal is to the posterior, the faster the chain converges. One common choice of proposal distribution is the random walk\u2013draw candidate parameters from $N(\\theta, s)$. Let\u2019s try it. \u03c4 = 10.0 k = size(x,2) \u03c3 = 1.0 for s in [0.01, 0.1, 1.0, 10.0] rw\u03b2 = mhchain(zeros(k), Int(1e5), \u03b2->exp(loglike(\u03b2, \u03c3, x, y)), \u03b2->pdf(MvNormal(zeros(k), \u03c4), \u03b2), \u03b2->MvNormal(\u03b2, s*I)) lims=(0.5, 1.5) h=histogram(rw\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(rw\u03b2[1,:], rw\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Random Walk with s=$s\"), h2, plot(rw\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(rw\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) end Another possible choice of proposal density is an independent \u2014 draw candidate parameters from some fixed distribution. Gibbs Sampling \u00b6 Gibbs sampling refers to when we sample part of our parameters conditional on others, then vice versa, and repeat. Generally, the more parameters than can be sampled at once, the better the chain will work. As a simple illustration, we can sample each of our regression coefficients conditional on the others. function gibbschain(\u03b80, iterations, x, y, \u03c3, \u03c4) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b2 = \u03b8[:,i-1] @views for j in 1:length(\u03b2) e = y .- x[:,1:end .!= j]*\u03b2[1:end .!= j] v = 1/(dot(x[:,j], x[:,j])/\u03c3^2 + 1/\u03c4^2) m = v * (dot(x[:,j],e)/\u03c3^2) \u03b2[j] = rand(Normal(m,sqrt(v))) end \u03b8[:,i] = \u03b2 end return(\u03b8) end g\u03b2 = gibbschain(zeros(2), Int(1e5), x, y, \u03c3, \u03c4) lims=(0.5, 1.5) h=histogram(g\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(g\u03b2[1,:], g\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Gibbs Sampling\"), h2, plot(g\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(g\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) Gibbs sampling is a particularly good idea when the parameters can be divided into groups such that posterior of parameters in each group conditional on the others has a known form. If some parameters do not have known posterior, then another sampling strategy within the Gibbs sampler can be used (like metropolis-hastings). With judicious choice of blocks and conjugate priors, Gibbs sampling can be applied to very complex models. OpenBUGS, WinBUGS and JAGS are (non-Julia) software packages designed around Gibbs sampling. Mamba.jl is a Julia package that could be used for Gibbs sampling (as well as other sampling methods). Hamiltonian Monte Carlo \u00b6 Recent efforts in Bayesian computation have shifted away from Gibbs sampling of blocks with closed form posteriors towards more general purpose methods. Metropolis-Hastings is a general purpose algorithm in that all you need to use it is to be able to compute the likelihood (up to a constant of proportionality) and the prior. Unfortunately, Metropolis-Hastings requires a good proposal distribution to work well, and it is hard to find a good proposal distribution in many situations. Hamiltonian Monte Carlo uses information about the gradient of the posterior to generate a good proposal distribution. The physical analogy here is that the log posterior acts like a gravitational field, and we generate samples by creating random trajectories through that field. With the right choice of trajectories (i.e. not too fast & not too slow), they will tend to concentrate in areas of high posterior probability, while also exploring the space of parameters well. using Zygote, ForwardDiff function hmcstep(\u03b8, logdens, \u03f5=0.1, L=5; \u2207logdens=\u03b8->Zygote.gradient(logdens, \u03b8)[1], M = I, iM=inv(M)) U(x) = -logdens(x) dU(x) = -\u2207logdens(x) K(m) = (m'*iM*m/2)[1] dK(m) = m'*iM m = rand(MvNormal(zeros(length(\u03b8)), M),1) \u03b8new = copy(\u03b8) mnew = copy(m) for s in 1:L mnew .+= -dU(\u03b8new)[:]*\u03f5/2 \u03b8new .+= dK(mnew)[:]*\u03f5 end \u03b1 = exp(-U(\u03b8new)+U(\u03b8)-K(mnew)+K(m)) u = rand() return(u < \u03b1 ? \u03b8new : \u03b8) end function hmcchain(\u03b80, iterations, logdens, \u2207logdens=\u03b8->Zygote.gradient(logdens, \u03b8)[1] ; \u03f5=0.1, L=5, M=I) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b8[:,i] .= hmcstep(\u03b8[:,i-1], logdens, \u03f5, L, M=M) end return(\u03b8) end logp = \u03b2->(loglike(\u03b2, \u03c3, x, y) + logpdf(MvNormal(zeros(k), \u03c4), \u03b2)) \u2207logp(\u03b2) = ForwardDiff.gradient(logp, \u03b2) for \u03f5 in [1e-3, 0.1] for L in [2, 10] @show \u03f5, L @time h\u03b2 = hmcchain(zeros(k), Int(1e4),logp, \u03f5=\u03f5, L=L) lims=(0.5, 1.5) h=histogram(h\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(h\u03b2[1,:], h\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"HMC with \u03f5=$\u03f5, L=$L\"), h2, plot(h\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(h\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) end end (\u03f5, L) = (0.001, 2) 2.025082 seconds (16.48 M allocations: 822.892 MiB, 7.75% gc time) (\u03f5, L) = (0.001, 10) 7.481677 seconds (76.23 M allocations: 3.599 GiB, 9.54% gc time) (\u03f5, L) = (0.1, 2) 1.596762 seconds (15.60 M allocations: 779.647 MiB, 9.47% gc time) (\u03f5, L) = (0.1, 10) 7.512895 seconds (76.23 M allocations: 3.599 GiB, 9.16% gc time) The step size, $\\epsilon$, and the number of steps, $L$, affect how well the chain performs. If $\\epsilon$ or $L$ is too large, then many proposals will get rejected. If $\\epsilon L$ is too small, then the draws will be very close together, and it will take more iterations of the chain to explore the space. NUTS \u00b6 The No-U-Turn-Sample (NUTS) ((Hoffman and Gelman 2014 )) algorithmically chooses $L$, $\\epsilon$, and $M$. Stan is (non-Julia, but callable from Julia) software for Bayesian modelling that largely uses NUTS for sampling. DynamicHMC.jl is a Julia implementation. Let\u2019s try it out. using DynamicHMC, Random, LogDensityProblems struct LinearRegressionKnownSigma{T} y::Vector{T} x::Matrix{T} \u03c3::T \u03c4::T end function LogDensityProblems.capabilities(::Type{<:LinearRegressionKnownSigma}) LogDensityProblems.LogDensityOrder{0}() end LogDensityProblems.dimension(lr::LinearRegressionKnownSigma) = size(lr.x,2) function LogDensityProblems.logdensity(lr::LinearRegressionKnownSigma, \u03b2) k = size(x,2) return(logpdf(MvNormal(zeros(k), lr.\u03c4), \u03b2) + loglikelihood(Normal(0, \u03c3), y - x*\u03b2)) end lr = LinearRegressionKnownSigma(y,x,\u03c3,\u03c4) \u2207logposterior=ADgradient(:Zygote, lr) @time results=DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, \u2207logposterior, 1000; warmup_stages= default_warmup_stages(init_steps=50, middle_steps=100, doubling_stages=2)); 11.208851 seconds (60.46 M allocations: 2.138 GiB, 4.90% gc time) dh\u03b2 = hcat(results.inference.chain...) h=histogram(dh\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(dh\u03b2[1,:], dh\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"DynamicHMC (NUTS)\"), h2, plot(dh\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(dh\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) See this example for a more canonical DynamicHMC linear regression implementation. Turing.jl \u00b6 Turing.jl is a probabilistic programming language (similar to STAN or BUGS), written in Julia. using Turing @model lreg(x, y, \u03c3, \u03c4) = begin # prior \u03b2 ~ MvNormal(zeros(size(x,2)), \u03c4) # model y ~ MvNormal(x*\u03b2,\u03c3) end lr = lreg(x,y,\u03c3,\u03c4) @time cc = Turing.sample(lr, NUTS(0.65), 2000) 9.958946 seconds (6.15 M allocations: 379.778 MiB, 1.15% gc time) t\u03b2 = cc.value[:,[\"\u03b2[1]\",\"\u03b2[2]\"],1]' h=histogram(t\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(t\u03b2[1,:], t\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Turing, NUTS\"), h2, plot(t\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(t\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) About \u00b6 This meant to accompany these slides on Bayesian methods in IO. This document was created using Weave.jl ((Pastell 2017 )). The code is available in on github . Hoffman, Matthew D, and Andrew Gelman. 2014. \u201cThe No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.\u201d Journal of Machine Learning Research 15 (1): 1593\u20131623. http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf . Pastell, Matti. 2017. \u201cWeave.jl: Scientific Reports Using Julia.\u201d The Journal of Open Source Software . https://doi.org/http://dx.doi.org/10.21105/joss.00204 .","title":"MCMC sampling methods"},{"location":"ols/#linear-regression","text":"Let\u2019s simulate a simple linear regression, and then sample from the posterior using a few methods. The model is y_i = x_i \\beta + \\epsilon_i with $\\epsilon_i \\sim N(0, \\sigma^2)$","title":"Linear Regression"},{"location":"ols/#simulation","text":"function simulateOLS(;N::Integer = Int(1e4), \u03b2 = ones(4), \u03c3 = 1) x = randn(N,length(\u03b2)) y = x*\u03b2 + randn(N)*\u03c3 return(x, y) end x, y = simulateOLS(N=Int(1e2), \u03b2=ones(2), \u03c3=1);","title":"Simulation"},{"location":"ols/#computing-posteriors","text":"To calculate the posterior distribution of $\\beta$ given the $x$ and $y$, we use a conditional version of Bayes\u2019 rule. p(\\beta|x, y, \\sigma) = \\frac{p(y|x, \\beta, \\sigma) p(\\beta|\\sigma, x) } {p(y |x,\\sigma)} To keep the calculations as simple as possible, we will treat $\\sigma$ as known for now. Since a regression model is agnostic about the distribution of $x$, it is standard to assume that p(\\beta|\\sigma, x) = p(\\beta|\\sigma) In other words, the distribution of $\\beta$ is independent of $x$. With normal errors, the log likelihood, i.e. the log conditional density of $y$ given the parameters , or, \\log(p(y|x, \\beta,\\sigma)) is: function loglike(\u03b2, \u03c3, x, y) return(-length(y)/2*log(2\u03c0) - 1/2*sum(((y .- x*\u03b2)./\u03c3).^2)) end loglike (generic function with 1 method)","title":"Computing Posteriors"},{"location":"ols/#exact-posterior","text":"If we assume that the prior for $\\beta$ is normal, \\beta|\\sigma \\sim N(0, \\tau^2 I_k) then the posterior can be computed exactly. It is: \\beta|x, y, \\sigma \\sim N\\left( (x'x/\\sigma^2 + I/\\tau^2)^{-1} x'y/\\sigma^2, (x'x/\\sigma^2 + I/\\tau^2)^{-1}\\right) function posterior(\u03b2, \u03c3, x, y, \u03c4) \u03a3 = inv(x'*x/\u03c3^2 + I/\u03c4^2) \u03bc = \u03a3*x'*y/\u03c3^2 return(pdf(MvNormal(\u03bc, \u03a3), \u03b2)) end posterior (generic function with 1 method)","title":"Exact Posterior"},{"location":"ols/#metropolis-hastings","text":"If we didn\u2019t know the exact posterior, we could sample from it in a number of ways. Metropolis Hastings is a simple general purpose method. function mhstep(\u03b8, likelihood::Function, prior::Function, proposal::Function) pold = likelihood(\u03b8)*prior(\u03b8) \u03b8new = rand(proposal(\u03b8)) pnew = likelihood(\u03b8new)*prior(\u03b8new) \u03b1 = pnew*pdf(proposal(\u03b8new), \u03b8)/(pold *pdf(proposal(\u03b8), \u03b8new)) u = rand() return( (u < \u03b1) ? \u03b8new : \u03b8) end function mhchain(\u03b80, iterations, likelihood, prior, proposal) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b8[:,i] .= mhstep(\u03b8[:,i-1], likelihood, prior, proposal) end return(\u03b8) end mhchain (generic function with 1 method) When using Metropolis-Hastings, the proposal density plays a key role. The close the proposal is to the posterior, the faster the chain converges. One common choice of proposal distribution is the random walk\u2013draw candidate parameters from $N(\\theta, s)$. Let\u2019s try it. \u03c4 = 10.0 k = size(x,2) \u03c3 = 1.0 for s in [0.01, 0.1, 1.0, 10.0] rw\u03b2 = mhchain(zeros(k), Int(1e5), \u03b2->exp(loglike(\u03b2, \u03c3, x, y)), \u03b2->pdf(MvNormal(zeros(k), \u03c4), \u03b2), \u03b2->MvNormal(\u03b2, s*I)) lims=(0.5, 1.5) h=histogram(rw\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(rw\u03b2[1,:], rw\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Random Walk with s=$s\"), h2, plot(rw\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(rw\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) end Another possible choice of proposal density is an independent \u2014 draw candidate parameters from some fixed distribution.","title":"Metropolis Hastings"},{"location":"ols/#gibbs-sampling","text":"Gibbs sampling refers to when we sample part of our parameters conditional on others, then vice versa, and repeat. Generally, the more parameters than can be sampled at once, the better the chain will work. As a simple illustration, we can sample each of our regression coefficients conditional on the others. function gibbschain(\u03b80, iterations, x, y, \u03c3, \u03c4) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b2 = \u03b8[:,i-1] @views for j in 1:length(\u03b2) e = y .- x[:,1:end .!= j]*\u03b2[1:end .!= j] v = 1/(dot(x[:,j], x[:,j])/\u03c3^2 + 1/\u03c4^2) m = v * (dot(x[:,j],e)/\u03c3^2) \u03b2[j] = rand(Normal(m,sqrt(v))) end \u03b8[:,i] = \u03b2 end return(\u03b8) end g\u03b2 = gibbschain(zeros(2), Int(1e5), x, y, \u03c3, \u03c4) lims=(0.5, 1.5) h=histogram(g\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(g\u03b2[1,:], g\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Gibbs Sampling\"), h2, plot(g\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(g\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) Gibbs sampling is a particularly good idea when the parameters can be divided into groups such that posterior of parameters in each group conditional on the others has a known form. If some parameters do not have known posterior, then another sampling strategy within the Gibbs sampler can be used (like metropolis-hastings). With judicious choice of blocks and conjugate priors, Gibbs sampling can be applied to very complex models. OpenBUGS, WinBUGS and JAGS are (non-Julia) software packages designed around Gibbs sampling. Mamba.jl is a Julia package that could be used for Gibbs sampling (as well as other sampling methods).","title":"Gibbs Sampling"},{"location":"ols/#hamiltonian-monte-carlo","text":"Recent efforts in Bayesian computation have shifted away from Gibbs sampling of blocks with closed form posteriors towards more general purpose methods. Metropolis-Hastings is a general purpose algorithm in that all you need to use it is to be able to compute the likelihood (up to a constant of proportionality) and the prior. Unfortunately, Metropolis-Hastings requires a good proposal distribution to work well, and it is hard to find a good proposal distribution in many situations. Hamiltonian Monte Carlo uses information about the gradient of the posterior to generate a good proposal distribution. The physical analogy here is that the log posterior acts like a gravitational field, and we generate samples by creating random trajectories through that field. With the right choice of trajectories (i.e. not too fast & not too slow), they will tend to concentrate in areas of high posterior probability, while also exploring the space of parameters well. using Zygote, ForwardDiff function hmcstep(\u03b8, logdens, \u03f5=0.1, L=5; \u2207logdens=\u03b8->Zygote.gradient(logdens, \u03b8)[1], M = I, iM=inv(M)) U(x) = -logdens(x) dU(x) = -\u2207logdens(x) K(m) = (m'*iM*m/2)[1] dK(m) = m'*iM m = rand(MvNormal(zeros(length(\u03b8)), M),1) \u03b8new = copy(\u03b8) mnew = copy(m) for s in 1:L mnew .+= -dU(\u03b8new)[:]*\u03f5/2 \u03b8new .+= dK(mnew)[:]*\u03f5 end \u03b1 = exp(-U(\u03b8new)+U(\u03b8)-K(mnew)+K(m)) u = rand() return(u < \u03b1 ? \u03b8new : \u03b8) end function hmcchain(\u03b80, iterations, logdens, \u2207logdens=\u03b8->Zygote.gradient(logdens, \u03b8)[1] ; \u03f5=0.1, L=5, M=I) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b8[:,i] .= hmcstep(\u03b8[:,i-1], logdens, \u03f5, L, M=M) end return(\u03b8) end logp = \u03b2->(loglike(\u03b2, \u03c3, x, y) + logpdf(MvNormal(zeros(k), \u03c4), \u03b2)) \u2207logp(\u03b2) = ForwardDiff.gradient(logp, \u03b2) for \u03f5 in [1e-3, 0.1] for L in [2, 10] @show \u03f5, L @time h\u03b2 = hmcchain(zeros(k), Int(1e4),logp, \u03f5=\u03f5, L=L) lims=(0.5, 1.5) h=histogram(h\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(h\u03b2[1,:], h\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"HMC with \u03f5=$\u03f5, L=$L\"), h2, plot(h\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(h\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) end end (\u03f5, L) = (0.001, 2) 2.025082 seconds (16.48 M allocations: 822.892 MiB, 7.75% gc time) (\u03f5, L) = (0.001, 10) 7.481677 seconds (76.23 M allocations: 3.599 GiB, 9.54% gc time) (\u03f5, L) = (0.1, 2) 1.596762 seconds (15.60 M allocations: 779.647 MiB, 9.47% gc time) (\u03f5, L) = (0.1, 10) 7.512895 seconds (76.23 M allocations: 3.599 GiB, 9.16% gc time) The step size, $\\epsilon$, and the number of steps, $L$, affect how well the chain performs. If $\\epsilon$ or $L$ is too large, then many proposals will get rejected. If $\\epsilon L$ is too small, then the draws will be very close together, and it will take more iterations of the chain to explore the space.","title":"Hamiltonian Monte Carlo"},{"location":"ols/#nuts","text":"The No-U-Turn-Sample (NUTS) ((Hoffman and Gelman 2014 )) algorithmically chooses $L$, $\\epsilon$, and $M$. Stan is (non-Julia, but callable from Julia) software for Bayesian modelling that largely uses NUTS for sampling. DynamicHMC.jl is a Julia implementation. Let\u2019s try it out. using DynamicHMC, Random, LogDensityProblems struct LinearRegressionKnownSigma{T} y::Vector{T} x::Matrix{T} \u03c3::T \u03c4::T end function LogDensityProblems.capabilities(::Type{<:LinearRegressionKnownSigma}) LogDensityProblems.LogDensityOrder{0}() end LogDensityProblems.dimension(lr::LinearRegressionKnownSigma) = size(lr.x,2) function LogDensityProblems.logdensity(lr::LinearRegressionKnownSigma, \u03b2) k = size(x,2) return(logpdf(MvNormal(zeros(k), lr.\u03c4), \u03b2) + loglikelihood(Normal(0, \u03c3), y - x*\u03b2)) end lr = LinearRegressionKnownSigma(y,x,\u03c3,\u03c4) \u2207logposterior=ADgradient(:Zygote, lr) @time results=DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, \u2207logposterior, 1000; warmup_stages= default_warmup_stages(init_steps=50, middle_steps=100, doubling_stages=2)); 11.208851 seconds (60.46 M allocations: 2.138 GiB, 4.90% gc time) dh\u03b2 = hcat(results.inference.chain...) h=histogram(dh\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(dh\u03b2[1,:], dh\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"DynamicHMC (NUTS)\"), h2, plot(dh\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(dh\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) See this example for a more canonical DynamicHMC linear regression implementation.","title":"NUTS"},{"location":"ols/#turingjl","text":"Turing.jl is a probabilistic programming language (similar to STAN or BUGS), written in Julia. using Turing @model lreg(x, y, \u03c3, \u03c4) = begin # prior \u03b2 ~ MvNormal(zeros(size(x,2)), \u03c4) # model y ~ MvNormal(x*\u03b2,\u03c3) end lr = lreg(x,y,\u03c3,\u03c4) @time cc = Turing.sample(lr, NUTS(0.65), 2000) 9.958946 seconds (6.15 M allocations: 379.778 MiB, 1.15% gc time) t\u03b2 = cc.value[:,[\"\u03b2[1]\",\"\u03b2[2]\"],1]' h=histogram(t\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(t\u03b2[1,:], t\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Turing, NUTS\"), h2, plot(t\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(t\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2)))","title":"Turing.jl"},{"location":"ols/#about","text":"This meant to accompany these slides on Bayesian methods in IO. This document was created using Weave.jl ((Pastell 2017 )). The code is available in on github . Hoffman, Matthew D, and Andrew Gelman. 2014. \u201cThe No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.\u201d Journal of Machine Learning Research 15 (1): 1593\u20131623. http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf . Pastell, Matti. 2017. \u201cWeave.jl: Scientific Reports Using Julia.\u201d The Journal of Open Source Software . https://doi.org/http://dx.doi.org/10.21105/joss.00204 .","title":"About"},{"location":"quasi/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License using Plots, Distributions Plots.pyplot() Plots.PyPlotBackend() Introduction \u00b6 Traditional bayesian methods require a fully specified statistical model, so that a likelihood can be calculated. In particular, this requires specifying the distribution of error terms. In IO, and economics more broadly, many applied models avoid placing distributional assumptions on error terms, and instead use GMM for estimation. There are two ways to convert a GMM model into something suitable for Bayesian methods. One is to just add distributional assumptions to the error terms. (Jiang, Manchanda, and Rossi 2009 ) applies this approach to a random coefficients demand model. A second approach is the Quasi-Bayesian approach of (Chernozhukov and Hong 2003 ). In this approach, you simply use the GMM objective function in place of the likelihood. We will look at it in some detail. Random Coefficients IV Logit \u00b6 As a working example, we\u2019ll use a random coefficients multinomial logit with endogeneity. To keep things simple for illustrustration, we\u2019ll assume that there\u2019s a single endogenous variable, $x \\in \\mathbb{R}^J$, and market shares are given by s^*(\\beta, \\sigma, \\xi; x) = \\int \\frac{e^{x_j\\beta + \\xi_j + x_j\\sigma \\nu}} {1 + \\sum_{\\ell} e^{x_\\ell\\beta + \\xi_\\ell + x_\\ell\\sigma \\nu}} d\\Phi(\\nu)/\\sigma The moment condition will be 0 = \\Er[\\xi | z ] Fully specified likelihood \u00b6 One way to estimate the model is to fully specify a parametric likleihood. In the simulated data, we know the correct likelihood, so the results we get will be about as good as we can possibly hope to get. Compared to GMM, the full likelihood approach here assumes that (conditional on the instruments, $z$), the market demand shocks are normally distributed, \\xi \\sim N(0, \\Omega) The endogenous variable, $x$, has a normal first stage, x = z \\Pi + \\xi \\Rho + u \\;,\\; u \\sim N(0, \\Sigma_x) and to keep the number of parameters small, we\u2019ll impose $\\Pi=\\pi I_J$ and $\\Rho=\\rho I_J$. Finally, we\u2019ll assume the observed market shares $s_j$ come from $M$ draws from a Multinomial distribtion M round(s) ~ Multinomial(M, s^*(\\beta,\\sigma,\\xi)) where $s^*()$ is the share function implied by the random coefficients model. This approach removes the need to solve for $\\xi$ as a function of $s$ and the other parameters. However, it has the downsides of needing to know $M$, and needing to sample $\\xi$ along with the parameters to generate the posterior. We implement the above model in Turing.jl. This gives a convenient way to both simulate the model and compute the posterior. However, it has the downside of putting an extra layer of abstraction between us and the core computations. This ends up costing us a bit of computation time, and arguably making the posterior calculation harder to debug and extend. using Turing, FastGaussQuadrature, LinearAlgebra, NNlib, JLD2 @model rcivlogit(x=missing, z=missing, s=missing, M=100, \u03bd=gausshermite(12), param=missing, N=10,J=1, ::Type{T}=Float64) where {T <: Real} = begin if x === missing @assert s===missing && z===missing x = Matrix{T}(undef, J, N) s = Matrix{Int64}(undef,J+1,N) z = randn(J,N) end J, N = size(x) \u03be = Matrix{T}(undef, J, N) if !ismissing(param) \u03b2 = pm.\u03b2 \u03c3 = pm.\u03c3 \u03a3x = pm.\u03a3x \u03c0 = pm.\u03c0 \u03c1 = pm.\u03c1 \u03a9 = pm.\u03a9 else # Priors \u03b2 ~ Normal(0, 20.0) \u03c3 ~ truncated(Normal(1.0, 10.0),0, Inf) \u03c1 ~ Normal(0, 20.0) \u03a3x ~ Wishart(J+2, diagm(ones(J))*1/(2*0.001)) \u03a9 ~ Wishart(J+2, diagm(ones(J))*1/(2*0.001)) \u03c0 ~ Normal(0.0, 20.0) end # The likelihood \u03be .~ MvNormal(zeros(J), Symmetric(\u03a9)) \u03bc = zeros(typeof(x[1][1]*\u03b2), J + 1 , length(\u03bd[1])) for i in 1:N x[:,i] ~ MvNormal(\u03c0*z[:,i] + \u03c1*\u03be[:,i], Symmetric(\u03a3x)) \u03bc[1:J,:] .= x[:,i]*\u03b2 + \u03be[:,i] .+ x[:,i]*\u03c3*sqrt(2)*\u03bd[1]' \u03bc = softmax(\u03bc, dims=1) p = (\u03bc*\u03bd[2])/sqrt(Base.\u03c0) s[:,i] ~ Multinomial(M,p) end return(x=x, z=z, s=s, \u03be=\u03be) end # some parameters for simulating data J = 2 pm = (\u03b2=-1.0, \u03c3=0.1, \u03c1=0.5, \u03a3x=diagm(ones(J)), \u03a9=I+0.5*ones(J,J), \u03c0=1.0) N = 20 M = 100 # simulate the data data=rcivlogit(missing, missing ,missing, M, gausshermite(3), pm, N, J)() (x = [1.5606236206850213 1.5219519158714623 \u2026 -1.0286360497735627 0.0108184 89156514982; -2.453567800278451 0.5074554461839629 \u2026 0.5625250614341037 0.4 003686868010303], z = [1.3985257483704419 0.29713928629288733 \u2026 -1.48477596 13888762 -0.3214252085197562; -1.6739253080692698 0.6973508925584603 \u2026 1.38 24130832342796 0.9358136182938399], s = [1 23 \u2026 50 39; 91 29 \u2026 2 41; 8 48 \u2026 48 20], \u03be = [0.8524649217560317 0.8993545466798069 \u2026 -0.572432434708106 1. 0527960731587946; 0.42256442675637257 0.025142021553804295 \u2026 -2.63427388968 96534 1.4402460980774723]) Some remarks on the code When a Turing model is passed missing arguments, it sample them from the specified distributions. When the arguments are not missing, they\u2019re treated as data and held fixed while calculating the posterior. We use Gauss Hermite quadrature to integrate out the random coefficient. 3 integration points is not going to calculate the integral very accurately, but, since we use the same integration approach during estimation, the inaccuracy won\u2019t matter. The Wishart prior distributions for the covariance matrices are not entirely standard. The inverse Wishart distribution is the conjugate prior for the covariance matrix of a Normal distribution, and is a common choice. However, when using HMC for sampling, conjugate priors do not matter. The modern view is that the inverse Wishart puts too much weight on covariances with high correlation, and other priors. The Wishart prior follows the advice of (Chung et al. 2015 ), and helps to avoid regions with degenerate covariance matrices (which were causing numeric problems during the tuning stages of NUTS with other priors, like the LKJ distribution). Results \u00b6 We can simulate the posterior with the following code. It takes some time to run. model = rcivlogit(data.x, data.z, data.s, M, gausshermite(3), missing, N,J) chain = Turing.sample(model, NUTS(0.65), 1000, progress=true, verbose=true) JLD2.@save \"turing.jld2\" chain model data Let\u2019s look at the posteriors. The chain also contains posteriors for all $JN$ values of $\\xi$, which are not going to be displayed. JLD2.@load \"turing.jld2\" chain model data Error: SystemError: opening file \"turing.jld2\": No such file or directory display(describe(chain[[keys(pm)...]])) Error: UndefVarError: chain not defined for k in keys(pm) display(plot(chain[k])) end Error: UndefVarError: chain not defined DynamicHMC Implementation \u00b6 Quasi-Bayesian \u00b6 Consider the following GMM setup. We have a data-depenendent function $g_i(\\theta)$ such that $\\Er[g_i(\\theta_0)] = 0$ . Let the usual GMM estimator be \\hat{\\theta}_n = \\argmin_\\theta n(\\frac{1}{n} g_i(\\theta) )' W (\\frac{1}{n} g_i(\\theta) ) Assume the usual regularity conditions so that \\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\to N\\left(0, (D'WD)^{-1}(D'W\\Omega W D) (D'WD)^{-1} \\right) where $D = D_\\theta \\Er[g_i(\\theta_0)]$ and $\\frac{1}{\\sqrt{n}} \\sum g_i(\\theta_0) \\to N(0, \\Omega)$. Quasi-Bayesian approaches are based on sampling from a quasi-posterior that also converges to $N\\left(0, (D\u2019WD)^{-1}(D\u2019W\\Omega W D) (D\u2019WD)^{-1} \\right)$ as $n\\to \\infty$. Let $\\Sigma = (D\u2019WD)^{-1}(D\u2019W\\Omega W D) (D\u2019WD)^{-1} \\right)$. Note that the log density of the asymptotic distribution is p_\\infy(\\theta) \\propto -\\frac{n}{2}(\\theta - \\theta_0)' \\Sigma^{-1} (\\theta - \\theta_0) Compare that to minus the GMM objective function p_n(\\theta) \\propto -n ... See these notes for more information, or (Chernozhukov and Hong 2003 ) for complete details. About \u00b6 This meant to accompany these slides on Bayesian methods in IO. This document was created using Weave.jl. The code is available in on github . Chernozhukov, Victor, and Han Hong. 2003. \u201cAn {Mcmc} Approach to Classical Estimation.\u201d Journal of Econometrics 115 (2): 293\u2013346. https://doi.org/http://dx.doi.org/10.1016/S0304-4076(03)00100-3 . Chung, Yeojin, Andrew Gelman, Sophia Rabe-Hesketh, Jingchen Liu, and Vincent Dorie. 2015. \u201cWeakly Informative Prior for Point Estimation of Covariance Matrices in Hierarchical Models.\u201d Journal of Educational and Behavioral Statistics 40 (2): 136\u201357. https://doi.org/10.3102/1076998615570945 . Jiang, R., P. Manchanda, and P. E. Rossi. 2009. \u201cBayesian Analysis of Random Coefficient Logit Models Using Aggregate Data.\u201d Journal of Econometrics 149 (2): 136\u201348. http://www.sciencedirect.com/science/article/pii/S0304407608002297 .","title":"Quasi-Bayesian"},{"location":"quasi/#introduction","text":"Traditional bayesian methods require a fully specified statistical model, so that a likelihood can be calculated. In particular, this requires specifying the distribution of error terms. In IO, and economics more broadly, many applied models avoid placing distributional assumptions on error terms, and instead use GMM for estimation. There are two ways to convert a GMM model into something suitable for Bayesian methods. One is to just add distributional assumptions to the error terms. (Jiang, Manchanda, and Rossi 2009 ) applies this approach to a random coefficients demand model. A second approach is the Quasi-Bayesian approach of (Chernozhukov and Hong 2003 ). In this approach, you simply use the GMM objective function in place of the likelihood. We will look at it in some detail.","title":"Introduction"},{"location":"quasi/#random-coefficients-iv-logit","text":"As a working example, we\u2019ll use a random coefficients multinomial logit with endogeneity. To keep things simple for illustrustration, we\u2019ll assume that there\u2019s a single endogenous variable, $x \\in \\mathbb{R}^J$, and market shares are given by s^*(\\beta, \\sigma, \\xi; x) = \\int \\frac{e^{x_j\\beta + \\xi_j + x_j\\sigma \\nu}} {1 + \\sum_{\\ell} e^{x_\\ell\\beta + \\xi_\\ell + x_\\ell\\sigma \\nu}} d\\Phi(\\nu)/\\sigma The moment condition will be 0 = \\Er[\\xi | z ]","title":"Random Coefficients IV Logit"},{"location":"quasi/#fully-specified-likelihood","text":"One way to estimate the model is to fully specify a parametric likleihood. In the simulated data, we know the correct likelihood, so the results we get will be about as good as we can possibly hope to get. Compared to GMM, the full likelihood approach here assumes that (conditional on the instruments, $z$), the market demand shocks are normally distributed, \\xi \\sim N(0, \\Omega) The endogenous variable, $x$, has a normal first stage, x = z \\Pi + \\xi \\Rho + u \\;,\\; u \\sim N(0, \\Sigma_x) and to keep the number of parameters small, we\u2019ll impose $\\Pi=\\pi I_J$ and $\\Rho=\\rho I_J$. Finally, we\u2019ll assume the observed market shares $s_j$ come from $M$ draws from a Multinomial distribtion M round(s) ~ Multinomial(M, s^*(\\beta,\\sigma,\\xi)) where $s^*()$ is the share function implied by the random coefficients model. This approach removes the need to solve for $\\xi$ as a function of $s$ and the other parameters. However, it has the downsides of needing to know $M$, and needing to sample $\\xi$ along with the parameters to generate the posterior. We implement the above model in Turing.jl. This gives a convenient way to both simulate the model and compute the posterior. However, it has the downside of putting an extra layer of abstraction between us and the core computations. This ends up costing us a bit of computation time, and arguably making the posterior calculation harder to debug and extend. using Turing, FastGaussQuadrature, LinearAlgebra, NNlib, JLD2 @model rcivlogit(x=missing, z=missing, s=missing, M=100, \u03bd=gausshermite(12), param=missing, N=10,J=1, ::Type{T}=Float64) where {T <: Real} = begin if x === missing @assert s===missing && z===missing x = Matrix{T}(undef, J, N) s = Matrix{Int64}(undef,J+1,N) z = randn(J,N) end J, N = size(x) \u03be = Matrix{T}(undef, J, N) if !ismissing(param) \u03b2 = pm.\u03b2 \u03c3 = pm.\u03c3 \u03a3x = pm.\u03a3x \u03c0 = pm.\u03c0 \u03c1 = pm.\u03c1 \u03a9 = pm.\u03a9 else # Priors \u03b2 ~ Normal(0, 20.0) \u03c3 ~ truncated(Normal(1.0, 10.0),0, Inf) \u03c1 ~ Normal(0, 20.0) \u03a3x ~ Wishart(J+2, diagm(ones(J))*1/(2*0.001)) \u03a9 ~ Wishart(J+2, diagm(ones(J))*1/(2*0.001)) \u03c0 ~ Normal(0.0, 20.0) end # The likelihood \u03be .~ MvNormal(zeros(J), Symmetric(\u03a9)) \u03bc = zeros(typeof(x[1][1]*\u03b2), J + 1 , length(\u03bd[1])) for i in 1:N x[:,i] ~ MvNormal(\u03c0*z[:,i] + \u03c1*\u03be[:,i], Symmetric(\u03a3x)) \u03bc[1:J,:] .= x[:,i]*\u03b2 + \u03be[:,i] .+ x[:,i]*\u03c3*sqrt(2)*\u03bd[1]' \u03bc = softmax(\u03bc, dims=1) p = (\u03bc*\u03bd[2])/sqrt(Base.\u03c0) s[:,i] ~ Multinomial(M,p) end return(x=x, z=z, s=s, \u03be=\u03be) end # some parameters for simulating data J = 2 pm = (\u03b2=-1.0, \u03c3=0.1, \u03c1=0.5, \u03a3x=diagm(ones(J)), \u03a9=I+0.5*ones(J,J), \u03c0=1.0) N = 20 M = 100 # simulate the data data=rcivlogit(missing, missing ,missing, M, gausshermite(3), pm, N, J)() (x = [1.5606236206850213 1.5219519158714623 \u2026 -1.0286360497735627 0.0108184 89156514982; -2.453567800278451 0.5074554461839629 \u2026 0.5625250614341037 0.4 003686868010303], z = [1.3985257483704419 0.29713928629288733 \u2026 -1.48477596 13888762 -0.3214252085197562; -1.6739253080692698 0.6973508925584603 \u2026 1.38 24130832342796 0.9358136182938399], s = [1 23 \u2026 50 39; 91 29 \u2026 2 41; 8 48 \u2026 48 20], \u03be = [0.8524649217560317 0.8993545466798069 \u2026 -0.572432434708106 1. 0527960731587946; 0.42256442675637257 0.025142021553804295 \u2026 -2.63427388968 96534 1.4402460980774723]) Some remarks on the code When a Turing model is passed missing arguments, it sample them from the specified distributions. When the arguments are not missing, they\u2019re treated as data and held fixed while calculating the posterior. We use Gauss Hermite quadrature to integrate out the random coefficient. 3 integration points is not going to calculate the integral very accurately, but, since we use the same integration approach during estimation, the inaccuracy won\u2019t matter. The Wishart prior distributions for the covariance matrices are not entirely standard. The inverse Wishart distribution is the conjugate prior for the covariance matrix of a Normal distribution, and is a common choice. However, when using HMC for sampling, conjugate priors do not matter. The modern view is that the inverse Wishart puts too much weight on covariances with high correlation, and other priors. The Wishart prior follows the advice of (Chung et al. 2015 ), and helps to avoid regions with degenerate covariance matrices (which were causing numeric problems during the tuning stages of NUTS with other priors, like the LKJ distribution).","title":"Fully specified likelihood"},{"location":"quasi/#results","text":"We can simulate the posterior with the following code. It takes some time to run. model = rcivlogit(data.x, data.z, data.s, M, gausshermite(3), missing, N,J) chain = Turing.sample(model, NUTS(0.65), 1000, progress=true, verbose=true) JLD2.@save \"turing.jld2\" chain model data Let\u2019s look at the posteriors. The chain also contains posteriors for all $JN$ values of $\\xi$, which are not going to be displayed. JLD2.@load \"turing.jld2\" chain model data Error: SystemError: opening file \"turing.jld2\": No such file or directory display(describe(chain[[keys(pm)...]])) Error: UndefVarError: chain not defined for k in keys(pm) display(plot(chain[k])) end Error: UndefVarError: chain not defined","title":"Results"},{"location":"quasi/#dynamichmc-implementation","text":"","title":"DynamicHMC Implementation"},{"location":"quasi/#quasi-bayesian","text":"Consider the following GMM setup. We have a data-depenendent function $g_i(\\theta)$ such that $\\Er[g_i(\\theta_0)] = 0$ . Let the usual GMM estimator be \\hat{\\theta}_n = \\argmin_\\theta n(\\frac{1}{n} g_i(\\theta) )' W (\\frac{1}{n} g_i(\\theta) ) Assume the usual regularity conditions so that \\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\to N\\left(0, (D'WD)^{-1}(D'W\\Omega W D) (D'WD)^{-1} \\right) where $D = D_\\theta \\Er[g_i(\\theta_0)]$ and $\\frac{1}{\\sqrt{n}} \\sum g_i(\\theta_0) \\to N(0, \\Omega)$. Quasi-Bayesian approaches are based on sampling from a quasi-posterior that also converges to $N\\left(0, (D\u2019WD)^{-1}(D\u2019W\\Omega W D) (D\u2019WD)^{-1} \\right)$ as $n\\to \\infty$. Let $\\Sigma = (D\u2019WD)^{-1}(D\u2019W\\Omega W D) (D\u2019WD)^{-1} \\right)$. Note that the log density of the asymptotic distribution is p_\\infy(\\theta) \\propto -\\frac{n}{2}(\\theta - \\theta_0)' \\Sigma^{-1} (\\theta - \\theta_0) Compare that to minus the GMM objective function p_n(\\theta) \\propto -n ... See these notes for more information, or (Chernozhukov and Hong 2003 ) for complete details.","title":"Quasi-Bayesian"},{"location":"quasi/#about","text":"This meant to accompany these slides on Bayesian methods in IO. This document was created using Weave.jl. The code is available in on github . Chernozhukov, Victor, and Han Hong. 2003. \u201cAn {Mcmc} Approach to Classical Estimation.\u201d Journal of Econometrics 115 (2): 293\u2013346. https://doi.org/http://dx.doi.org/10.1016/S0304-4076(03)00100-3 . Chung, Yeojin, Andrew Gelman, Sophia Rabe-Hesketh, Jingchen Liu, and Vincent Dorie. 2015. \u201cWeakly Informative Prior for Point Estimation of Covariance Matrices in Hierarchical Models.\u201d Journal of Educational and Behavioral Statistics 40 (2): 136\u201357. https://doi.org/10.3102/1076998615570945 . Jiang, R., P. Manchanda, and P. E. Rossi. 2009. \u201cBayesian Analysis of Random Coefficient Logit Models Using Aggregate Data.\u201d Journal of Econometrics 149 (2): 136\u201348. http://www.sciencedirect.com/science/article/pii/S0304407608002297 .","title":"About"}]}