{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bayes101.jl \u00b6 These notes and examples are meant to accompany the slides at https://faculty.arts.ubc.ca/pschrimpf/565/565.html MCMC sampling methods: ols.jmd Quasi-Bayesian: quasi.jmd","title":"Home"},{"location":"#bayes101jl","text":"These notes and examples are meant to accompany the slides at https://faculty.arts.ubc.ca/pschrimpf/565/565.html MCMC sampling methods: ols.jmd Quasi-Bayesian: quasi.jmd <!--` \u2013>","title":"Bayes101.jl"},{"location":"license/","text":"These notes are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf. BibTeX citation.","title":"License"},{"location":"ols/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License using Plots, Distributions, StatsPlots, LinearAlgebra Plots.pyplot() Plots.PyPlotBackend() Linear Regression \u00b6 Let\u2019s simulate a simple linear regression, and then sample from the posterior using a few methods. The model is y_i = x_i \\beta + \\epsilon_i with $\\epsilon_i \\sim N(0, \\sigma^2)$ Simulation \u00b6 function simulateOLS(;N::Integer = Int(1e4), \u03b2 = ones(4), \u03c3 = 1) x = randn(N,length(\u03b2)) y = x*\u03b2 + randn(N)*\u03c3 return(x, y) end x, y = simulateOLS(N=Int(1e2), \u03b2=ones(2), \u03c3=1); Computing Posteriors \u00b6 To calculate the posterior distribution of $\\beta$ given the $x$ and $y$, we use a conditional version of Bayes\u2019 rule. p(\\beta|x, y, \\sigma) = \\frac{p(y|x, \\beta, \\sigma) p(\\beta|\\sigma, x) } {p(y |x,\\sigma)} To keep the calculations as simple as possible, we will treat $\\sigma$ as known for now. Since a regression model is agnostic about the distribution of $x$, it is standard to assume that p(\\beta|\\sigma, x) = p(\\beta|\\sigma) In other words, the distribution of $\\beta$ is independent of $x$. With normal errors, the log likelihood, i.e. the log conditional density of $y$ given the parameters , or, \\log(p(y|x, \\beta,\\sigma)) is: function loglike(\u03b2, \u03c3, x, y) return(-length(y)/2*log(2\u03c0) - 1/2*sum(((y .- x*\u03b2)./\u03c3).^2)) end loglike (generic function with 1 method) Exact Posterior \u00b6 If we assume that the prior for $\\beta$ is normal, \\beta|\\sigma \\sim N(0, \\tau^2 I_k) then the posterior can be computed exactly. It is: \\beta|x, y, \\sigma \\sim N\\left( (x'x/\\sigma^2 + I/\\tau^2)^{-1} x'y/\\sigma^2, (x'x/\\sigma^2 + I/\\tau^2)^{-1}\\right) function posterior(\u03b2, \u03c3, x, y, \u03c4) \u03a3 = inv(x'*x/\u03c3^2 + I/\u03c4^2) \u03bc = \u03a3*x'*y/\u03c3^2 return(pdf(MvNormal(\u03bc, \u03a3), \u03b2)) end posterior (generic function with 1 method) Metropolis Hastings \u00b6 If we didn\u2019t know the exact posterior, we could sample from it in a number of ways. Metropolis Hastings is a simple general purpose method. function mhstep(\u03b8, likelihood::Function, prior::Function, proposal::Function) pold = likelihood(\u03b8)*prior(\u03b8) \u03b8new = rand(proposal(\u03b8)) pnew = likelihood(\u03b8new)*prior(\u03b8new) \u03b1 = pnew*pdf(proposal(\u03b8new), \u03b8)/(pold *pdf(proposal(\u03b8), \u03b8new)) u = rand() return( (u < \u03b1) ? \u03b8new : \u03b8) end function mhchain(\u03b80, iterations, likelihood, prior, proposal) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b8[:,i] .= mhstep(\u03b8[:,i-1], likelihood, prior, proposal) end return(\u03b8) end mhchain (generic function with 1 method) When using Metropolis-Hastings, the proposal density plays a key role. The close the proposal is to the posterior, the faster the chain converges. One common choice of proposal distribution is the random walk\u2013draw candidate parameters from $N(\\theta, s)$. Let\u2019s try it. \u03c4 = 10.0 k = size(x,2) \u03c3 = 1.0 for s in [0.01, 0.1, 1.0, 10.0] rw\u03b2 = mhchain(zeros(k), Int(1e5), \u03b2->exp(loglike(\u03b2, \u03c3, x, y)), \u03b2->pdf(MvNormal(zeros(k), \u03c4), \u03b2), \u03b2->MvNormal(\u03b2, s*I)) lims=(0.5, 1.5) h=histogram(rw\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(rw\u03b2[1,:], rw\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Random Walk with s=$s\"), h2, plot(rw\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(rw\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) end Another possible choice of proposal density is an independent \u2014 draw candidate parameters from some fixed distribution. Gibbs Sampling \u00b6 Gibbs sampling refers to when we sample part of our parameters conditional on others, then vice versa, and repeat. Generally, the more parameters than can be sampled at once, the better the chain will work. As a simple illustration, we can sample each of our regression coefficients conditional on the others. function gibbschain(\u03b80, iterations, x, y, \u03c3, \u03c4) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b2 = \u03b8[:,i-1] @views for j in 1:length(\u03b2) e = y .- x[:,1:end .!= j]*\u03b2[1:end .!= j] v = 1/(dot(x[:,j], x[:,j])/\u03c3^2 + 1/\u03c4^2) m = v * (dot(x[:,j],e)/\u03c3^2) \u03b2[j] = rand(Normal(m,sqrt(v))) end \u03b8[:,i] = \u03b2 end return(\u03b8) end g\u03b2 = gibbschain(zeros(2), Int(1e5), x, y, \u03c3, \u03c4) lims=(0.5, 1.5) h=histogram(g\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(g\u03b2[1,:], g\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Gibbs Sampling\"), h2, plot(g\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(g\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) Gibbs sampling is a particularly good idea when the parameters can be divided into groups such that posterior of parameters in each group conditional on the others has a known form. If some parameters do not have known posterior, then another sampling strategy within the Gibbs sampler can be used (like metropolis-hastings). With judicious choice of blocks and conjugate priors, Gibbs sampling can be applied to very complex models. OpenBUGS, WinBUGS and JAGS are (non-Julia) software packages designed around Gibbs sampling. Mamba.jl is a Julia package that could be used for Gibbs sampling (as well as other sampling methods). Hamiltonian Monte Carlo \u00b6 Recent efforts in Bayesian computation have shifted away from Gibbs sampling of blocks with closed form posteriors towards more general purpose methods. Metropolis-Hastings is a general purpose algorithm in that all you need to use it is to be able to compute the likelihood (up to a constant of proportionality) and the prior. Unfortunately, Metropolis-Hastings requires a good proposal distribution to work well, and it is hard to find a good proposal distribution in many situations. Hamiltonian Monte Carlo uses information about the gradient of the posterior to generate a good proposal distribution. The physical analogy here is that the log posterior acts like a gravitational field, and we generate samples by creating random trajectories through that field. With the right choice of trajectories (i.e. not too fast & not too slow), they will tend to concentrate in areas of high posterior probability, while also exploring the space of parameters well. using Zygote, ForwardDiff function hmcstep(\u03b8, logdens, \u03f5=0.1, L=5; \u2207logdens=\u03b8->Zygote.gradient(logdens, \u03b8)[1], M = I, iM=inv(M)) U(x) = -logdens(x) dU(x) = -\u2207logdens(x) K(m) = (m'*iM*m/2)[1] dK(m) = m'*iM m = rand(MvNormal(zeros(length(\u03b8)), M),1) \u03b8new = copy(\u03b8) mnew = copy(m) for s in 1:L mnew .+= -dU(\u03b8new)[:]*\u03f5/2 \u03b8new .+= dK(mnew)[:]*\u03f5 end \u03b1 = exp(-U(\u03b8new)+U(\u03b8)-K(mnew)+K(m)) u = rand() return(u < \u03b1 ? \u03b8new : \u03b8) end function hmcchain(\u03b80, iterations, logdens, \u2207logdens=\u03b8->Zygote.gradient(logdens, \u03b8)[1] ; \u03f5=0.1, L=5, M=I) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b8[:,i] .= hmcstep(\u03b8[:,i-1], logdens, \u03f5, L, M=M) end return(\u03b8) end logp = \u03b2->(loglike(\u03b2, \u03c3, x, y) + logpdf(MvNormal(zeros(k), \u03c4), \u03b2)) \u2207logp(\u03b2) = ForwardDiff.gradient(logp, \u03b2) for \u03f5 in [1e-3, 0.1] for L in [2, 10] @show \u03f5, L @time h\u03b2 = hmcchain(zeros(k), Int(1e4),logp, \u03f5=\u03f5, L=L) lims=(0.5, 1.5) h=histogram(h\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(h\u03b2[1,:], h\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"HMC with \u03f5=$\u03f5, L=$L\"), h2, plot(h\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(h\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) end end (\u03f5, L) = (0.001, 2) 2.025082 seconds (16.48 M allocations: 822.892 MiB, 7.75% gc time) (\u03f5, L) = (0.001, 10) 7.481677 seconds (76.23 M allocations: 3.599 GiB, 9.54% gc time) (\u03f5, L) = (0.1, 2) 1.596762 seconds (15.60 M allocations: 779.647 MiB, 9.47% gc time) (\u03f5, L) = (0.1, 10) 7.512895 seconds (76.23 M allocations: 3.599 GiB, 9.16% gc time) The step size, $\\epsilon$, and the number of steps, $L$, affect how well the chain performs. If $\\epsilon$ or $L$ is too large, then many proposals will get rejected. If $\\epsilon L$ is too small, then the draws will be very close together, and it will take more iterations of the chain to explore the space. NUTS \u00b6 The No-U-Turn-Sample (NUTS) ((Hoffman and Gelman 2014 )) algorithmically chooses $L$, $\\epsilon$, and $M$. Stan is (non-Julia, but callable from Julia) software for Bayesian modelling that largely uses NUTS for sampling. DynamicHMC.jl is a Julia implementation. Let\u2019s try it out. using DynamicHMC, Random, LogDensityProblems struct LinearRegressionKnownSigma{T} y::Vector{T} x::Matrix{T} \u03c3::T \u03c4::T end function LogDensityProblems.capabilities(::Type{<:LinearRegressionKnownSigma}) LogDensityProblems.LogDensityOrder{0}() end LogDensityProblems.dimension(lr::LinearRegressionKnownSigma) = size(lr.x,2) function LogDensityProblems.logdensity(lr::LinearRegressionKnownSigma, \u03b2) k = size(x,2) return(logpdf(MvNormal(zeros(k), lr.\u03c4), \u03b2) + loglikelihood(Normal(0, \u03c3), y - x*\u03b2)) end lr = LinearRegressionKnownSigma(y,x,\u03c3,\u03c4) \u2207logposterior=ADgradient(:Zygote, lr) @time results=DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, \u2207logposterior, 1000; warmup_stages= default_warmup_stages(init_steps=50, middle_steps=100, doubling_stages=2)); 11.208851 seconds (60.46 M allocations: 2.138 GiB, 4.90% gc time) dh\u03b2 = hcat(results.inference.chain...) h=histogram(dh\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(dh\u03b2[1,:], dh\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"DynamicHMC (NUTS)\"), h2, plot(dh\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(dh\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) See this example for a more canonical DynamicHMC linear regression implementation. Turing.jl \u00b6 Turing.jl is a probabilistic programming language (similar to STAN or BUGS), written in Julia. using Turing @model lreg(x, y, \u03c3, \u03c4) = begin # prior \u03b2 ~ MvNormal(zeros(size(x,2)), \u03c4) # model y ~ MvNormal(x*\u03b2,\u03c3) end lr = lreg(x,y,\u03c3,\u03c4) @time cc = Turing.sample(lr, NUTS(0.65), 2000) 9.958946 seconds (6.15 M allocations: 379.778 MiB, 1.15% gc time) t\u03b2 = cc.value[:,[\"\u03b2[1]\",\"\u03b2[2]\"],1]' h=histogram(t\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(t\u03b2[1,:], t\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Turing, NUTS\"), h2, plot(t\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(t\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) About \u00b6 This meant to accompany these slides on Bayesian methods in IO. This document was created using Weave.jl ((Pastell 2017 )). The code is available in on github . Hoffman, Matthew D, and Andrew Gelman. 2014. \u201cThe No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.\u201d Journal of Machine Learning Research 15 (1): 1593\u20131623. http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf . Pastell, Matti. 2017. \u201cWeave.jl: Scientific Reports Using Julia.\u201d The Journal of Open Source Software . https://doi.org/http://dx.doi.org/10.21105/joss.00204 .","title":"MCMC sampling methods"},{"location":"ols/#linear-regression","text":"Let\u2019s simulate a simple linear regression, and then sample from the posterior using a few methods. The model is y_i = x_i \\beta + \\epsilon_i with $\\epsilon_i \\sim N(0, \\sigma^2)$","title":"Linear Regression"},{"location":"ols/#simulation","text":"function simulateOLS(;N::Integer = Int(1e4), \u03b2 = ones(4), \u03c3 = 1) x = randn(N,length(\u03b2)) y = x*\u03b2 + randn(N)*\u03c3 return(x, y) end x, y = simulateOLS(N=Int(1e2), \u03b2=ones(2), \u03c3=1);","title":"Simulation"},{"location":"ols/#computing-posteriors","text":"To calculate the posterior distribution of $\\beta$ given the $x$ and $y$, we use a conditional version of Bayes\u2019 rule. p(\\beta|x, y, \\sigma) = \\frac{p(y|x, \\beta, \\sigma) p(\\beta|\\sigma, x) } {p(y |x,\\sigma)} To keep the calculations as simple as possible, we will treat $\\sigma$ as known for now. Since a regression model is agnostic about the distribution of $x$, it is standard to assume that p(\\beta|\\sigma, x) = p(\\beta|\\sigma) In other words, the distribution of $\\beta$ is independent of $x$. With normal errors, the log likelihood, i.e. the log conditional density of $y$ given the parameters , or, \\log(p(y|x, \\beta,\\sigma)) is: function loglike(\u03b2, \u03c3, x, y) return(-length(y)/2*log(2\u03c0) - 1/2*sum(((y .- x*\u03b2)./\u03c3).^2)) end loglike (generic function with 1 method)","title":"Computing Posteriors"},{"location":"ols/#exact-posterior","text":"If we assume that the prior for $\\beta$ is normal, \\beta|\\sigma \\sim N(0, \\tau^2 I_k) then the posterior can be computed exactly. It is: \\beta|x, y, \\sigma \\sim N\\left( (x'x/\\sigma^2 + I/\\tau^2)^{-1} x'y/\\sigma^2, (x'x/\\sigma^2 + I/\\tau^2)^{-1}\\right) function posterior(\u03b2, \u03c3, x, y, \u03c4) \u03a3 = inv(x'*x/\u03c3^2 + I/\u03c4^2) \u03bc = \u03a3*x'*y/\u03c3^2 return(pdf(MvNormal(\u03bc, \u03a3), \u03b2)) end posterior (generic function with 1 method)","title":"Exact Posterior"},{"location":"ols/#metropolis-hastings","text":"If we didn\u2019t know the exact posterior, we could sample from it in a number of ways. Metropolis Hastings is a simple general purpose method. function mhstep(\u03b8, likelihood::Function, prior::Function, proposal::Function) pold = likelihood(\u03b8)*prior(\u03b8) \u03b8new = rand(proposal(\u03b8)) pnew = likelihood(\u03b8new)*prior(\u03b8new) \u03b1 = pnew*pdf(proposal(\u03b8new), \u03b8)/(pold *pdf(proposal(\u03b8), \u03b8new)) u = rand() return( (u < \u03b1) ? \u03b8new : \u03b8) end function mhchain(\u03b80, iterations, likelihood, prior, proposal) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b8[:,i] .= mhstep(\u03b8[:,i-1], likelihood, prior, proposal) end return(\u03b8) end mhchain (generic function with 1 method) When using Metropolis-Hastings, the proposal density plays a key role. The close the proposal is to the posterior, the faster the chain converges. One common choice of proposal distribution is the random walk\u2013draw candidate parameters from $N(\\theta, s)$. Let\u2019s try it. \u03c4 = 10.0 k = size(x,2) \u03c3 = 1.0 for s in [0.01, 0.1, 1.0, 10.0] rw\u03b2 = mhchain(zeros(k), Int(1e5), \u03b2->exp(loglike(\u03b2, \u03c3, x, y)), \u03b2->pdf(MvNormal(zeros(k), \u03c4), \u03b2), \u03b2->MvNormal(\u03b2, s*I)) lims=(0.5, 1.5) h=histogram(rw\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(rw\u03b2[1,:], rw\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Random Walk with s=$s\"), h2, plot(rw\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(rw\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) end Another possible choice of proposal density is an independent \u2014 draw candidate parameters from some fixed distribution.","title":"Metropolis Hastings"},{"location":"ols/#gibbs-sampling","text":"Gibbs sampling refers to when we sample part of our parameters conditional on others, then vice versa, and repeat. Generally, the more parameters than can be sampled at once, the better the chain will work. As a simple illustration, we can sample each of our regression coefficients conditional on the others. function gibbschain(\u03b80, iterations, x, y, \u03c3, \u03c4) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b2 = \u03b8[:,i-1] @views for j in 1:length(\u03b2) e = y .- x[:,1:end .!= j]*\u03b2[1:end .!= j] v = 1/(dot(x[:,j], x[:,j])/\u03c3^2 + 1/\u03c4^2) m = v * (dot(x[:,j],e)/\u03c3^2) \u03b2[j] = rand(Normal(m,sqrt(v))) end \u03b8[:,i] = \u03b2 end return(\u03b8) end g\u03b2 = gibbschain(zeros(2), Int(1e5), x, y, \u03c3, \u03c4) lims=(0.5, 1.5) h=histogram(g\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(g\u03b2[1,:], g\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Gibbs Sampling\"), h2, plot(g\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(g\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) Gibbs sampling is a particularly good idea when the parameters can be divided into groups such that posterior of parameters in each group conditional on the others has a known form. If some parameters do not have known posterior, then another sampling strategy within the Gibbs sampler can be used (like metropolis-hastings). With judicious choice of blocks and conjugate priors, Gibbs sampling can be applied to very complex models. OpenBUGS, WinBUGS and JAGS are (non-Julia) software packages designed around Gibbs sampling. Mamba.jl is a Julia package that could be used for Gibbs sampling (as well as other sampling methods).","title":"Gibbs Sampling"},{"location":"ols/#hamiltonian-monte-carlo","text":"Recent efforts in Bayesian computation have shifted away from Gibbs sampling of blocks with closed form posteriors towards more general purpose methods. Metropolis-Hastings is a general purpose algorithm in that all you need to use it is to be able to compute the likelihood (up to a constant of proportionality) and the prior. Unfortunately, Metropolis-Hastings requires a good proposal distribution to work well, and it is hard to find a good proposal distribution in many situations. Hamiltonian Monte Carlo uses information about the gradient of the posterior to generate a good proposal distribution. The physical analogy here is that the log posterior acts like a gravitational field, and we generate samples by creating random trajectories through that field. With the right choice of trajectories (i.e. not too fast & not too slow), they will tend to concentrate in areas of high posterior probability, while also exploring the space of parameters well. using Zygote, ForwardDiff function hmcstep(\u03b8, logdens, \u03f5=0.1, L=5; \u2207logdens=\u03b8->Zygote.gradient(logdens, \u03b8)[1], M = I, iM=inv(M)) U(x) = -logdens(x) dU(x) = -\u2207logdens(x) K(m) = (m'*iM*m/2)[1] dK(m) = m'*iM m = rand(MvNormal(zeros(length(\u03b8)), M),1) \u03b8new = copy(\u03b8) mnew = copy(m) for s in 1:L mnew .+= -dU(\u03b8new)[:]*\u03f5/2 \u03b8new .+= dK(mnew)[:]*\u03f5 end \u03b1 = exp(-U(\u03b8new)+U(\u03b8)-K(mnew)+K(m)) u = rand() return(u < \u03b1 ? \u03b8new : \u03b8) end function hmcchain(\u03b80, iterations, logdens, \u2207logdens=\u03b8->Zygote.gradient(logdens, \u03b8)[1] ; \u03f5=0.1, L=5, M=I) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b8[:,i] .= hmcstep(\u03b8[:,i-1], logdens, \u03f5, L, M=M) end return(\u03b8) end logp = \u03b2->(loglike(\u03b2, \u03c3, x, y) + logpdf(MvNormal(zeros(k), \u03c4), \u03b2)) \u2207logp(\u03b2) = ForwardDiff.gradient(logp, \u03b2) for \u03f5 in [1e-3, 0.1] for L in [2, 10] @show \u03f5, L @time h\u03b2 = hmcchain(zeros(k), Int(1e4),logp, \u03f5=\u03f5, L=L) lims=(0.5, 1.5) h=histogram(h\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(h\u03b2[1,:], h\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"HMC with \u03f5=$\u03f5, L=$L\"), h2, plot(h\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(h\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) end end (\u03f5, L) = (0.001, 2) 2.025082 seconds (16.48 M allocations: 822.892 MiB, 7.75% gc time) (\u03f5, L) = (0.001, 10) 7.481677 seconds (76.23 M allocations: 3.599 GiB, 9.54% gc time) (\u03f5, L) = (0.1, 2) 1.596762 seconds (15.60 M allocations: 779.647 MiB, 9.47% gc time) (\u03f5, L) = (0.1, 10) 7.512895 seconds (76.23 M allocations: 3.599 GiB, 9.16% gc time) The step size, $\\epsilon$, and the number of steps, $L$, affect how well the chain performs. If $\\epsilon$ or $L$ is too large, then many proposals will get rejected. If $\\epsilon L$ is too small, then the draws will be very close together, and it will take more iterations of the chain to explore the space.","title":"Hamiltonian Monte Carlo"},{"location":"ols/#nuts","text":"The No-U-Turn-Sample (NUTS) ((Hoffman and Gelman 2014 )) algorithmically chooses $L$, $\\epsilon$, and $M$. Stan is (non-Julia, but callable from Julia) software for Bayesian modelling that largely uses NUTS for sampling. DynamicHMC.jl is a Julia implementation. Let\u2019s try it out. using DynamicHMC, Random, LogDensityProblems struct LinearRegressionKnownSigma{T} y::Vector{T} x::Matrix{T} \u03c3::T \u03c4::T end function LogDensityProblems.capabilities(::Type{<:LinearRegressionKnownSigma}) LogDensityProblems.LogDensityOrder{0}() end LogDensityProblems.dimension(lr::LinearRegressionKnownSigma) = size(lr.x,2) function LogDensityProblems.logdensity(lr::LinearRegressionKnownSigma, \u03b2) k = size(x,2) return(logpdf(MvNormal(zeros(k), lr.\u03c4), \u03b2) + loglikelihood(Normal(0, \u03c3), y - x*\u03b2)) end lr = LinearRegressionKnownSigma(y,x,\u03c3,\u03c4) \u2207logposterior=ADgradient(:Zygote, lr) @time results=DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, \u2207logposterior, 1000; warmup_stages= default_warmup_stages(init_steps=50, middle_steps=100, doubling_stages=2)); 11.208851 seconds (60.46 M allocations: 2.138 GiB, 4.90% gc time) dh\u03b2 = hcat(results.inference.chain...) h=histogram(dh\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(dh\u03b2[1,:], dh\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"DynamicHMC (NUTS)\"), h2, plot(dh\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(dh\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) See this example for a more canonical DynamicHMC linear regression implementation.","title":"NUTS"},{"location":"ols/#turingjl","text":"Turing.jl is a probabilistic programming language (similar to STAN or BUGS), written in Julia. using Turing @model lreg(x, y, \u03c3, \u03c4) = begin # prior \u03b2 ~ MvNormal(zeros(size(x,2)), \u03c4) # model y ~ MvNormal(x*\u03b2,\u03c3) end lr = lreg(x,y,\u03c3,\u03c4) @time cc = Turing.sample(lr, NUTS(0.65), 2000) 9.958946 seconds (6.15 M allocations: 379.778 MiB, 1.15% gc time) t\u03b2 = cc.value[:,[\"\u03b2[1]\",\"\u03b2[2]\"],1]' h=histogram(t\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(t\u03b2[1,:], t\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Turing, NUTS\"), h2, plot(t\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(t\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2)))","title":"Turing.jl"},{"location":"ols/#about","text":"This meant to accompany these slides on Bayesian methods in IO. This document was created using Weave.jl ((Pastell 2017 )). The code is available in on github . Hoffman, Matthew D, and Andrew Gelman. 2014. \u201cThe No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.\u201d Journal of Machine Learning Research 15 (1): 1593\u20131623. http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf . Pastell, Matti. 2017. \u201cWeave.jl: Scientific Reports Using Julia.\u201d The Journal of Open Source Software . https://doi.org/http://dx.doi.org/10.21105/joss.00204 .","title":"About"},{"location":"quasi/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License using Plots, Distributions Plots.pyplot() Plots.PyPlotBackend() \\def{\\Er}{\\mathrm{E}} Introduction \u00b6 Traditional bayesian methods require a fully specified statistical model, so that a likelihood can be calculated. In particular, this requires specifying the distribution of error terms. In IO, and economics more broadly, many applied models avoid placing distributional assumptions on error terms, and instead use GMM for estimation. There are two ways to convert a GMM model into something suitable for Bayesian methods. One is to just add distributional assumptions to the error terms. (Jiang, Manchanda, and Rossi 2009 ) applies this approach to a random coefficients demand model. A second approach is the Quasi-Bayesian approach of (Chernozhukov and Hong 2003 ). In this approach, you simply use the GMM objective function in place of the likelihood. We will look at it in some detail. Random Coefficients IV Logit \u00b6 As a working example, we\u2019ll use a random coefficients multinomial logit with endogeneity. To keep things simple for illustrustration, we\u2019ll assume that there\u2019s a single endogenous variable, $x \\in \\mathbb{R}^J$, and market shares are given by s^*(\\beta, \\sigma, \\xi; x) = \\int \\frac{e^{x_j\\beta + \\xi_j + x_j\\sigma \\nu}} {1 + \\sum_{\\ell} e^{x_\\ell\\beta + \\xi_\\ell + x_\\ell\\sigma \\nu}} d\\Phi(\\nu)/\\sigma The moment condition will be 0 = \\Er[\\xi | z ] Fully specified likelihood \u00b6 One way to estimate the model is to fully specify a parametric likleihood. In the simulated data, we know the correct likelihood, so the results we get will be about as good as we can possibly hope to get. Compared to GMM, the full likelihood approach here assumes that (conditional on the instruments, $z$), the market demand shocks are normally distributed, \\xi \\sim N(0, \\Omega) The endogenous variable, $x$, has a normal first stage, x = z \\Pi + \\xi \\Xi + u \\;,\\; u \\sim N(0, \\Sigma_x) and to keep the number of parameters small, we\u2019ll impose $\\Pi=\\pi I_J$ and $\\Xi=\\rho I_J$. Finally, we\u2019ll assume the observed market shares $s_j$ come from $M$ draws from a Multinomial distribtion M \\mathrm{round}(s) ~ \\mathrm{Multinomial}(M, s^*(\\beta,\\sigma,\\xi)) where $s^*()$ is the share function implied by the random coefficients model. This approach removes the need to solve for $\\xi$ as a function of $s$ and the other parameters. However, it has the downsides of needing to know $M$, and needing to sample $\\xi$ along with the parameters to generate the posterior. We implement the above model in Turing.jl. This gives a convenient way to both simulate the model and compute the posterior. However, it has the downside of putting an extra layer of abstraction between us and the core computations. This ends up costing us a bit of computation time, and arguably making the posterior calculation harder to debug and extend. using Turing, FastGaussQuadrature, LinearAlgebra, NNlib, JLD2 @model rcivlogit(x=missing, z=missing, s=missing, M=100, \u03bd=gausshermite(12), param=missing, N=10,J=1, ::Type{T}=Float64) where {T <: Real} = begin if x === missing @assert s===missing && z===missing x = Matrix{T}(undef, J, N) s = Matrix{Int64}(undef,J+1,N) z = randn(J,N) end J, N = size(x) \u03be = Matrix{T}(undef, J, N) if !ismissing(param) \u03b2 = pm.\u03b2 \u03c3 = pm.\u03c3 \u03a3x = pm.\u03a3x \u03c0 = pm.\u03c0 \u03c1 = pm.\u03c1 \u03a9 = pm.\u03a9 else # Priors \u03b2 ~ Normal(0, 20.0) \u03c3 ~ truncated(Normal(1.0, 10.0),0, Inf) \u03c1 ~ Normal(0, 20.0) \u03a3x ~ Wishart(J+2, diagm(ones(J))*1/(2*0.001)) \u03a9 ~ Wishart(J+2, diagm(ones(J))*1/(2*0.001)) \u03c0 ~ Normal(0.0, 20.0) end # The likelihood \u03be .~ MvNormal(zeros(J), Symmetric(\u03a9)) \u03bc = zeros(typeof(x[1][1]*\u03b2), J + 1 , length(\u03bd[1])) for i in 1:N x[:,i] ~ MvNormal(\u03c0*z[:,i] + \u03c1*\u03be[:,i], Symmetric(\u03a3x)) \u03bc[1:J,:] .= x[:,i]*\u03b2 + \u03be[:,i] .+ x[:,i]*\u03c3*sqrt(2)*\u03bd[1]' \u03bc = softmax(\u03bc, dims=1) p = (\u03bc*\u03bd[2])/sqrt(Base.\u03c0) s[:,i] ~ Multinomial(M,p) end return(x=x, z=z, s=s, \u03be=\u03be) end # some parameters for simulating data J = 2 pm = (\u03b2=-1.0, \u03c3=0.1, \u03c1=0.5, \u03a3x=diagm(ones(J)), \u03a9=I+0.5*ones(J,J), \u03c0=1.0) N = 20 M = 100 # simulate the data data=rcivlogit(missing, missing ,missing, M, gausshermite(3), pm, N, J)(); Some remarks on the code When a Turing model is passed missing arguments, it sample them from the specified distributions. When the arguments are not missing, they\u2019re treated as data and held fixed while calculating the posterior. We use Gauss Hermite quadrature to integrate out the random coefficient. 3 integration points is not going to calculate the integral very accurately, but, since we use the same integration approach during estimation it will work out well. The Wishart prior distributions for the covariance matrices are not entirely standard. The inverse Wishart distribution is the conjugate prior for the covariance matrix of a Normal distribution, and is a common choice. However, when using HMC for sampling, conjugate priors do not matter. The modern view is that the inverse Wishart puts too much weight on covariances with high correlation, and other priors. The Wishart prior follows the advice of (Chung et al. 2015 ), and helps to avoid regions with degenerate covariance matrices (which were causing numeric problems during the tuning stages of NUTS with other priors, like the LKJ distribution). Results \u00b6 We can sample from the posterior with the following code. It takes some time to run. model = rcivlogit(data.x, data.z, data.s, M, gausshermite(3), missing, N,J) chain = Turing.sample(model, NUTS(0.65), 1000, progress=true, verbose=true) JLD2.@save \"jmd/turing.jld2\" chain model data Let\u2019s look at the posteriors. The chain also contains posteriors for all $JN$ values of $\\xi$, which are not going to be displayed. JLD2.@load \"jmd/turing.jld2\" chain model data for k in keys(pm) display(plot(chain[k])) end display(describe(chain[[keys(pm)...]])) 2-element Array{ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03a3x[1, 1] 3.0778 1.3840 0.0619 0.0960 120.1318 1.0019 \u03a3x[1, 2] 1.0670 0.8911 0.0399 0.0712 158.8332 1.0025 \u03a3x[2, 1] 1.0670 0.8911 0.0399 0.0712 158.8332 1.0025 \u03a3x[2, 2] 2.1036 0.9176 0.0410 0.0595 348.2472 1.0059 \u03a9[1, 1] 2.4330 1.6947 0.0758 0.1013 153.8935 1.0009 \u03a9[1, 2] 0.8539 1.1533 0.0516 0.0817 161.7063 1.0019 \u03a9[2, 1] 0.8539 1.1533 0.0516 0.0817 161.7063 1.0019 \u03a9[2, 2] 2.7162 1.2958 0.0579 0.0726 281.2821 0.9985 \u03b2 -0.9214 0.2821 0.0126 0.0385 62.2979 1.0289 \u03c0 1.0659 0.2592 0.0116 0.0070 292.6790 0.9984 \u03c1 0.2482 0.3257 0.0146 0.0335 100.6957 1.0181 \u03c3 0.2897 0.1845 0.0083 0.0100 212.8711 0.9984 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03a3x[1, 1] 0.9943 2.0881 2.8644 3.9272 6.2623 \u03a3x[1, 2] -0.2738 0.4539 0.9735 1.5556 3.2447 \u03a3x[2, 1] -0.2738 0.4539 0.9735 1.5556 3.2447 \u03a3x[2, 2] 0.9298 1.4544 1.8973 2.5971 4.2834 \u03a9[1, 1] 0.7670 1.4292 1.9901 2.8824 6.4902 \u03a9[1, 2] -0.9247 0.1637 0.7029 1.2866 3.8990 \u03a9[2, 1] -0.9247 0.1637 0.7029 1.2866 3.8990 \u03a9[2, 2] 1.1296 1.8870 2.3947 3.2030 5.7431 \u03b2 -1.5187 -1.0873 -0.9218 -0.7284 -0.3709 \u03c0 0.4887 0.9059 1.0715 1.2454 1.5612 \u03c1 -0.4314 0.0055 0.2708 0.4913 0.8023 \u03c3 0.0165 0.1439 0.2621 0.4246 0.6918 We can see that the chain mixed pretty well. The posterior of the covariance matrices are somewhat high, but the other posteriors of the other parameters appear centered on their true values. Poorly estimated covariances are typical for this model (and many other models), regardless of estimation method. DynamicHMC Implementation \u00b6 DynamicHMC.jl is a useful alternative to Turing.jl. With DynamicHMC, you must write a function to evaluate the log posterior. This is arguably slightly more work than Turing\u2019s modeling language (although it avoids needing to learn Turing\u2019s language), but it also gives you more control over the code. This tends to result in more efficient code. using DynamicHMC, Random, LogDensityProblems, Parameters, TransformVariables, Distributions, MCMCChains defaultpriors = (\u03b2=Normal(0,20), \u03c3=truncated(Normal(0, 10), 0, Inf), \u03c0=Normal(0,20), \u03c1=Normal(0,20), \u03a3x=Wishart(J+2, diagm(ones(J))*1/(2*0.001)), \u03a9 =Wishart(J+2, diagm(ones(J))*1/(2*0.001))) macro evalpriors(priors) first = true expr = :(1+1) for k in keys(defaultpriors) if first expr = :(logpdf($(priors).$k, $k)) first = false else expr = :($expr + logpdf($(priors).$k, $k)) end end return(esc(expr)) end # We define a struct to hold the data relevant to the model struct RCIVLogit{T,Stype} x::Matrix{T} z::Matrix{T} s::Matrix{Stype} N::Int64 J::Int64 \u03bd::typeof(gausshermite(1)) priors::typeof(defaultpriors) end # A function (d::RCIVLogit)(\u03b8) @unpack \u03be, \u03b2, \u03c3, \u03c0, \u03c1, s\u03be, u\u03be, sx, ux = \u03b8 @unpack x, z, s, N, J, \u03bd = d T = typeof(d.x[1,1]*\u03b2) \u03a3x =Symmetric(ux'*Diagonal(exp.(sx).^2)*ux) \u03a9 = Symmetric(u\u03be'*Diagonal(exp.(s\u03be).^2)*u\u03be) # priors logp=@evalpriors(d.priors) # loglikelihood logp += loglikelihood(MvNormal(zeros(J), \u03a9), \u03be) logp += loglikelihood(MvNormal(zeros(J), \u03a3x), x - \u03c0*z - \u03c1*\u03be) \u03bc = zeros(T, J + 1 , length(\u03bd[1])) for i in 1:N @views \u03bc[1:J,:] .= x[:,i]*\u03b2 + \u03be[:,i] .+ x[:,i]*\u03c3*sqrt(2)*\u03bd[1]' \u03bc = softmax(\u03bc, dims=1) p = (\u03bc*\u03bd[2])/sqrt(Base.\u03c0) logp += logpdf(Multinomial(sum(s[:,i]), p),s[:,i]) end return(logp) end \u03bd = gausshermite(3) prob = RCIVLogit(data.x, data.z, data.s, N, J,\u03bd , defaultpriors) \u03b80 = ( \u03be=data.\u03be, \u03b2=pm.\u03b2, \u03c3=pm.\u03c3, \u03c0=pm.\u03c0, \u03c1=pm.\u03c1, s\u03be = log.(sqrt.(diag(pm.\u03a9))), u\u03be=cholesky(pm.\u03a9 ./ sqrt.(diag(pm.\u03a9)*diag(pm.\u03a9)')).U, sx = log.(sqrt.(diag(pm.\u03a3x))), ux=cholesky(pm.\u03a3x ./ sqrt.(diag(pm.\u03a3x)*diag(pm.\u03a3x)')).U ) prob(\u03b80) # A transformation from \u211d^(# parameters) to the parameters # see Transformations.jl t= as((\u03be=as(Array,as\u211d,J,N), \u03b2 = as\u211d, \u03c3=as\u211d\u208a, \u03c0=as\u211d, \u03c1=as\u211d, s\u03be=as(Array,as\u211d,J), u\u03be=CorrCholeskyFactor(J), sx=as(Array,as\u211d,J), ux=CorrCholeskyFactor(J)) ) x0 = inverse(t, \u03b80) # wrap our logposterior and transform into the struc that DynamicHMC # uses for sampling P = TransformedLogDensity(t, prob) \u2207P = ADgradient(:ForwardDiff, P); warmup_stages = default_warmup_stages(;doubling_stages=2, M=Symmetric) results = DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, \u2207P, 1000; initialization = (q = x0, ), reporter = LogProgressReport(nothing, 25, 15), warmup_stages=warmup_stages) JLD2.@save \"jmd/dhmc.jld2\" results JLD2.@load \"jmd/dhmc.jld2\" results post = transform.(t,results.inference.chain) p = post[1] names = vcat([length(p[s])==1 ? String(s) : String.(s).*string.(1:length(p[s])) for s in keys(p)]...) vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)' chain = MCMCChains.Chains(reshape(vals, size(vals)..., 1), names) describe(chain[[\"\u03b2\",\"\u03c3\",\"\u03c1\",\"\u03c0\"]]) plot(chain[[\"\u03b2\",\"\u03c3\",\"\u03c1\",\"\u03c0\"]]) Quasi-Bayesian \u00b6 Consider the following GMM setup. We have a data-depenendent function $g_i(\\theta)$ such that $\\Er[g_i(\\theta_0)] = 0$ . Let the usual GMM estimator be \\hat{\\theta}^{GMM} = \\argmin_\\theta n(\\frac{1}{n} g_i(\\theta) )' W (\\frac{1}{n} g_i(\\theta)) Assume the usual regularity conditions so that \\sqrt{n}(\\hat{\\theta}^{GMM} - \\theta_0) \\to N\\left(0, (D'WD)^{-1}(D'W\\Omega W D) (D'WD)^{-1} \\right) where $D = D_\\theta \\Er[g_i(\\theta_0)]$ and $\\frac{1}{\\sqrt{n}} \\sum g_i(\\theta_0) \\to N(0, \\Omega)$. Quasi-Bayesian approaches are based on sampling from a quasi-posterior that also converges to $N\\left(0, (D\u2019WD)^{-1}(D\u2019W\\Omega W D) (D\u2019WD)^{-1} \\right)$ as $n\\to \\infty$. Let $\\Sigma = (D\u2019WD)^{-1}(D\u2019W\\Omega W D) (D\u2019WD)^{-1} \\right)$. Note that the log density of the asymptotic distribution is p_\\infty(\\theta) \\propto -\\frac{n}{2}(\\theta - \\theta_0)' \\Sigma^{-1} (\\theta - \\theta_0) Compare that to minus the GMM objective function (which we\u2019ll denote by $\\log(p_n(\\theta)$) \\log(p_n(\\theta)) = -n(\\frac{1}{n} g_i(\\theta) )' W (\\frac{1}{n} g_i(\\theta) ) If we take a second order Taylor expansion, \\log(p_n(\\theta)) \\approx log(p_n(\\theta_0)) + -n 2(\\theta - \\theta_0)' D' W (\\frac{1}{n} g_i(\\theta_0)) + \\frac{-n}{2} (\\theta-\\theta_0)'D W D' (\\theta-\\theta_0) and complete the square log(p_n(\\theta)) \\approx -\\frac{n}{2} ( (\\theta - \\theta_0) - (D'WD)^{-1} \\frac{1}{n}\\sum g_i(\\theta_0) )' D' W D ((\\theta - \\theta_0) - (D'WD)^{-1} \\frac{1}{n}\\sum g_i(\\theta_0) ) + C where $C$ is a constant that does not depend on $\\theta$. From this we can see that if we treat $\\log(p_n(\\theta))$ as a log posterior distribution, we get that that conditional on the data, the quasi-posterior is approximately normal with a data dependent mean and variance $D\u2019WD$. p_n( \\sqrt{n}(\\theta - \\theta_0) ) \\approx N\\left( (D'WD)^{-1} \\frac{1}{\\sqrt{n}}\\sum g_i(\\theta_0) , D'WD \\right) Finally, note that the usual asymptotic expansion of the GMM objective gives \\$\\$ \\sqrt{n} ( \\hat{\\theta} \\^{GMM} - \\theta _0 ) = (D\u2019WD)\\^{-1} \\frac{1}{\\sqrt{n}} \\sum g_i( \\theta _0) + o_p(1), then p_n( \\sqrt{n}(\\theta - \\theta_0) ) \\approx N\\left(\\sqrt{n}(\\hat{\\theta}^{GMM} - \\theta_0), D'WD \\right) That is, the quasi-posterior is approximately normal with mean equal to the usual GMM estimate, and variance $D\u2019WD$. If we treat the quasi-posterior and as a real posterior, the quasi-posterior mean or median will be consistent since they each approach $\\hat{\\theta}^{GMM}$ and $\\hat{\\theta}^{GMM}$ is consistent. Additionally, if we use quasi-posterior credible intervals for inference, they may or may not be consistent. In general, the quasi-posterior variance, $D\u2019WD$, differs from the asymptotic variance, $(D\u2019WD)^{-1}(D\u2019W\\Omega W D)(D\u2019WD)^{-1}$, and quasi-posterior credible intervals will not be consistent. However, if we use an efficient weighting matrix, $W=\\Omega^{-1}$, then the quasi-posterior credible intervals will be consistent. See these notes for more information, or (Chernozhukov and Hong 2003 ) for complete details. Implementation \u00b6 To compute moment conditions, we need to solve for $\\xi$ given the parameters. Here\u2019s the associated code. It\u2019s similar to what we used in BLPDemand.jl . using DynamicHMC, Random, LogDensityProblems, Parameters, TransformVariables, Distributions, ForwardDiff, NLsolve function share(\u03b4::AbstractVector, \u03c3, x::AbstractVector, \u03bd::typeof(gausshermite(1))) J = length(\u03b4) S = length(\u03bd[1]) s = zeros(promote_type(eltype(\u03b4), eltype(\u03c3)),size(\u03b4)) si = similar(s) @inbounds for i in 1:S si .= \u03b4 + \u03c3*x*\u03bd[1][i] # to prevent overflow from exp(\u03b4 + ...) simax = max(maximum(si), 0) si .-= simax si .= exp.(si) si .= si./(exp.(-simax) + sum(si)) s .+= si*\u03bd[2][i] end s .+= eps(eltype(s)) return(s) end function delta(s::AbstractVector{T}, x::AbstractVector{T}, \u03c3::T, \u03bd::typeof(gausshermite(1))) where T tol = 1e-6 out = try sol=NLsolve.fixedpoint(d->(d .+ log.(s) .- log.(share(d, \u03c3, x, \u03bd))), log.(s) .- log.(1-sum(s)), method = :anderson, m=4, xtol=tol, ftol=tol, iterations=100, show_trace=false) sol.zero catch log.(s) .- log.(1-sum(s)) end return(out) end # Use the implicit function theorem to make ForwardDiff work with delta() function delta(s::AbstractVector, x::AbstractVector, \u03c3::D, \u03bd::typeof(gausshermite(1))) where {D <: ForwardDiff.Dual} \u03c3val = ForwardDiff.value.(\u03c3) \u03b4 = delta(s,x,\u03c3val, \u03bd) \u2202\u03b4 = ForwardDiff.jacobian(d -> share(d, \u03c3val, x, \u03bd), \u03b4) \u2202\u03c3 = ForwardDiff.jacobian(s -> share(\u03b4, s..., x, \u03bd), [\u03c3val]) out = Vector{typeof(\u03c3)}(undef, length(\u03b4)) Jv = try -\u2202\u03b4 \\ \u2202\u03c3 catch zeros(eltype(\u2202\u03c3),size(\u2202\u03c3)) end Jc = zeros(ForwardDiff.valtype(D), length(\u03c3), ForwardDiff.npartials(D)) for i in eachindex(\u03c3) Jc[i,:] .= ForwardDiff.partials(\u03c3[i]) end Jn = Jv * Jc for i in eachindex(out) out[i] = D(\u03b4[i], ForwardDiff.Partials(tuple(Jn[i,:]...))) end return(out) end delta (generic function with 2 methods) Now the code for the quasi-posterior. using DynamicHMC, Random, LogDensityProblems, Parameters, TransformVariables, Distributions struct RCIVLogitQB{T,Stype} x::Matrix{T} z::Matrix{T} s::Matrix{Stype} N::Int64 J::Int64 \u03bd::typeof(gausshermite(1)) priors::typeof(defaultpriors) W::Matrix{T} end Error: invalid redefinition of constant RCIVLogitQB defaultpriors = (\u03b2=Normal(0,20), \u03c3=truncated(Normal(0, 10), 0, Inf)) macro evalpriors(priors) first = true expr = :(1+1) for k in keys(defaultpriors) if first expr = :(logpdf($(priors).$k, $k)) first = false else expr = :($expr + logpdf($(priors).$k, $k)) end end return(esc(expr)) end function (d::RCIVLogitQB)(\u03b8) @unpack \u03b2, \u03c3 = \u03b8 @unpack x, z, s, N, J, \u03bd, priors, W = d T = typeof(x[1,1]*\u03b2) # priors logp = @evalpriors(d.priors) # quasi-loglikelihood m = zeros(T, J*J) for i in 1:N \u03be = delta(s[:,i], x[:,i], \u03c3, \u03bd) - x[:,i]*\u03b2 m .+= vec(\u03be*z[:,i]') end m ./= N logp += -0.5*N*m'*W*m #logpdf(MvNormal(zeros(length(m)), W), m) return(logp) end gh = gausshermite(3) \u03bd = (gh[1],gh[2]/sqrt(Base.\u03c0)) prob = RCIVLogitQB(data.x, data.z, max.(1,data.s[1:J,:])./M, N, J,\u03bd , defaultpriors, diagm(ones(J*J))) \u03b80 = (\u03b2=pm.\u03b2, \u03c3=pm.\u03c3 ) prob(\u03b80) t=as((\u03b2 = as\u211d, \u03c3=as\u211d\u208a)) x0 = inverse(t, \u03b80) P = TransformedLogDensity(t, prob) \u2207P = ADgradient(:ForwardDiff, P) LogDensityProblems.logdensity_and_gradient(\u2207P, x0); The chain here actually takes very little time to run, but we still cache the results. warmup=default_warmup_stages(stepsize_search=nothing, doubling_stages=2) results = DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, \u2207P, 1000; initialization = (q = x0, \u03f5=1e-5 ), reporter = LogProgressReport(nothing, 25, 15), warmup_stages =warmup) JLD2.@save \"jmd/qb.jld2\" results JLD2.@load \"jmd/qb.jld2\" results post = transform.(t,results.inference.chain) p = post[1] names = vcat([length(p[s])==1 ? String(s) : String.(s).*string.(1:length(p[s])) for s in keys(p)]...) vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)' chain = MCMCChains.Chains(reshape(vals, size(vals)..., 1), names) describe(chain[[\"\u03b2\",\"\u03c3\"]]) plot(chain[[\"\u03b2\",\"\u03c3\"]]) The quasi-posterior means look okay. The standard deviation and quantiles are not consistent because we didn\u2019t use an efficient weighting matrix above. Let\u2019s use one and repeat. \u03c3 = mean(chain[\"\u03c3\"].value) \u03b2 = mean(chain[\"\u03b2\"].value) m = zeros(J*J, N) for i in 1:N \u03be = delta(prob.s[:,i], prob.x[:,i], \u03c3, \u03bd) - prob.x[:,i]*\u03b2 m[:,i] .= vec(\u03be*prob.z[:,i]') end W = Symmetric(inv(cov(m'))) prob = RCIVLogitQB(data.x, data.z, max.(1,data.s[1:J,:])./M, N, J,\u03bd , defaultpriors,W) Error: MethodError: no method matching RCIVLogitQB(::Array{Float64,2}, ::Ar ray{Float64,2}, ::Array{Float64,2}, ::Int64, ::Int64, ::Tuple{Array{Float64 ,1},Array{Float64,1}}, ::NamedTuple{(:\u03b2, :\u03c3),Tuple{Normal{Float64},Truncate d{Normal{Float64},Continuous,Float64}}}, ::Symmetric{Float64,Array{Float64, 2}}) Closest candidates are: RCIVLogitQB(::Array{T,2}, ::Array{T,2}, ::Array{Stype,2}, ::Int64, ::Int6 4, ::Tuple{Array{Float64,1},Array{Float64,1}}, ::NamedTuple{(:\u03b2, :\u03c3),Tuple{ Normal{Float64},Truncated{Normal{Float64},Continuous,Float64}}}, !Matched:: Array{T,2}) where {T, Stype} at /home/paul/.julia/dev/Bayes101/docs/jmd/qb. jl:67 \u03b80 = (\u03b2=pm.\u03b2, \u03c3=pm.\u03c3 ) prob(\u03b80) P = TransformedLogDensity(t, prob) \u2207P = ADgradient(:ForwardDiff, P) LogDensityProblems.logdensity_and_gradient(\u2207P, x0); warmup=default_warmup_stages(stepsize_search=nothing, doubling_stages=2) results = DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, \u2207P, 1000;initialization = (q = x0, \u03f5=1e-5 ), reporter = LogProgressReport(nothing, 25, 15), warmup_stages =warmup) JLD2.@save \"jmd/qbw.jld2\" results JLD2.@load \"jmd/qbw.jld2\" results post = transform.(t,results.inference.chain) p = post[1] names = vcat([length(p[s])==1 ? String(s) : String.(s).*string.(1:length(p[s])) for s in keys(p)]...) vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)' chain = MCMCChains.Chains(reshape(vals, size(vals)..., 1), names) describe(chain[[\"\u03b2\",\"\u03c3\"]]) plot(chain[[\"\u03b2\",\"\u03c3\"]]) These posterior standard deviations and quantiles can be used for inference. About \u00b6 This meant to accompany these slides on Bayesian methods in IO. This document was created using Weave.jl. The code is available in on github . Chernozhukov, Victor, and Han Hong. 2003. \u201cAn {Mcmc} Approach to Classical Estimation.\u201d Journal of Econometrics 115 (2): 293\u2013346. https://doi.org/http://dx.doi.org/10.1016/S0304-4076(03)00100-3 . Chung, Yeojin, Andrew Gelman, Sophia Rabe-Hesketh, Jingchen Liu, and Vincent Dorie. 2015. \u201cWeakly Informative Prior for Point Estimation of Covariance Matrices in Hierarchical Models.\u201d Journal of Educational and Behavioral Statistics 40 (2): 136\u201357. https://doi.org/10.3102/1076998615570945 . Jiang, R., P. Manchanda, and P. E. Rossi. 2009. \u201cBayesian Analysis of Random Coefficient Logit Models Using Aggregate Data.\u201d Journal of Econometrics 149 (2): 136\u201348. http://www.sciencedirect.com/science/article/pii/S0304407608002297 .","title":"Quasi-Bayesian"},{"location":"quasi/#introduction","text":"Traditional bayesian methods require a fully specified statistical model, so that a likelihood can be calculated. In particular, this requires specifying the distribution of error terms. In IO, and economics more broadly, many applied models avoid placing distributional assumptions on error terms, and instead use GMM for estimation. There are two ways to convert a GMM model into something suitable for Bayesian methods. One is to just add distributional assumptions to the error terms. (Jiang, Manchanda, and Rossi 2009 ) applies this approach to a random coefficients demand model. A second approach is the Quasi-Bayesian approach of (Chernozhukov and Hong 2003 ). In this approach, you simply use the GMM objective function in place of the likelihood. We will look at it in some detail.","title":"Introduction"},{"location":"quasi/#random-coefficients-iv-logit","text":"As a working example, we\u2019ll use a random coefficients multinomial logit with endogeneity. To keep things simple for illustrustration, we\u2019ll assume that there\u2019s a single endogenous variable, $x \\in \\mathbb{R}^J$, and market shares are given by s^*(\\beta, \\sigma, \\xi; x) = \\int \\frac{e^{x_j\\beta + \\xi_j + x_j\\sigma \\nu}} {1 + \\sum_{\\ell} e^{x_\\ell\\beta + \\xi_\\ell + x_\\ell\\sigma \\nu}} d\\Phi(\\nu)/\\sigma The moment condition will be 0 = \\Er[\\xi | z ]","title":"Random Coefficients IV Logit"},{"location":"quasi/#fully-specified-likelihood","text":"One way to estimate the model is to fully specify a parametric likleihood. In the simulated data, we know the correct likelihood, so the results we get will be about as good as we can possibly hope to get. Compared to GMM, the full likelihood approach here assumes that (conditional on the instruments, $z$), the market demand shocks are normally distributed, \\xi \\sim N(0, \\Omega) The endogenous variable, $x$, has a normal first stage, x = z \\Pi + \\xi \\Xi + u \\;,\\; u \\sim N(0, \\Sigma_x) and to keep the number of parameters small, we\u2019ll impose $\\Pi=\\pi I_J$ and $\\Xi=\\rho I_J$. Finally, we\u2019ll assume the observed market shares $s_j$ come from $M$ draws from a Multinomial distribtion M \\mathrm{round}(s) ~ \\mathrm{Multinomial}(M, s^*(\\beta,\\sigma,\\xi)) where $s^*()$ is the share function implied by the random coefficients model. This approach removes the need to solve for $\\xi$ as a function of $s$ and the other parameters. However, it has the downsides of needing to know $M$, and needing to sample $\\xi$ along with the parameters to generate the posterior. We implement the above model in Turing.jl. This gives a convenient way to both simulate the model and compute the posterior. However, it has the downside of putting an extra layer of abstraction between us and the core computations. This ends up costing us a bit of computation time, and arguably making the posterior calculation harder to debug and extend. using Turing, FastGaussQuadrature, LinearAlgebra, NNlib, JLD2 @model rcivlogit(x=missing, z=missing, s=missing, M=100, \u03bd=gausshermite(12), param=missing, N=10,J=1, ::Type{T}=Float64) where {T <: Real} = begin if x === missing @assert s===missing && z===missing x = Matrix{T}(undef, J, N) s = Matrix{Int64}(undef,J+1,N) z = randn(J,N) end J, N = size(x) \u03be = Matrix{T}(undef, J, N) if !ismissing(param) \u03b2 = pm.\u03b2 \u03c3 = pm.\u03c3 \u03a3x = pm.\u03a3x \u03c0 = pm.\u03c0 \u03c1 = pm.\u03c1 \u03a9 = pm.\u03a9 else # Priors \u03b2 ~ Normal(0, 20.0) \u03c3 ~ truncated(Normal(1.0, 10.0),0, Inf) \u03c1 ~ Normal(0, 20.0) \u03a3x ~ Wishart(J+2, diagm(ones(J))*1/(2*0.001)) \u03a9 ~ Wishart(J+2, diagm(ones(J))*1/(2*0.001)) \u03c0 ~ Normal(0.0, 20.0) end # The likelihood \u03be .~ MvNormal(zeros(J), Symmetric(\u03a9)) \u03bc = zeros(typeof(x[1][1]*\u03b2), J + 1 , length(\u03bd[1])) for i in 1:N x[:,i] ~ MvNormal(\u03c0*z[:,i] + \u03c1*\u03be[:,i], Symmetric(\u03a3x)) \u03bc[1:J,:] .= x[:,i]*\u03b2 + \u03be[:,i] .+ x[:,i]*\u03c3*sqrt(2)*\u03bd[1]' \u03bc = softmax(\u03bc, dims=1) p = (\u03bc*\u03bd[2])/sqrt(Base.\u03c0) s[:,i] ~ Multinomial(M,p) end return(x=x, z=z, s=s, \u03be=\u03be) end # some parameters for simulating data J = 2 pm = (\u03b2=-1.0, \u03c3=0.1, \u03c1=0.5, \u03a3x=diagm(ones(J)), \u03a9=I+0.5*ones(J,J), \u03c0=1.0) N = 20 M = 100 # simulate the data data=rcivlogit(missing, missing ,missing, M, gausshermite(3), pm, N, J)(); Some remarks on the code When a Turing model is passed missing arguments, it sample them from the specified distributions. When the arguments are not missing, they\u2019re treated as data and held fixed while calculating the posterior. We use Gauss Hermite quadrature to integrate out the random coefficient. 3 integration points is not going to calculate the integral very accurately, but, since we use the same integration approach during estimation it will work out well. The Wishart prior distributions for the covariance matrices are not entirely standard. The inverse Wishart distribution is the conjugate prior for the covariance matrix of a Normal distribution, and is a common choice. However, when using HMC for sampling, conjugate priors do not matter. The modern view is that the inverse Wishart puts too much weight on covariances with high correlation, and other priors. The Wishart prior follows the advice of (Chung et al. 2015 ), and helps to avoid regions with degenerate covariance matrices (which were causing numeric problems during the tuning stages of NUTS with other priors, like the LKJ distribution).","title":"Fully specified likelihood"},{"location":"quasi/#results","text":"We can sample from the posterior with the following code. It takes some time to run. model = rcivlogit(data.x, data.z, data.s, M, gausshermite(3), missing, N,J) chain = Turing.sample(model, NUTS(0.65), 1000, progress=true, verbose=true) JLD2.@save \"jmd/turing.jld2\" chain model data Let\u2019s look at the posteriors. The chain also contains posteriors for all $JN$ values of $\\xi$, which are not going to be displayed. JLD2.@load \"jmd/turing.jld2\" chain model data for k in keys(pm) display(plot(chain[k])) end display(describe(chain[[keys(pm)...]])) 2-element Array{ChainDataFrame,1} Summary Statistics parameters mean std naive_se mcse ess r_hat \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500 \u03a3x[1, 1] 3.0778 1.3840 0.0619 0.0960 120.1318 1.0019 \u03a3x[1, 2] 1.0670 0.8911 0.0399 0.0712 158.8332 1.0025 \u03a3x[2, 1] 1.0670 0.8911 0.0399 0.0712 158.8332 1.0025 \u03a3x[2, 2] 2.1036 0.9176 0.0410 0.0595 348.2472 1.0059 \u03a9[1, 1] 2.4330 1.6947 0.0758 0.1013 153.8935 1.0009 \u03a9[1, 2] 0.8539 1.1533 0.0516 0.0817 161.7063 1.0019 \u03a9[2, 1] 0.8539 1.1533 0.0516 0.0817 161.7063 1.0019 \u03a9[2, 2] 2.7162 1.2958 0.0579 0.0726 281.2821 0.9985 \u03b2 -0.9214 0.2821 0.0126 0.0385 62.2979 1.0289 \u03c0 1.0659 0.2592 0.0116 0.0070 292.6790 0.9984 \u03c1 0.2482 0.3257 0.0146 0.0335 100.6957 1.0181 \u03c3 0.2897 0.1845 0.0083 0.0100 212.8711 0.9984 Quantiles parameters 2.5% 25.0% 50.0% 75.0% 97.5% \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u03a3x[1, 1] 0.9943 2.0881 2.8644 3.9272 6.2623 \u03a3x[1, 2] -0.2738 0.4539 0.9735 1.5556 3.2447 \u03a3x[2, 1] -0.2738 0.4539 0.9735 1.5556 3.2447 \u03a3x[2, 2] 0.9298 1.4544 1.8973 2.5971 4.2834 \u03a9[1, 1] 0.7670 1.4292 1.9901 2.8824 6.4902 \u03a9[1, 2] -0.9247 0.1637 0.7029 1.2866 3.8990 \u03a9[2, 1] -0.9247 0.1637 0.7029 1.2866 3.8990 \u03a9[2, 2] 1.1296 1.8870 2.3947 3.2030 5.7431 \u03b2 -1.5187 -1.0873 -0.9218 -0.7284 -0.3709 \u03c0 0.4887 0.9059 1.0715 1.2454 1.5612 \u03c1 -0.4314 0.0055 0.2708 0.4913 0.8023 \u03c3 0.0165 0.1439 0.2621 0.4246 0.6918 We can see that the chain mixed pretty well. The posterior of the covariance matrices are somewhat high, but the other posteriors of the other parameters appear centered on their true values. Poorly estimated covariances are typical for this model (and many other models), regardless of estimation method.","title":"Results"},{"location":"quasi/#dynamichmc-implementation","text":"DynamicHMC.jl is a useful alternative to Turing.jl. With DynamicHMC, you must write a function to evaluate the log posterior. This is arguably slightly more work than Turing\u2019s modeling language (although it avoids needing to learn Turing\u2019s language), but it also gives you more control over the code. This tends to result in more efficient code. using DynamicHMC, Random, LogDensityProblems, Parameters, TransformVariables, Distributions, MCMCChains defaultpriors = (\u03b2=Normal(0,20), \u03c3=truncated(Normal(0, 10), 0, Inf), \u03c0=Normal(0,20), \u03c1=Normal(0,20), \u03a3x=Wishart(J+2, diagm(ones(J))*1/(2*0.001)), \u03a9 =Wishart(J+2, diagm(ones(J))*1/(2*0.001))) macro evalpriors(priors) first = true expr = :(1+1) for k in keys(defaultpriors) if first expr = :(logpdf($(priors).$k, $k)) first = false else expr = :($expr + logpdf($(priors).$k, $k)) end end return(esc(expr)) end # We define a struct to hold the data relevant to the model struct RCIVLogit{T,Stype} x::Matrix{T} z::Matrix{T} s::Matrix{Stype} N::Int64 J::Int64 \u03bd::typeof(gausshermite(1)) priors::typeof(defaultpriors) end # A function (d::RCIVLogit)(\u03b8) @unpack \u03be, \u03b2, \u03c3, \u03c0, \u03c1, s\u03be, u\u03be, sx, ux = \u03b8 @unpack x, z, s, N, J, \u03bd = d T = typeof(d.x[1,1]*\u03b2) \u03a3x =Symmetric(ux'*Diagonal(exp.(sx).^2)*ux) \u03a9 = Symmetric(u\u03be'*Diagonal(exp.(s\u03be).^2)*u\u03be) # priors logp=@evalpriors(d.priors) # loglikelihood logp += loglikelihood(MvNormal(zeros(J), \u03a9), \u03be) logp += loglikelihood(MvNormal(zeros(J), \u03a3x), x - \u03c0*z - \u03c1*\u03be) \u03bc = zeros(T, J + 1 , length(\u03bd[1])) for i in 1:N @views \u03bc[1:J,:] .= x[:,i]*\u03b2 + \u03be[:,i] .+ x[:,i]*\u03c3*sqrt(2)*\u03bd[1]' \u03bc = softmax(\u03bc, dims=1) p = (\u03bc*\u03bd[2])/sqrt(Base.\u03c0) logp += logpdf(Multinomial(sum(s[:,i]), p),s[:,i]) end return(logp) end \u03bd = gausshermite(3) prob = RCIVLogit(data.x, data.z, data.s, N, J,\u03bd , defaultpriors) \u03b80 = ( \u03be=data.\u03be, \u03b2=pm.\u03b2, \u03c3=pm.\u03c3, \u03c0=pm.\u03c0, \u03c1=pm.\u03c1, s\u03be = log.(sqrt.(diag(pm.\u03a9))), u\u03be=cholesky(pm.\u03a9 ./ sqrt.(diag(pm.\u03a9)*diag(pm.\u03a9)')).U, sx = log.(sqrt.(diag(pm.\u03a3x))), ux=cholesky(pm.\u03a3x ./ sqrt.(diag(pm.\u03a3x)*diag(pm.\u03a3x)')).U ) prob(\u03b80) # A transformation from \u211d^(# parameters) to the parameters # see Transformations.jl t= as((\u03be=as(Array,as\u211d,J,N), \u03b2 = as\u211d, \u03c3=as\u211d\u208a, \u03c0=as\u211d, \u03c1=as\u211d, s\u03be=as(Array,as\u211d,J), u\u03be=CorrCholeskyFactor(J), sx=as(Array,as\u211d,J), ux=CorrCholeskyFactor(J)) ) x0 = inverse(t, \u03b80) # wrap our logposterior and transform into the struc that DynamicHMC # uses for sampling P = TransformedLogDensity(t, prob) \u2207P = ADgradient(:ForwardDiff, P); warmup_stages = default_warmup_stages(;doubling_stages=2, M=Symmetric) results = DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, \u2207P, 1000; initialization = (q = x0, ), reporter = LogProgressReport(nothing, 25, 15), warmup_stages=warmup_stages) JLD2.@save \"jmd/dhmc.jld2\" results JLD2.@load \"jmd/dhmc.jld2\" results post = transform.(t,results.inference.chain) p = post[1] names = vcat([length(p[s])==1 ? String(s) : String.(s).*string.(1:length(p[s])) for s in keys(p)]...) vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)' chain = MCMCChains.Chains(reshape(vals, size(vals)..., 1), names) describe(chain[[\"\u03b2\",\"\u03c3\",\"\u03c1\",\"\u03c0\"]]) plot(chain[[\"\u03b2\",\"\u03c3\",\"\u03c1\",\"\u03c0\"]])","title":"DynamicHMC Implementation"},{"location":"quasi/#quasi-bayesian","text":"Consider the following GMM setup. We have a data-depenendent function $g_i(\\theta)$ such that $\\Er[g_i(\\theta_0)] = 0$ . Let the usual GMM estimator be \\hat{\\theta}^{GMM} = \\argmin_\\theta n(\\frac{1}{n} g_i(\\theta) )' W (\\frac{1}{n} g_i(\\theta)) Assume the usual regularity conditions so that \\sqrt{n}(\\hat{\\theta}^{GMM} - \\theta_0) \\to N\\left(0, (D'WD)^{-1}(D'W\\Omega W D) (D'WD)^{-1} \\right) where $D = D_\\theta \\Er[g_i(\\theta_0)]$ and $\\frac{1}{\\sqrt{n}} \\sum g_i(\\theta_0) \\to N(0, \\Omega)$. Quasi-Bayesian approaches are based on sampling from a quasi-posterior that also converges to $N\\left(0, (D\u2019WD)^{-1}(D\u2019W\\Omega W D) (D\u2019WD)^{-1} \\right)$ as $n\\to \\infty$. Let $\\Sigma = (D\u2019WD)^{-1}(D\u2019W\\Omega W D) (D\u2019WD)^{-1} \\right)$. Note that the log density of the asymptotic distribution is p_\\infty(\\theta) \\propto -\\frac{n}{2}(\\theta - \\theta_0)' \\Sigma^{-1} (\\theta - \\theta_0) Compare that to minus the GMM objective function (which we\u2019ll denote by $\\log(p_n(\\theta)$) \\log(p_n(\\theta)) = -n(\\frac{1}{n} g_i(\\theta) )' W (\\frac{1}{n} g_i(\\theta) ) If we take a second order Taylor expansion, \\log(p_n(\\theta)) \\approx log(p_n(\\theta_0)) + -n 2(\\theta - \\theta_0)' D' W (\\frac{1}{n} g_i(\\theta_0)) + \\frac{-n}{2} (\\theta-\\theta_0)'D W D' (\\theta-\\theta_0) and complete the square log(p_n(\\theta)) \\approx -\\frac{n}{2} ( (\\theta - \\theta_0) - (D'WD)^{-1} \\frac{1}{n}\\sum g_i(\\theta_0) )' D' W D ((\\theta - \\theta_0) - (D'WD)^{-1} \\frac{1}{n}\\sum g_i(\\theta_0) ) + C where $C$ is a constant that does not depend on $\\theta$. From this we can see that if we treat $\\log(p_n(\\theta))$ as a log posterior distribution, we get that that conditional on the data, the quasi-posterior is approximately normal with a data dependent mean and variance $D\u2019WD$. p_n( \\sqrt{n}(\\theta - \\theta_0) ) \\approx N\\left( (D'WD)^{-1} \\frac{1}{\\sqrt{n}}\\sum g_i(\\theta_0) , D'WD \\right) Finally, note that the usual asymptotic expansion of the GMM objective gives \\$\\$ \\sqrt{n} ( \\hat{\\theta} \\^{GMM} - \\theta _0 ) = (D\u2019WD)\\^{-1} \\frac{1}{\\sqrt{n}} \\sum g_i( \\theta _0) + o_p(1), then p_n( \\sqrt{n}(\\theta - \\theta_0) ) \\approx N\\left(\\sqrt{n}(\\hat{\\theta}^{GMM} - \\theta_0), D'WD \\right) That is, the quasi-posterior is approximately normal with mean equal to the usual GMM estimate, and variance $D\u2019WD$. If we treat the quasi-posterior and as a real posterior, the quasi-posterior mean or median will be consistent since they each approach $\\hat{\\theta}^{GMM}$ and $\\hat{\\theta}^{GMM}$ is consistent. Additionally, if we use quasi-posterior credible intervals for inference, they may or may not be consistent. In general, the quasi-posterior variance, $D\u2019WD$, differs from the asymptotic variance, $(D\u2019WD)^{-1}(D\u2019W\\Omega W D)(D\u2019WD)^{-1}$, and quasi-posterior credible intervals will not be consistent. However, if we use an efficient weighting matrix, $W=\\Omega^{-1}$, then the quasi-posterior credible intervals will be consistent. See these notes for more information, or (Chernozhukov and Hong 2003 ) for complete details.","title":"Quasi-Bayesian"},{"location":"quasi/#implementation","text":"To compute moment conditions, we need to solve for $\\xi$ given the parameters. Here\u2019s the associated code. It\u2019s similar to what we used in BLPDemand.jl . using DynamicHMC, Random, LogDensityProblems, Parameters, TransformVariables, Distributions, ForwardDiff, NLsolve function share(\u03b4::AbstractVector, \u03c3, x::AbstractVector, \u03bd::typeof(gausshermite(1))) J = length(\u03b4) S = length(\u03bd[1]) s = zeros(promote_type(eltype(\u03b4), eltype(\u03c3)),size(\u03b4)) si = similar(s) @inbounds for i in 1:S si .= \u03b4 + \u03c3*x*\u03bd[1][i] # to prevent overflow from exp(\u03b4 + ...) simax = max(maximum(si), 0) si .-= simax si .= exp.(si) si .= si./(exp.(-simax) + sum(si)) s .+= si*\u03bd[2][i] end s .+= eps(eltype(s)) return(s) end function delta(s::AbstractVector{T}, x::AbstractVector{T}, \u03c3::T, \u03bd::typeof(gausshermite(1))) where T tol = 1e-6 out = try sol=NLsolve.fixedpoint(d->(d .+ log.(s) .- log.(share(d, \u03c3, x, \u03bd))), log.(s) .- log.(1-sum(s)), method = :anderson, m=4, xtol=tol, ftol=tol, iterations=100, show_trace=false) sol.zero catch log.(s) .- log.(1-sum(s)) end return(out) end # Use the implicit function theorem to make ForwardDiff work with delta() function delta(s::AbstractVector, x::AbstractVector, \u03c3::D, \u03bd::typeof(gausshermite(1))) where {D <: ForwardDiff.Dual} \u03c3val = ForwardDiff.value.(\u03c3) \u03b4 = delta(s,x,\u03c3val, \u03bd) \u2202\u03b4 = ForwardDiff.jacobian(d -> share(d, \u03c3val, x, \u03bd), \u03b4) \u2202\u03c3 = ForwardDiff.jacobian(s -> share(\u03b4, s..., x, \u03bd), [\u03c3val]) out = Vector{typeof(\u03c3)}(undef, length(\u03b4)) Jv = try -\u2202\u03b4 \\ \u2202\u03c3 catch zeros(eltype(\u2202\u03c3),size(\u2202\u03c3)) end Jc = zeros(ForwardDiff.valtype(D), length(\u03c3), ForwardDiff.npartials(D)) for i in eachindex(\u03c3) Jc[i,:] .= ForwardDiff.partials(\u03c3[i]) end Jn = Jv * Jc for i in eachindex(out) out[i] = D(\u03b4[i], ForwardDiff.Partials(tuple(Jn[i,:]...))) end return(out) end delta (generic function with 2 methods) Now the code for the quasi-posterior. using DynamicHMC, Random, LogDensityProblems, Parameters, TransformVariables, Distributions struct RCIVLogitQB{T,Stype} x::Matrix{T} z::Matrix{T} s::Matrix{Stype} N::Int64 J::Int64 \u03bd::typeof(gausshermite(1)) priors::typeof(defaultpriors) W::Matrix{T} end Error: invalid redefinition of constant RCIVLogitQB defaultpriors = (\u03b2=Normal(0,20), \u03c3=truncated(Normal(0, 10), 0, Inf)) macro evalpriors(priors) first = true expr = :(1+1) for k in keys(defaultpriors) if first expr = :(logpdf($(priors).$k, $k)) first = false else expr = :($expr + logpdf($(priors).$k, $k)) end end return(esc(expr)) end function (d::RCIVLogitQB)(\u03b8) @unpack \u03b2, \u03c3 = \u03b8 @unpack x, z, s, N, J, \u03bd, priors, W = d T = typeof(x[1,1]*\u03b2) # priors logp = @evalpriors(d.priors) # quasi-loglikelihood m = zeros(T, J*J) for i in 1:N \u03be = delta(s[:,i], x[:,i], \u03c3, \u03bd) - x[:,i]*\u03b2 m .+= vec(\u03be*z[:,i]') end m ./= N logp += -0.5*N*m'*W*m #logpdf(MvNormal(zeros(length(m)), W), m) return(logp) end gh = gausshermite(3) \u03bd = (gh[1],gh[2]/sqrt(Base.\u03c0)) prob = RCIVLogitQB(data.x, data.z, max.(1,data.s[1:J,:])./M, N, J,\u03bd , defaultpriors, diagm(ones(J*J))) \u03b80 = (\u03b2=pm.\u03b2, \u03c3=pm.\u03c3 ) prob(\u03b80) t=as((\u03b2 = as\u211d, \u03c3=as\u211d\u208a)) x0 = inverse(t, \u03b80) P = TransformedLogDensity(t, prob) \u2207P = ADgradient(:ForwardDiff, P) LogDensityProblems.logdensity_and_gradient(\u2207P, x0); The chain here actually takes very little time to run, but we still cache the results. warmup=default_warmup_stages(stepsize_search=nothing, doubling_stages=2) results = DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, \u2207P, 1000; initialization = (q = x0, \u03f5=1e-5 ), reporter = LogProgressReport(nothing, 25, 15), warmup_stages =warmup) JLD2.@save \"jmd/qb.jld2\" results JLD2.@load \"jmd/qb.jld2\" results post = transform.(t,results.inference.chain) p = post[1] names = vcat([length(p[s])==1 ? String(s) : String.(s).*string.(1:length(p[s])) for s in keys(p)]...) vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)' chain = MCMCChains.Chains(reshape(vals, size(vals)..., 1), names) describe(chain[[\"\u03b2\",\"\u03c3\"]]) plot(chain[[\"\u03b2\",\"\u03c3\"]]) The quasi-posterior means look okay. The standard deviation and quantiles are not consistent because we didn\u2019t use an efficient weighting matrix above. Let\u2019s use one and repeat. \u03c3 = mean(chain[\"\u03c3\"].value) \u03b2 = mean(chain[\"\u03b2\"].value) m = zeros(J*J, N) for i in 1:N \u03be = delta(prob.s[:,i], prob.x[:,i], \u03c3, \u03bd) - prob.x[:,i]*\u03b2 m[:,i] .= vec(\u03be*prob.z[:,i]') end W = Symmetric(inv(cov(m'))) prob = RCIVLogitQB(data.x, data.z, max.(1,data.s[1:J,:])./M, N, J,\u03bd , defaultpriors,W) Error: MethodError: no method matching RCIVLogitQB(::Array{Float64,2}, ::Ar ray{Float64,2}, ::Array{Float64,2}, ::Int64, ::Int64, ::Tuple{Array{Float64 ,1},Array{Float64,1}}, ::NamedTuple{(:\u03b2, :\u03c3),Tuple{Normal{Float64},Truncate d{Normal{Float64},Continuous,Float64}}}, ::Symmetric{Float64,Array{Float64, 2}}) Closest candidates are: RCIVLogitQB(::Array{T,2}, ::Array{T,2}, ::Array{Stype,2}, ::Int64, ::Int6 4, ::Tuple{Array{Float64,1},Array{Float64,1}}, ::NamedTuple{(:\u03b2, :\u03c3),Tuple{ Normal{Float64},Truncated{Normal{Float64},Continuous,Float64}}}, !Matched:: Array{T,2}) where {T, Stype} at /home/paul/.julia/dev/Bayes101/docs/jmd/qb. jl:67 \u03b80 = (\u03b2=pm.\u03b2, \u03c3=pm.\u03c3 ) prob(\u03b80) P = TransformedLogDensity(t, prob) \u2207P = ADgradient(:ForwardDiff, P) LogDensityProblems.logdensity_and_gradient(\u2207P, x0); warmup=default_warmup_stages(stepsize_search=nothing, doubling_stages=2) results = DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, \u2207P, 1000;initialization = (q = x0, \u03f5=1e-5 ), reporter = LogProgressReport(nothing, 25, 15), warmup_stages =warmup) JLD2.@save \"jmd/qbw.jld2\" results JLD2.@load \"jmd/qbw.jld2\" results post = transform.(t,results.inference.chain) p = post[1] names = vcat([length(p[s])==1 ? String(s) : String.(s).*string.(1:length(p[s])) for s in keys(p)]...) vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)' chain = MCMCChains.Chains(reshape(vals, size(vals)..., 1), names) describe(chain[[\"\u03b2\",\"\u03c3\"]]) plot(chain[[\"\u03b2\",\"\u03c3\"]]) These posterior standard deviations and quantiles can be used for inference.","title":"Implementation"},{"location":"quasi/#about","text":"This meant to accompany these slides on Bayesian methods in IO. This document was created using Weave.jl. The code is available in on github . Chernozhukov, Victor, and Han Hong. 2003. \u201cAn {Mcmc} Approach to Classical Estimation.\u201d Journal of Econometrics 115 (2): 293\u2013346. https://doi.org/http://dx.doi.org/10.1016/S0304-4076(03)00100-3 . Chung, Yeojin, Andrew Gelman, Sophia Rabe-Hesketh, Jingchen Liu, and Vincent Dorie. 2015. \u201cWeakly Informative Prior for Point Estimation of Covariance Matrices in Hierarchical Models.\u201d Journal of Educational and Behavioral Statistics 40 (2): 136\u201357. https://doi.org/10.3102/1076998615570945 . Jiang, R., P. Manchanda, and P. E. Rossi. 2009. \u201cBayesian Analysis of Random Coefficient Logit Models Using Aggregate Data.\u201d Journal of Econometrics 149 (2): 136\u201348. http://www.sciencedirect.com/science/article/pii/S0304407608002297 .","title":"About"}]}