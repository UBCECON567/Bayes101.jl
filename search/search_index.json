{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bayes101.jl \u00b6 These notes and examples are meant to accompany the slides at https://faculty.arts.ubc.ca/pschrimpf/565/565.html MCMC sampling methods: ols.jmd Quasi-Bayesian: quasi.jmd","title":"Home"},{"location":"#bayes101jl","text":"These notes and examples are meant to accompany the slides at https://faculty.arts.ubc.ca/pschrimpf/565/565.html MCMC sampling methods: ols.jmd Quasi-Bayesian: quasi.jmd <!--` \u2013>","title":"Bayes101.jl"},{"location":"license/","text":"These notes are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License and were written by Paul Schrimpf. BibTeX citation.","title":"License"},{"location":"ols/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License using Plots, Distributions, StatsPlots, LinearAlgebra Plots.pyplot() Plots.PyPlotBackend() Linear Regression \u00b6 Let\u2019s simulate a simple linear regression, and then sample from the posterior using a few methods. The model is y_i = x_i \\beta + \\epsilon_i with $\\epsilon_i \\sim N(0, \\sigma^2)$ Simulation \u00b6 function simulateOLS(;N::Integer = Int(1e4), \u03b2 = ones(4), \u03c3 = 1) x = randn(N,length(\u03b2)) y = x*\u03b2 + randn(N)*\u03c3 return(x, y) end x, y = simulateOLS(N=Int(1e2), \u03b2=ones(2), \u03c3=1); Computing Posteriors \u00b6 To calculate the posterior distribution of $\\beta$ given the $x$ and $y$, we use a conditional version of Bayes\u2019 rule. p(\\beta|x, y, \\sigma) = \\frac{p(y|x, \\beta, \\sigma) p(\\beta|\\sigma, x) } {p(y |x,\\sigma)} To keep the calculations as simple as possible, we will treat $\\sigma$ as known for now. Since a regression model is agnostic about the distribution of $x$, it is standard to assume that p(\\beta|\\sigma, x) = p(\\beta|\\sigma) In other words, the distribution of $\\beta$ is independent of $x$. With normal errors, the log likelihood, i.e. the log conditional density of $y$ given the parameters , or, \\log(p(y|x, \\beta,\\sigma)) is: function loglike(\u03b2, \u03c3, x, y) return(-length(y)/2*log(2\u03c0) - 1/2*sum(((y .- x*\u03b2)./\u03c3).^2)) end loglike (generic function with 1 method) Exact Posterior \u00b6 If we assume that the prior for $\\beta$ is normal, \\beta|\\sigma \\sim N(0, \\tau^2 I_k) then the posterior can be computed exactly. It is: \\beta|x, y, \\sigma \\sim N\\left( (x'x/\\sigma^2 + I/\\tau^2)^{-1} x'y/\\sigma^2, (x'x/\\sigma^2 + I/\\tau^2)^{-1}\\right) function posterior(\u03b2, \u03c3, x, y, \u03c4) \u03a3 = inv(x'*x/\u03c3^2 + I/\u03c4^2) \u03bc = \u03a3*x'*y/\u03c3^2 return(pdf(MvNormal(\u03bc, \u03a3), \u03b2)) end posterior (generic function with 1 method) Metropolis Hastings \u00b6 If we didn\u2019t know the exact posterior, we could sample from it in a number of ways. Metropolis Hastings is a simple general purpose method. function mhstep(\u03b8, likelihood::Function, prior::Function, proposal::Function) pold = likelihood(\u03b8)*prior(\u03b8) \u03b8new = rand(proposal(\u03b8)) pnew = likelihood(\u03b8new)*prior(\u03b8new) \u03b1 = pnew*pdf(proposal(\u03b8new), \u03b8)/(pold *pdf(proposal(\u03b8), \u03b8new)) u = rand() return( (u < \u03b1) ? \u03b8new : \u03b8) end function mhchain(\u03b80, iterations, likelihood, prior, proposal) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b8[:,i] .= mhstep(\u03b8[:,i-1], likelihood, prior, proposal) end return(\u03b8) end mhchain (generic function with 1 method) When using Metropolis-Hastings, the proposal density plays a key role. The close the proposal is to the posterior, the faster the chain converges. One common choice of proposal distribution is the random walk\u2013draw candidate parameters from $N(\\theta, s)$. Let\u2019s try it. \u03c4 = 10.0 k = size(x,2) \u03c3 = 1.0 for s in [0.01, 0.1, 1.0, 10.0] rw\u03b2 = mhchain(zeros(k), Int(1e5), \u03b2->exp(loglike(\u03b2, \u03c3, x, y)), \u03b2->pdf(MvNormal(zeros(k), \u03c4), \u03b2), \u03b2->MvNormal(\u03b2, s*I)) lims=(0.5, 1.5) h=histogram(rw\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(rw\u03b2[1,:], rw\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Random Walk with s=$s\"), h2, plot(rw\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(rw\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) end Another possible choice of proposal density is an independent \u2014 draw candidate parameters from some fixed distribution. Gibbs Sampling \u00b6 Gibbs sampling refers to when we sample part of our parameters conditional on others, then vice versa, and repeat. Generally, the more parameters than can be sampled at once, the better the chain will work. As a simple illustration, we can sample each of our regression coefficients conditional on the others. function gibbschain(\u03b80, iterations, x, y, \u03c3, \u03c4) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b2 = \u03b8[:,i-1] @views for j in 1:length(\u03b2) e = y .- x[:,1:end .!= j]*\u03b2[1:end .!= j] v = 1/(dot(x[:,j], x[:,j])/\u03c3^2 + 1/\u03c4^2) m = v * (dot(x[:,j],e)/\u03c3^2) \u03b2[j] = rand(Normal(m,sqrt(v))) end \u03b8[:,i] = \u03b2 end return(\u03b8) end g\u03b2 = gibbschain(zeros(2), Int(1e5), x, y, \u03c3, \u03c4) lims=(0.5, 1.5) h=histogram(g\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(g\u03b2[1,:], g\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Gibbs Sampling\"), h2, plot(g\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(g\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) Gibbs sampling is a particularly good idea when the parameters can be divided into groups such that posterior of parameters in each group conditional on the others has a known form. If some parameters do not have known posterior, then another sampling strategy within the Gibbs sampler can be used (like metropolis-hastings). With judicious choice of blocks and conjugate priors, Gibbs sampling can be applied to very complex models. OpenBUGS, WinBUGS and JAGS are (non-Julia) software packages designed around Gibbs sampling. Mamba.jl is a Julia package that could be used for Gibbs sampling (as well as other sampling methods). Hamiltonian Monte Carlo \u00b6 Recent efforts in Bayesian computation have shifted away from Gibbs sampling of blocks with closed form posteriors towards more general purpose methods. Metropolis-Hastings is a general purpose algorithm in that all you need to use it is to be able to compute the likelihood (up to a constant of proportionality) and the prior. Unfortunately, Metropolis-Hastings requires a good proposal distribution to work well, and it is hard to find a good proposal distribution in many situations. Hamiltonian Monte Carlo uses information about the gradient of the posterior to generate a good proposal distribution. The physical analogy here is that the log posterior acts like a gravitational field, and we generate samples by creating random trajectories through that field. With the right choice of trajectories (i.e. not too fast & not too slow), they will tend to concentrate in areas of high posterior probability, while also exploring the space of parameters well. using Zygote, ForwardDiff function hmcstep(\u03b8, logdens, \u03f5=0.1, L=5; \u2207logdens=\u03b8->Zygote.gradient(logdens, \u03b8)[1], M = I, iM=inv(M)) U(x) = -logdens(x) dU(x) = -\u2207logdens(x) K(m) = (m'*iM*m/2)[1] dK(m) = m'*iM m = rand(MvNormal(zeros(length(\u03b8)), M),1) \u03b8new = copy(\u03b8) mnew = copy(m) for s in 1:L mnew .+= -dU(\u03b8new)[:]*\u03f5/2 \u03b8new .+= dK(mnew)[:]*\u03f5 end \u03b1 = exp(-U(\u03b8new)+U(\u03b8)-K(mnew)+K(m)) u = rand() return(u < \u03b1 ? \u03b8new : \u03b8) end function hmcchain(\u03b80, iterations, logdens, \u2207logdens=\u03b8->Zygote.gradient(logdens, \u03b8)[1] ; \u03f5=0.1, L=5, M=I) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b8[:,i] .= hmcstep(\u03b8[:,i-1], logdens, \u03f5, L, M=M) end return(\u03b8) end logp = \u03b2->(loglike(\u03b2, \u03c3, x, y) + logpdf(MvNormal(zeros(k), \u03c4), \u03b2)) \u2207logp(\u03b2) = ForwardDiff.gradient(logp, \u03b2) for \u03f5 in [1e-3, 0.1] for L in [2, 10] @show \u03f5, L @time h\u03b2 = hmcchain(zeros(k), Int(1e4),logp, \u03f5=\u03f5, L=L) lims=(0.5, 1.5) h=histogram(h\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(h\u03b2[1,:], h\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"HMC with \u03f5=$\u03f5, L=$L\"), h2, plot(h\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(h\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) end end (\u03f5, L) = (0.001, 2) 2.058724 seconds (16.52 M allocations: 824.979 MiB, 10.34% gc time) (\u03f5, L) = (0.001, 10) 7.335227 seconds (76.23 M allocations: 3.599 GiB, 10.76% gc time) (\u03f5, L) = (0.1, 2) 1.542728 seconds (15.60 M allocations: 779.647 MiB, 10.41% gc time) (\u03f5, L) = (0.1, 10) 7.259481 seconds (76.23 M allocations: 3.599 GiB, 10.37% gc time) The step size, $\\epsilon$, and the number of steps, $L$, affect how well the chain performs. If $\\epsilon$ or $L$ is too large, then many proposals will get rejected. If $\\epsilon L$ is too small, then the draws will be very close together, and it will take more iterations of the chain to explore the space. NUTS \u00b6 The No-U-Turn-Sample (NUTS) ((Hoffman and Gelman 2014 )) algorithmically chooses $L$, $\\epsilon$, and $M$. Stan is (non-Julia, but callable from Julia) software for Bayesian modelling that largely uses NUTS for sampling. DynamicHMC.jl is a Julia implementation. Let\u2019s try it out. using DynamicHMC, Random, LogDensityProblems struct LinearRegressionKnownSigma{T} y::Vector{T} x::Matrix{T} \u03c3::T \u03c4::T end function LogDensityProblems.capabilities(::Type{<:LinearRegressionKnownSigma}) LogDensityProblems.LogDensityOrder{0}() end LogDensityProblems.dimension(lr::LinearRegressionKnownSigma) = size(lr.x,2) function LogDensityProblems.logdensity(lr::LinearRegressionKnownSigma, \u03b2) k = size(x,2) return(logpdf(MvNormal(zeros(k), lr.\u03c4), \u03b2) + loglikelihood(Normal(0, \u03c3), y - x*\u03b2)) end lr = LinearRegressionKnownSigma(y,x,\u03c3,\u03c4) \u2207logposterior=ADgradient(:Zygote, lr) @time results=DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, \u2207logposterior, 500; warmup_stages= default_warmup_stages(init_steps=50, middle_steps=100, doubling_stages=2)); 9.116231 seconds (47.66 M allocations: 1.691 GiB, 5.40% gc time) dh\u03b2 = hcat(results.inference.chain...) h=histogram(dh\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(dh\u03b2[1,:], dh\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"DynamicHMC (NUTS)\"), h2, plot(dh\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(dh\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) See this example for a more canonical DynamicHMC linear regression implementation. Turing.jl \u00b6 Turing.jl is a probabilistic programming language (similar to STAN or BUGS), written in Julia. using Turing @model lreg(x, y, \u03c3, \u03c4) = begin # prior \u03b2 ~ MvNormal(zeros(size(x,2)), \u03c4) # model y ~ MvNormal(x*\u03b2,\u03c3) end lr = lreg(x,y,\u03c3,\u03c4) @time cc = Turing.sample(lr, NUTS(0.65), 1000) 2.437747 seconds (5.33 M allocations: 311.639 MiB, 5.25% gc time) t\u03b2 = cc.value[:,[\"\u03b2[1]\",\"\u03b2[2]\"],1]' t\u03b2 = hcat(results.inference.chain...) h=histogram(t\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(t\u03b2[1,:], t\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"DynamicHMC (NUTS)\"), h2, plot(t\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(t\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) About \u00b6 This meant to accompany these slides on Bayesian methods in IO. This document was created using Weave.jl ((Pastell 2017 )). The code is available in on github . Hoffman, Matthew D, and Andrew Gelman. 2014. \u201cThe No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.\u201d Journal of Machine Learning Research 15 (1): 1593\u20131623. http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf . Pastell, Matti. 2017. \u201cWeave.jl: Scientific Reports Using Julia.\u201d The Journal of Open Source Software . https://doi.org/http://dx.doi.org/10.21105/joss.00204 .","title":"MCMC sampling methods"},{"location":"ols/#linear-regression","text":"Let\u2019s simulate a simple linear regression, and then sample from the posterior using a few methods. The model is y_i = x_i \\beta + \\epsilon_i with $\\epsilon_i \\sim N(0, \\sigma^2)$","title":"Linear Regression"},{"location":"ols/#simulation","text":"function simulateOLS(;N::Integer = Int(1e4), \u03b2 = ones(4), \u03c3 = 1) x = randn(N,length(\u03b2)) y = x*\u03b2 + randn(N)*\u03c3 return(x, y) end x, y = simulateOLS(N=Int(1e2), \u03b2=ones(2), \u03c3=1);","title":"Simulation"},{"location":"ols/#computing-posteriors","text":"To calculate the posterior distribution of $\\beta$ given the $x$ and $y$, we use a conditional version of Bayes\u2019 rule. p(\\beta|x, y, \\sigma) = \\frac{p(y|x, \\beta, \\sigma) p(\\beta|\\sigma, x) } {p(y |x,\\sigma)} To keep the calculations as simple as possible, we will treat $\\sigma$ as known for now. Since a regression model is agnostic about the distribution of $x$, it is standard to assume that p(\\beta|\\sigma, x) = p(\\beta|\\sigma) In other words, the distribution of $\\beta$ is independent of $x$. With normal errors, the log likelihood, i.e. the log conditional density of $y$ given the parameters , or, \\log(p(y|x, \\beta,\\sigma)) is: function loglike(\u03b2, \u03c3, x, y) return(-length(y)/2*log(2\u03c0) - 1/2*sum(((y .- x*\u03b2)./\u03c3).^2)) end loglike (generic function with 1 method)","title":"Computing Posteriors"},{"location":"ols/#exact-posterior","text":"If we assume that the prior for $\\beta$ is normal, \\beta|\\sigma \\sim N(0, \\tau^2 I_k) then the posterior can be computed exactly. It is: \\beta|x, y, \\sigma \\sim N\\left( (x'x/\\sigma^2 + I/\\tau^2)^{-1} x'y/\\sigma^2, (x'x/\\sigma^2 + I/\\tau^2)^{-1}\\right) function posterior(\u03b2, \u03c3, x, y, \u03c4) \u03a3 = inv(x'*x/\u03c3^2 + I/\u03c4^2) \u03bc = \u03a3*x'*y/\u03c3^2 return(pdf(MvNormal(\u03bc, \u03a3), \u03b2)) end posterior (generic function with 1 method)","title":"Exact Posterior"},{"location":"ols/#metropolis-hastings","text":"If we didn\u2019t know the exact posterior, we could sample from it in a number of ways. Metropolis Hastings is a simple general purpose method. function mhstep(\u03b8, likelihood::Function, prior::Function, proposal::Function) pold = likelihood(\u03b8)*prior(\u03b8) \u03b8new = rand(proposal(\u03b8)) pnew = likelihood(\u03b8new)*prior(\u03b8new) \u03b1 = pnew*pdf(proposal(\u03b8new), \u03b8)/(pold *pdf(proposal(\u03b8), \u03b8new)) u = rand() return( (u < \u03b1) ? \u03b8new : \u03b8) end function mhchain(\u03b80, iterations, likelihood, prior, proposal) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b8[:,i] .= mhstep(\u03b8[:,i-1], likelihood, prior, proposal) end return(\u03b8) end mhchain (generic function with 1 method) When using Metropolis-Hastings, the proposal density plays a key role. The close the proposal is to the posterior, the faster the chain converges. One common choice of proposal distribution is the random walk\u2013draw candidate parameters from $N(\\theta, s)$. Let\u2019s try it. \u03c4 = 10.0 k = size(x,2) \u03c3 = 1.0 for s in [0.01, 0.1, 1.0, 10.0] rw\u03b2 = mhchain(zeros(k), Int(1e5), \u03b2->exp(loglike(\u03b2, \u03c3, x, y)), \u03b2->pdf(MvNormal(zeros(k), \u03c4), \u03b2), \u03b2->MvNormal(\u03b2, s*I)) lims=(0.5, 1.5) h=histogram(rw\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(rw\u03b2[1,:], rw\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Random Walk with s=$s\"), h2, plot(rw\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(rw\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) end Another possible choice of proposal density is an independent \u2014 draw candidate parameters from some fixed distribution.","title":"Metropolis Hastings"},{"location":"ols/#gibbs-sampling","text":"Gibbs sampling refers to when we sample part of our parameters conditional on others, then vice versa, and repeat. Generally, the more parameters than can be sampled at once, the better the chain will work. As a simple illustration, we can sample each of our regression coefficients conditional on the others. function gibbschain(\u03b80, iterations, x, y, \u03c3, \u03c4) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b2 = \u03b8[:,i-1] @views for j in 1:length(\u03b2) e = y .- x[:,1:end .!= j]*\u03b2[1:end .!= j] v = 1/(dot(x[:,j], x[:,j])/\u03c3^2 + 1/\u03c4^2) m = v * (dot(x[:,j],e)/\u03c3^2) \u03b2[j] = rand(Normal(m,sqrt(v))) end \u03b8[:,i] = \u03b2 end return(\u03b8) end g\u03b2 = gibbschain(zeros(2), Int(1e5), x, y, \u03c3, \u03c4) lims=(0.5, 1.5) h=histogram(g\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(g\u03b2[1,:], g\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"Gibbs Sampling\"), h2, plot(g\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(g\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) Gibbs sampling is a particularly good idea when the parameters can be divided into groups such that posterior of parameters in each group conditional on the others has a known form. If some parameters do not have known posterior, then another sampling strategy within the Gibbs sampler can be used (like metropolis-hastings). With judicious choice of blocks and conjugate priors, Gibbs sampling can be applied to very complex models. OpenBUGS, WinBUGS and JAGS are (non-Julia) software packages designed around Gibbs sampling. Mamba.jl is a Julia package that could be used for Gibbs sampling (as well as other sampling methods).","title":"Gibbs Sampling"},{"location":"ols/#hamiltonian-monte-carlo","text":"Recent efforts in Bayesian computation have shifted away from Gibbs sampling of blocks with closed form posteriors towards more general purpose methods. Metropolis-Hastings is a general purpose algorithm in that all you need to use it is to be able to compute the likelihood (up to a constant of proportionality) and the prior. Unfortunately, Metropolis-Hastings requires a good proposal distribution to work well, and it is hard to find a good proposal distribution in many situations. Hamiltonian Monte Carlo uses information about the gradient of the posterior to generate a good proposal distribution. The physical analogy here is that the log posterior acts like a gravitational field, and we generate samples by creating random trajectories through that field. With the right choice of trajectories (i.e. not too fast & not too slow), they will tend to concentrate in areas of high posterior probability, while also exploring the space of parameters well. using Zygote, ForwardDiff function hmcstep(\u03b8, logdens, \u03f5=0.1, L=5; \u2207logdens=\u03b8->Zygote.gradient(logdens, \u03b8)[1], M = I, iM=inv(M)) U(x) = -logdens(x) dU(x) = -\u2207logdens(x) K(m) = (m'*iM*m/2)[1] dK(m) = m'*iM m = rand(MvNormal(zeros(length(\u03b8)), M),1) \u03b8new = copy(\u03b8) mnew = copy(m) for s in 1:L mnew .+= -dU(\u03b8new)[:]*\u03f5/2 \u03b8new .+= dK(mnew)[:]*\u03f5 end \u03b1 = exp(-U(\u03b8new)+U(\u03b8)-K(mnew)+K(m)) u = rand() return(u < \u03b1 ? \u03b8new : \u03b8) end function hmcchain(\u03b80, iterations, logdens, \u2207logdens=\u03b8->Zygote.gradient(logdens, \u03b8)[1] ; \u03f5=0.1, L=5, M=I) \u03b8 = similar(\u03b80, length(\u03b80), iterations) \u03b8[:,1] .= \u03b80 for i in 2:iterations \u03b8[:,i] .= hmcstep(\u03b8[:,i-1], logdens, \u03f5, L, M=M) end return(\u03b8) end logp = \u03b2->(loglike(\u03b2, \u03c3, x, y) + logpdf(MvNormal(zeros(k), \u03c4), \u03b2)) \u2207logp(\u03b2) = ForwardDiff.gradient(logp, \u03b2) for \u03f5 in [1e-3, 0.1] for L in [2, 10] @show \u03f5, L @time h\u03b2 = hmcchain(zeros(k), Int(1e4),logp, \u03f5=\u03f5, L=L) lims=(0.5, 1.5) h=histogram(h\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(h\u03b2[1,:], h\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"HMC with \u03f5=$\u03f5, L=$L\"), h2, plot(h\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(h\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) end end (\u03f5, L) = (0.001, 2) 2.058724 seconds (16.52 M allocations: 824.979 MiB, 10.34% gc time) (\u03f5, L) = (0.001, 10) 7.335227 seconds (76.23 M allocations: 3.599 GiB, 10.76% gc time) (\u03f5, L) = (0.1, 2) 1.542728 seconds (15.60 M allocations: 779.647 MiB, 10.41% gc time) (\u03f5, L) = (0.1, 10) 7.259481 seconds (76.23 M allocations: 3.599 GiB, 10.37% gc time) The step size, $\\epsilon$, and the number of steps, $L$, affect how well the chain performs. If $\\epsilon$ or $L$ is too large, then many proposals will get rejected. If $\\epsilon L$ is too small, then the draws will be very close together, and it will take more iterations of the chain to explore the space.","title":"Hamiltonian Monte Carlo"},{"location":"ols/#nuts","text":"The No-U-Turn-Sample (NUTS) ((Hoffman and Gelman 2014 )) algorithmically chooses $L$, $\\epsilon$, and $M$. Stan is (non-Julia, but callable from Julia) software for Bayesian modelling that largely uses NUTS for sampling. DynamicHMC.jl is a Julia implementation. Let\u2019s try it out. using DynamicHMC, Random, LogDensityProblems struct LinearRegressionKnownSigma{T} y::Vector{T} x::Matrix{T} \u03c3::T \u03c4::T end function LogDensityProblems.capabilities(::Type{<:LinearRegressionKnownSigma}) LogDensityProblems.LogDensityOrder{0}() end LogDensityProblems.dimension(lr::LinearRegressionKnownSigma) = size(lr.x,2) function LogDensityProblems.logdensity(lr::LinearRegressionKnownSigma, \u03b2) k = size(x,2) return(logpdf(MvNormal(zeros(k), lr.\u03c4), \u03b2) + loglikelihood(Normal(0, \u03c3), y - x*\u03b2)) end lr = LinearRegressionKnownSigma(y,x,\u03c3,\u03c4) \u2207logposterior=ADgradient(:Zygote, lr) @time results=DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, \u2207logposterior, 500; warmup_stages= default_warmup_stages(init_steps=50, middle_steps=100, doubling_stages=2)); 9.116231 seconds (47.66 M allocations: 1.691 GiB, 5.40% gc time) dh\u03b2 = hcat(results.inference.chain...) h=histogram(dh\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(dh\u03b2[1,:], dh\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"DynamicHMC (NUTS)\"), h2, plot(dh\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(dh\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2))) See this example for a more canonical DynamicHMC linear regression implementation.","title":"NUTS"},{"location":"ols/#turingjl","text":"Turing.jl is a probabilistic programming language (similar to STAN or BUGS), written in Julia. using Turing @model lreg(x, y, \u03c3, \u03c4) = begin # prior \u03b2 ~ MvNormal(zeros(size(x,2)), \u03c4) # model y ~ MvNormal(x*\u03b2,\u03c3) end lr = lreg(x,y,\u03c3,\u03c4) @time cc = Turing.sample(lr, NUTS(0.65), 1000) 2.437747 seconds (5.33 M allocations: 311.639 MiB, 5.25% gc time) t\u03b2 = cc.value[:,[\"\u03b2[1]\",\"\u03b2[2]\"],1]' t\u03b2 = hcat(results.inference.chain...) h=histogram(t\u03b2', bins=100, labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], fillalpha=0.5, linealpha=0, normalize=:pdf, xlim=lims, legend=false) h2=histogram2d(t\u03b2[1,:], t\u03b2[2,:], xlim=lims, ylim=lims, normalize=:pdf, alpha=0.5, legend=false) h2=contour!(h2, range(lims..., length=100), range(lims..., length=100), (b1,b2)->posterior([b1,b2],\u03c3, x, y, \u03c4)) display(plot(plot!(h,title=\"DynamicHMC (NUTS)\"), h2, plot(t\u03b2[:,1:100]', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"First 100 iterations\", legend=false), plot(t\u03b2', labels=[\"\u03b2\u2081\" \"\u03b2\u2082\"], title=\"All iterations\", legend=false), layout=(2,2)))","title":"Turing.jl"},{"location":"ols/#about","text":"This meant to accompany these slides on Bayesian methods in IO. This document was created using Weave.jl ((Pastell 2017 )). The code is available in on github . Hoffman, Matthew D, and Andrew Gelman. 2014. \u201cThe No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.\u201d Journal of Machine Learning Research 15 (1): 1593\u20131623. http://www.jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf . Pastell, Matti. 2017. \u201cWeave.jl: Scientific Reports Using Julia.\u201d The Journal of Open Source Software . https://doi.org/http://dx.doi.org/10.21105/joss.00204 .","title":"About"},{"location":"quasi/","text":"This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License using Plots, Distributions Plots.pyplot() Plots.PyPlotBackend() Introduction \u00b6 Traditional bayesian methods require a fully specified statistical model, so that a likelihood can be calculated. In particular, this requires specifying the distribution of error terms. In IO, and economics more broadly, many applied models avoid placing distributional assumptions on error terms, and instead use GMM for estimation. There are two ways to convert a GMM model into something suitable for Bayesian methods. One is to just add distributional assumptions to the error terms. (Jiang, Manchanda, and Rossi 2009 ) applies this approach to a random coefficients demand model. A second approach is the Quasi-Bayesian approach of (Chernozhukov and Hong 2003 ). In this approach, you simply use the GMM objective function in place of the likelihood. We will look at it in some detail. Random Coefficients IV Logit \u00b6 As a working example, we\u2019ll use a random coefficients logit with endogeneity. We will use Turing.jl, which is a fairly convenient way to both specify the likelihood and simulate the model. using Turing, FastGaussQuadrature @model rcivlogit(x=missing, z=missing, s=missing; N=size(x,1), J=size(x,2), \u03bd=gausshermite(12), M=1000, ::type{R}=Float64) where {R <: Real} = begin \u03be = Matrix{R}(undef, J,N) # Priors \u03b2 ~ Normal(0, 20.0) \u03c3 ~ InverseGamma(2,3) \u03c1 ~ Normal(0, 20.0) \u03a9 ~ InverseWishart(J+3, I) \u03a3x ~ InverseWishart(J+3, I) if (ismissing(x)) x = Matrix{R}(undef, J,N) end if ismissing(s) s = Matrix{R}(undef, J,N) end if (ismissing(z)) z = randn(J,N) end \u03bc = similar(x[1,1]*\u03b2, J, length(\u03bd)) for i in 1:N \u03be[:,i] ~ MvNormal(0, \u03a9) x[:,i] ~ MvNormal(\u03c0*z + \u03c1*\u03be, \u03a3x) for j in 1:J for k in 1:length(\u03bd[1]) \u03bc[j,k] = x[j,i]*\u03b2 .+ x[j,i]*\u03c3*\u03bd[1][k] + \u03be[j,i] end end \u03bc .= exp(\u03bc) for k in 1:length(\u03bd[1]) \u03bc[:,k] ./ (1 + sum(\u03bc[:,k])) end s[i] ~ Multinomial(M, \u03bc*\u03bd[2]) end return(x=x, z=z, s=s, \u03be=\u03be, \u03b2=\u03b2, \u03c3=\u03c3, \u03c1=\u03c1, \u03a3x=\u03a3x, \u03a9=\u03a9) end ##rcivlogit#1145 (generic function with 4 methods) TO BE CONTINUED About \u00b6 This meant to accompany these slides on Bayesian methods in IO. This document was created using Weave.jl. The code is available in on github . Chernozhukov, Victor, and Han Hong. 2003. \u201cAn {Mcmc} Approach to Classical Estimation.\u201d Journal of Econometrics 115 (2): 293\u2013346. https://doi.org/http://dx.doi.org/10.1016/S0304-4076(03)00100-3 . Jiang, R., P. Manchanda, and P. E. Rossi. 2009. \u201cBayesian Analysis of Random Coefficient Logit Models Using Aggregate Data.\u201d Journal of Econometrics 149 (2): 136\u201348. http://www.sciencedirect.com/science/article/pii/S0304407608002297 .","title":"Quasi-Bayesian"},{"location":"quasi/#introduction","text":"Traditional bayesian methods require a fully specified statistical model, so that a likelihood can be calculated. In particular, this requires specifying the distribution of error terms. In IO, and economics more broadly, many applied models avoid placing distributional assumptions on error terms, and instead use GMM for estimation. There are two ways to convert a GMM model into something suitable for Bayesian methods. One is to just add distributional assumptions to the error terms. (Jiang, Manchanda, and Rossi 2009 ) applies this approach to a random coefficients demand model. A second approach is the Quasi-Bayesian approach of (Chernozhukov and Hong 2003 ). In this approach, you simply use the GMM objective function in place of the likelihood. We will look at it in some detail.","title":"Introduction"},{"location":"quasi/#random-coefficients-iv-logit","text":"As a working example, we\u2019ll use a random coefficients logit with endogeneity. We will use Turing.jl, which is a fairly convenient way to both specify the likelihood and simulate the model. using Turing, FastGaussQuadrature @model rcivlogit(x=missing, z=missing, s=missing; N=size(x,1), J=size(x,2), \u03bd=gausshermite(12), M=1000, ::type{R}=Float64) where {R <: Real} = begin \u03be = Matrix{R}(undef, J,N) # Priors \u03b2 ~ Normal(0, 20.0) \u03c3 ~ InverseGamma(2,3) \u03c1 ~ Normal(0, 20.0) \u03a9 ~ InverseWishart(J+3, I) \u03a3x ~ InverseWishart(J+3, I) if (ismissing(x)) x = Matrix{R}(undef, J,N) end if ismissing(s) s = Matrix{R}(undef, J,N) end if (ismissing(z)) z = randn(J,N) end \u03bc = similar(x[1,1]*\u03b2, J, length(\u03bd)) for i in 1:N \u03be[:,i] ~ MvNormal(0, \u03a9) x[:,i] ~ MvNormal(\u03c0*z + \u03c1*\u03be, \u03a3x) for j in 1:J for k in 1:length(\u03bd[1]) \u03bc[j,k] = x[j,i]*\u03b2 .+ x[j,i]*\u03c3*\u03bd[1][k] + \u03be[j,i] end end \u03bc .= exp(\u03bc) for k in 1:length(\u03bd[1]) \u03bc[:,k] ./ (1 + sum(\u03bc[:,k])) end s[i] ~ Multinomial(M, \u03bc*\u03bd[2]) end return(x=x, z=z, s=s, \u03be=\u03be, \u03b2=\u03b2, \u03c3=\u03c3, \u03c1=\u03c1, \u03a3x=\u03a3x, \u03a9=\u03a9) end ##rcivlogit#1145 (generic function with 4 methods) TO BE CONTINUED","title":"Random Coefficients IV Logit"},{"location":"quasi/#about","text":"This meant to accompany these slides on Bayesian methods in IO. This document was created using Weave.jl. The code is available in on github . Chernozhukov, Victor, and Han Hong. 2003. \u201cAn {Mcmc} Approach to Classical Estimation.\u201d Journal of Econometrics 115 (2): 293\u2013346. https://doi.org/http://dx.doi.org/10.1016/S0304-4076(03)00100-3 . Jiang, R., P. Manchanda, and P. E. Rossi. 2009. \u201cBayesian Analysis of Random Coefficient Logit Models Using Aggregate Data.\u201d Journal of Econometrics 149 (2): 136\u201348. http://www.sciencedirect.com/science/article/pii/S0304407608002297 .","title":"About"}]}