<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Paul Schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Quasi-Bayesian - Bayes101.jl</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/rainbow.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">
        <link href="../assets/extra.css" rel="stylesheet">

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="..">Bayes101.jl</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Notes <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../ols/" class="dropdown-item">MCMC sampling methods</a>
</li>
                                    
<li>
    <a href="./" class="dropdown-item active">Quasi-Bayesian</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../license/" class="dropdown-item">License</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../ols/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../license/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/ubcecon567/Bayes101.jl/edit/master/docs/quasi.md" class="nav-link"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#introduction" class="nav-link">Introduction</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#random-coefficients-iv-logit" class="nav-link">Random Coefficients IV Logit</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#fully-specified-likelihood" class="nav-link">Fully specified likelihood</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#quasi-bayesian" class="nav-link">Quasi-Bayesian</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#implementation" class="nav-link">Implementation</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#about" class="nav-link">About</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a></p>
<pre><code class="julia">using Plots, Distributions
Plots.pyplot()
</code></pre>

<pre><code>Plots.PyPlotBackend()
</code></pre>
<p>
<script type="math/tex; mode=display">
\def{\Er}{\mathrm{E}}
</script>
</p>
<h1 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h1>
<p>Traditional bayesian methods require a fully specified statistical
model, so that a likelihood can be calculated. In particular, this
requires specifying the distribution of error terms. In IO, and
economics more broadly, many applied models avoid placing distributional
assumptions on error terms, and instead use GMM for estimation.</p>
<p>There are two ways to convert a GMM model into something suitable for
Bayesian methods. One is to just add distributional assumptions to the
error terms. (Jiang, Manchanda, and Rossi <a href="#ref-jiang2009">2009</a>)
applies this approach to a random coefficients demand model.</p>
<p>A second approach is the Quasi-Bayesian approach of (Chernozhukov and
Hong <a href="#ref-chernozhukov2003">2003</a>). In this approach, you simply use
the GMM objective function in place of the likelihood. We will look at
it in some detail.</p>
<h1 id="random-coefficients-iv-logit">Random Coefficients IV Logit<a class="headerlink" href="#random-coefficients-iv-logit" title="Permanent link">&para;</a></h1>
<p>As a working example, we’ll use a random coefficients multinomial logit
with endogeneity. To keep things simple for illustrustration, we’ll
assume that there’s a single endogenous variable, $x \in \mathbb{R}^J$,
and market shares are given by <script type="math/tex; mode=display">
s^*(\beta, \sigma, \xi; x) = \int \frac{e^{x_j\beta + \xi_j + x_j\sigma \nu}} 
{1 + \sum_{\ell} e^{x_\ell\beta + \xi_\ell + x_\ell\sigma \nu}} d\Phi(\nu)/\sigma
</script>
</p>
<p>The moment condition will be <script type="math/tex; mode=display">
0 = \Er[\xi | z ] 
</script>
</p>
<h2 id="fully-specified-likelihood">Fully specified likelihood<a class="headerlink" href="#fully-specified-likelihood" title="Permanent link">&para;</a></h2>
<p>One way to estimate the model is to fully specify a parametric
likleihood. In the simulated data, we know the correct likelihood, so
the results we get will be about as good as we can possibly hope to get.
Compared to GMM, the full likelihood approach here assumes that
(conditional on the instruments, $z$), the market demand shocks are
normally distributed,</p>
<p>
<script type="math/tex; mode=display">
 \xi \sim N(0, \Omega)
</script>
</p>
<p>The endogenous variable, $x$, has a normal first stage,</p>
<p>
<script type="math/tex; mode=display">
x = z \Pi + \xi \Xi + u \;,\; u \sim N(0, \Sigma_x)
</script>
</p>
<p>and to keep the number of parameters small, we’ll impose $\Pi=\pi I_J$
and $\Xi=\rho I_J$.</p>
<p>Finally, we’ll assume the observed market shares $s_j$ come from $M$
draws from a Multinomial distribtion</p>
<p>
<script type="math/tex; mode=display">
M \mathrm{round}(s) ~ \mathrm{Multinomial}(M, s^*(\beta,\sigma,\xi)) 
</script>
</p>
<p>where $s^*()$ is the share function implied by the random coefficients
model. This approach removes the need to solve for $\xi$ as a function
of $s$ and the other parameters. However, it has the downsides of
needing to know $M$, and needing to sample $\xi$ along with the
parameters to generate the posterior.</p>
<p>We implement the above model in Turing.jl. This gives a convenient way
to both simulate the model and compute the posterior. However, it has
the downside of putting an extra layer of abstraction between us and the
core computations. This ends up costing us a bit of computation time,
and arguably making the posterior calculation harder to debug and
extend.</p>
<pre><code class="julia">using Turing, FastGaussQuadrature, LinearAlgebra, NNlib, JLD2

@model rcivlogit(x=missing, z=missing, s=missing, 
                 M=100, ν=gausshermite(12),
                 param=missing, N=10,J=1, ::Type{T}=Float64) where {T &lt;: Real} =
  begin
    if x === missing
      @assert s===missing &amp;&amp; z===missing
      x = Matrix{T}(undef, J, N)
      s = Matrix{Int64}(undef,J+1,N)
      z = randn(J,N)
    end
    J, N = size(x)
    ξ = Matrix{T}(undef, J, N)
    if !ismissing(param)
      β = pm.β
      σ = pm.σ
      Σx = pm.Σx
      π = pm.π
      ρ = pm.ρ
      Ω = pm.Ω
    else
      # Priors
      β ~ Normal(0, 20.0)
      σ ~ truncated(Normal(1.0, 10.0),0, Inf)
      ρ ~ Normal(0, 20.0)
      Σx ~ Wishart(J+2, diagm(ones(J))*1/(2*0.001))
      Ω ~ Wishart(J+2, diagm(ones(J))*1/(2*0.001))
      π ~ Normal(0.0, 20.0)
    end
    # The likelihood
    ξ .~ MvNormal(zeros(J), Symmetric(Ω))
    μ = zeros(typeof(x[1][1]*β),  J + 1 , length(ν[1]))
    for i in 1:N
      x[:,i] ~ MvNormal(π*z[:,i] + ρ*ξ[:,i], Symmetric(Σx))
      μ[1:J,:] .= x[:,i]*β + ξ[:,i] .+ x[:,i]*σ*sqrt(2)*ν[1]'
      μ = softmax(μ, dims=1)
      p = (μ*ν[2])/sqrt(Base.π)
      s[:,i] ~ Multinomial(M,p)
    end
    return(x=x, z=z, s=s, ξ=ξ)
  end

# some parameters for simulating data
J = 2
pm = (β=-1.0, σ=0.1, ρ=0.5, Σx=diagm(ones(J)), Ω=I+0.5*ones(J,J), π=1.0)
N = 20
M = 100
# simulate the data
data=rcivlogit(missing, missing ,missing, M, gausshermite(3), pm, N, J)();
</code></pre>

<p>Some remarks on the code</p>
<ul>
<li>
<p>When a Turing model is passed missing arguments, it sample them from
    the specified distributions. When the arguments are not missing,
    they’re treated as data and held fixed while calculating the
    posterior.</p>
</li>
<li>
<p>We use Gauss Hermite quadrature to integrate out the random
    coefficient. 3 integration points is not going to calculate the
    integral very accurately, but, since we use the same integration
    approach during estimation it will work out well.</p>
</li>
</ul>
<p>The Wishart prior distributions for the covariance matrices are not
entirely standard. The <strong>inverse</strong> Wishart distribution is the conjugate
prior for the covariance matrix of a Normal distribution, and is a
common choice. However, when using HMC for sampling, conjugate priors do
not matter. The modern view is that the inverse Wishart puts too much
weight on covariances with high correlation, and other priors. The
Wishart prior follows the advice of (Chung et al.
<a href="#ref-chung2015">2015</a>), and helps to avoid regions with degenerate
covariance matrices (which were causing numeric problems during the
tuning stages of NUTS with other priors, like the LKJ distribution).</p>
<h3 id="results">Results<a class="headerlink" href="#results" title="Permanent link">&para;</a></h3>
<p>We can sample from the posterior with the following code. It takes some
time to run.</p>
<pre><code class="julia">model = rcivlogit(data.x, data.z, data.s, M, gausshermite(3), missing, N,J)
chain = Turing.sample(model, NUTS(0.65), 1000, progress=true, verbose=true)
JLD2.@save &quot;jmd/turing.jld2&quot; chain model data
</code></pre>

<p>Let’s look at the posteriors. The chain also contains posteriors for all
$JN$ values of $\xi$, which are not going to be displayed.</p>
<pre><code class="julia">JLD2.@load &quot;jmd/turing.jld2&quot; chain model data
for k in keys(pm)
  display(plot(chain[k]))
end
</code></pre>

<p><img alt="" src="../figures/quasi_4_1.png" /> <img alt="" src="../figures/quasi_4_2.png" />
<img alt="" src="../figures/quasi_4_3.png" /> <img alt="" src="../figures/quasi_4_4.png" />
<img alt="" src="../figures/quasi_4_5.png" /> <img alt="" src="../figures/quasi_4_6.png" /></p>
<pre><code class="julia">display(describe(chain[[keys(pm)...]]))
</code></pre>

<pre><code>2-element Array{ChainDataFrame,1}

Summary Statistics
  parameters     mean     std  naive_se    mcse       ess   r_hat
  ──────────  ───────  ──────  ────────  ──────  ────────  ──────
    Σx[1, 1]   3.0778  1.3840    0.0619  0.0960  120.1318  1.0019
    Σx[1, 2]   1.0670  0.8911    0.0399  0.0712  158.8332  1.0025
    Σx[2, 1]   1.0670  0.8911    0.0399  0.0712  158.8332  1.0025
    Σx[2, 2]   2.1036  0.9176    0.0410  0.0595  348.2472  1.0059
     Ω[1, 1]   2.4330  1.6947    0.0758  0.1013  153.8935  1.0009
     Ω[1, 2]   0.8539  1.1533    0.0516  0.0817  161.7063  1.0019
     Ω[2, 1]   0.8539  1.1533    0.0516  0.0817  161.7063  1.0019
     Ω[2, 2]   2.7162  1.2958    0.0579  0.0726  281.2821  0.9985
           β  -0.9214  0.2821    0.0126  0.0385   62.2979  1.0289
           π   1.0659  0.2592    0.0116  0.0070  292.6790  0.9984
           ρ   0.2482  0.3257    0.0146  0.0335  100.6957  1.0181
           σ   0.2897  0.1845    0.0083  0.0100  212.8711  0.9984

Quantiles
  parameters     2.5%    25.0%    50.0%    75.0%    97.5%
  ──────────  ───────  ───────  ───────  ───────  ───────
    Σx[1, 1]   0.9943   2.0881   2.8644   3.9272   6.2623
    Σx[1, 2]  -0.2738   0.4539   0.9735   1.5556   3.2447
    Σx[2, 1]  -0.2738   0.4539   0.9735   1.5556   3.2447
    Σx[2, 2]   0.9298   1.4544   1.8973   2.5971   4.2834
     Ω[1, 1]   0.7670   1.4292   1.9901   2.8824   6.4902
     Ω[1, 2]  -0.9247   0.1637   0.7029   1.2866   3.8990
     Ω[2, 1]  -0.9247   0.1637   0.7029   1.2866   3.8990
     Ω[2, 2]   1.1296   1.8870   2.3947   3.2030   5.7431
           β  -1.5187  -1.0873  -0.9218  -0.7284  -0.3709
           π   0.4887   0.9059   1.0715   1.2454   1.5612
           ρ  -0.4314   0.0055   0.2708   0.4913   0.8023
           σ   0.0165   0.1439   0.2621   0.4246   0.6918
</code></pre>
<p>We can see that the chain mixed pretty well. The posterior of the
covariance matrices are somewhat high, but the other posteriors of the
other parameters appear centered on their true values. Poorly estimated
covariances are typical for this model (and many other models),
regardless of estimation method.</p>
<h3 id="dynamichmc-implementation">DynamicHMC Implementation<a class="headerlink" href="#dynamichmc-implementation" title="Permanent link">&para;</a></h3>
<p>DynamicHMC.jl is a useful alternative to Turing.jl. With DynamicHMC, you
must write a function to evaluate the log posterior. This is arguably
slightly more work than Turing’s modeling language (although it avoids
needing to learn Turing’s language), but it also gives you more control
over the code. This tends to result in more efficient code.</p>
<pre><code class="julia">using DynamicHMC, Random, LogDensityProblems, Parameters, TransformVariables, Distributions, MCMCChains

defaultpriors = (β=Normal(0,20),
                 σ=truncated(Normal(0, 10), 0, Inf),
                 π=Normal(0,20),
                 ρ=Normal(0,20),
                 Σx=Wishart(J+2, diagm(ones(J))*1/(2*0.001)),
                 Ω =Wishart(J+2, diagm(ones(J))*1/(2*0.001)))
macro evalpriors(priors)
  first = true
  expr = :(1+1)
  for k in keys(defaultpriors)
    if first
      expr = :(logpdf($(priors).$k, $k))
      first = false
    else
      expr = :($expr + logpdf($(priors).$k, $k))
    end
  end
  return(esc(expr))
end

# We define a struct to hold the data relevant to the model
struct RCIVLogit{T,Stype}
  x::Matrix{T}
  z::Matrix{T}
  s::Matrix{Stype}
  N::Int64
  J::Int64
  ν::typeof(gausshermite(1))
  priors::typeof(defaultpriors)
end


# A 
function (d::RCIVLogit)(θ)
  @unpack ξ, β, σ, π, ρ, sξ, uξ, sx, ux = θ
  @unpack x, z, s, N, J, ν = d
  T = typeof(d.x[1,1]*β)
  Σx =Symmetric(ux'*Diagonal(exp.(sx).^2)*ux)
  Ω = Symmetric(uξ'*Diagonal(exp.(sξ).^2)*uξ)
  # priors
  logp=@evalpriors(d.priors)  
  # loglikelihood
  logp += loglikelihood(MvNormal(zeros(J), Ω), ξ)
  logp += loglikelihood(MvNormal(zeros(J), Σx), x - π*z - ρ*ξ)
  μ = zeros(T,  J + 1 , length(ν[1]))
  for i in 1:N
    @views μ[1:J,:] .= x[:,i]*β + ξ[:,i] .+ x[:,i]*σ*sqrt(2)*ν[1]'
    μ = softmax(μ, dims=1)
    p = (μ*ν[2])/sqrt(Base.π)
    logp += logpdf(Multinomial(sum(s[:,i]), p),s[:,i])
  end  
  return(logp)
end

ν = gausshermite(3)
prob = RCIVLogit(data.x, data.z, data.s, N, J,ν , defaultpriors)
θ0 = ( ξ=data.ξ, β=pm.β, σ=pm.σ, π=pm.π, ρ=pm.ρ, 
       sξ = log.(sqrt.(diag(pm.Ω))),
       uξ=cholesky(pm.Ω ./ sqrt.(diag(pm.Ω)*diag(pm.Ω)')).U,
       sx = log.(sqrt.(diag(pm.Σx))),
       ux=cholesky(pm.Σx ./ sqrt.(diag(pm.Σx)*diag(pm.Σx)')).U
      )
prob(θ0)

# A transformation from ℝ^(# parameters) to the parameters
# see Transformations.jl
t= as((ξ=as(Array,asℝ,J,N),
       β = asℝ, σ=asℝ₊, π=asℝ, ρ=asℝ,
       sξ=as(Array,asℝ,J),
       uξ=CorrCholeskyFactor(J),
       sx=as(Array,asℝ,J),
       ux=CorrCholeskyFactor(J)) )
x0 = inverse(t, θ0)

# wrap our logposterior and transform into the struc that DynamicHMC
# uses for sampling
P = TransformedLogDensity(t, prob)
∇P = ADgradient(:ForwardDiff, P);
</code></pre>

<pre><code class="julia">warmup_stages = default_warmup_stages(;doubling_stages=2, M=Symmetric)
results = DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, ∇P, 1000;
                                      initialization = (q = x0, ),
                                      reporter = LogProgressReport(nothing, 25, 15),
                                      warmup_stages=warmup_stages)
JLD2.@save &quot;jmd/dhmc.jld2&quot; results
</code></pre>

<pre><code class="julia">JLD2.@load &quot;jmd/dhmc.jld2&quot; results
post = transform.(t,results.inference.chain)
p = post[1]
names = vcat([length(p[s])==1 ? String(s) : String.(s).*string.(1:length(p[s])) for s in keys(p)]...)
vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)'
chain = MCMCChains.Chains(reshape(vals, size(vals)..., 1), names)
describe(chain[[&quot;β&quot;,&quot;σ&quot;,&quot;ρ&quot;,&quot;π&quot;]])
plot(chain[[&quot;β&quot;,&quot;σ&quot;,&quot;ρ&quot;,&quot;π&quot;]])
</code></pre>

<p><img alt="" src="../figures/quasi_7_1.png" /></p>
<h1 id="quasi-bayesian">Quasi-Bayesian<a class="headerlink" href="#quasi-bayesian" title="Permanent link">&para;</a></h1>
<p>Consider the following GMM setup. We have a data-depenendent function
$g_i(\theta)$ such that $\Er[g_i(\theta_0)] = 0$ . Let the usual GMM
estimator be</p>
<p>
<script type="math/tex; mode=display">
\hat{\theta}^{GMM} = \argmin_\theta n(\frac{1}{n} g_i(\theta) )' W (\frac{1}{n} g_i(\theta))
</script>
</p>
<p>Assume the usual regularity conditions so that</p>
<p>
<script type="math/tex; mode=display">
    \sqrt{n}(\hat{\theta}^{GMM} - \theta_0) \to N\left(0, (D'WD)^{-1}(D'W\Omega W D)  (D'WD)^{-1} \right)
</script>
</p>
<p>where $D = D_\theta \Er[g_i(\theta_0)]$ and
$\frac{1}{\sqrt{n}} \sum  g_i(\theta_0) \to N(0, \Omega)$.</p>
<p>Quasi-Bayesian approaches are based on sampling from a quasi-posterior
that also converges to
$N\left(0, (D&rsquo;WD)^{-1}(D&rsquo;W\Omega W D)  (D&rsquo;WD)^{-1} \right)$ as
$n\to \infty$.</p>
<p>Let $\Sigma = (D&rsquo;WD)^{-1}(D&rsquo;W\Omega W D) (D&rsquo;WD)^{-1} \right)$. Note that
the log density of the asymptotic distribution is</p>
<p>
<script type="math/tex; mode=display">
  p_\infty(\theta) \propto -\frac{n}{2}(\theta - \theta_0)' \Sigma^{-1} (\theta - \theta_0)
</script>
</p>
<p>Compare that to minus the GMM objective function (which we’ll denote by
$\log(p_n(\theta)$)</p>
<p>
<script type="math/tex; mode=display">
\log(p_n(\theta)) = -n(\frac{1}{n} g_i(\theta) )' W (\frac{1}{n} g_i(\theta) )
</script>
</p>
<p>If we take a second order Taylor expansion,</p>
<p>
<script type="math/tex; mode=display">
\log(p_n(\theta)) \approx  log(p_n(\theta_0)) + -n 2(\theta - \theta_0)' D' W (\frac{1}{n} g_i(\theta_0)) + \frac{-n}{2} (\theta-\theta_0)'D W D' (\theta-\theta_0) 
</script>
</p>
<p>and complete the square</p>
<p>
<script type="math/tex; mode=display">
log(p_n(\theta)) \approx  -\frac{n}{2} ( (\theta - \theta_0) - (D'WD)^{-1} \frac{1}{n}\sum g_i(\theta_0) )' D' W D ((\theta - \theta_0) - (D'WD)^{-1} \frac{1}{n}\sum g_i(\theta_0) ) + C
</script>
</p>
<p>where $C$ is a constant that does not depend on $\theta$.</p>
<p>From this we can see that if we treat $\log(p_n(\theta))$ as a log
posterior distribution, we get that that conditional on the data, the
quasi-posterior is approximately normal with a data dependent mean and
variance $D&rsquo;WD$.</p>
<p>
<script type="math/tex; mode=display">
p_n( \sqrt{n}(\theta - \theta_0) ) \approx N\left( (D'WD)^{-1} \frac{1}{\sqrt{n}}\sum g_i(\theta_0) , D'WD \right)
</script>
</p>
<p>Finally, note that the usual asymptotic expansion of the GMM objective
gives</p>
<p>\$\$</p>
<p><code>\sqrt{n}</code>( <code>\hat{\theta}</code>\^{GMM} - <code>\theta</code>_0 ) =
(D’WD)\^{-1}
<code>\frac{1}{\sqrt{n}}</code><code>\sum</code>g_i(<code>\theta</code>_0) +
o_p(1), then</p>
<p>
<script type="math/tex; mode=display">
p_n( \sqrt{n}(\theta - \theta_0) ) \approx N\left(\sqrt{n}(\hat{\theta}^{GMM} - \theta_0), D'WD \right)
</script>
</p>
<p>That is, the quasi-posterior is approximately normal with mean equal to
the usual GMM estimate, and variance $D&rsquo;WD$.</p>
<p>If we treat the quasi-posterior and as a real posterior, the
quasi-posterior mean or median will be consistent since they each
approach $\hat{\theta}^{GMM}$ and $\hat{\theta}^{GMM}$ is consistent.</p>
<p>Additionally, if we use quasi-posterior credible intervals for
inference, they may or may not be consistent. In general, the
quasi-posterior variance, $D&rsquo;WD$, differs from the asymptotic variance,
$(D&rsquo;WD)^{-1}(D&rsquo;W\Omega W D)(D&rsquo;WD)^{-1}$, and quasi-posterior credible
intervals will not be consistent. However, if we use an efficient
weighting matrix, $W=\Omega^{-1}$, then the quasi-posterior credible
intervals will be consistent.</p>
<p>See <a href="https://ocw.mit.edu/courses/economics/14-385-nonlinear-econometric-analysis-fall-2007/lecture-notes/lecture10_11.pdf">these
notes</a>
for more information, or (Chernozhukov and Hong
<a href="#ref-chernozhukov2003">2003</a>) for complete details.</p>
<h2 id="implementation">Implementation<a class="headerlink" href="#implementation" title="Permanent link">&para;</a></h2>
<p>To compute moment conditions, we need to solve for $\xi$ given the
parameters. Here’s the associated code. It’s similar to what we used in
<a href="https://github.com/UBCECON567/BLPDemand.jl">BLPDemand.jl</a>.</p>
<pre><code class="julia">using DynamicHMC, Random, LogDensityProblems, Parameters, TransformVariables, Distributions, ForwardDiff, NLsolve

function share(δ::AbstractVector, σ, x::AbstractVector, ν::typeof(gausshermite(1)))
  J = length(δ)
  S = length(ν[1])
  s = zeros(promote_type(eltype(δ), eltype(σ)),size(δ))
  si = similar(s)
  @inbounds for i in 1:S
    si .= δ + σ*x*ν[1][i]
    # to prevent overflow from exp(δ + ...)
    simax = max(maximum(si), 0)
    si .-= simax
    si .= exp.(si)
    si .= si./(exp.(-simax) + sum(si))
    s .+= si*ν[2][i]
  end
  s .+= eps(eltype(s))
  return(s)
end

function delta(s::AbstractVector{T}, x::AbstractVector{T},
               σ::T, ν::typeof(gausshermite(1))) where T
  tol = 1e-6
  out = try
    sol=NLsolve.fixedpoint(d-&gt;(d .+ log.(s) .- log.(share(d, σ, x, ν))),
                           log.(s) .- log.(1-sum(s)),
                           method = :anderson, m=4,
                           xtol=tol, ftol=tol,
                           iterations=100, show_trace=false)
    sol.zero
  catch
    log.(s) .- log.(1-sum(s))
  end
  return(out)
end

# Use the implicit function theorem to make ForwardDiff work with delta()
function delta(s::AbstractVector, x::AbstractVector, 
               σ::D, ν::typeof(gausshermite(1))) where {D &lt;: ForwardDiff.Dual}
  σval = ForwardDiff.value.(σ)
  δ = delta(s,x,σval, ν)
  ∂δ = ForwardDiff.jacobian(d -&gt; share(d, σval, x, ν), δ)
  ∂σ = ForwardDiff.jacobian(s -&gt; share(δ, s..., x, ν), [σval])
  out = Vector{typeof(σ)}(undef, length(δ))
  Jv = try
    -∂δ \ ∂σ
  catch
    zeros(eltype(∂σ),size(∂σ))
  end
  Jc = zeros(ForwardDiff.valtype(D), length(σ), ForwardDiff.npartials(D))
  for i in eachindex(σ)
    Jc[i,:] .= ForwardDiff.partials(σ[i])
  end
  Jn = Jv * Jc
  for i in eachindex(out)
    out[i] = D(δ[i], ForwardDiff.Partials(tuple(Jn[i,:]...)))
  end
  return(out)
end
</code></pre>

<pre><code>delta (generic function with 2 methods)
</code></pre>
<p>Now the code for the quasi-posterior.</p>
<pre><code class="julia">using DynamicHMC, Random, LogDensityProblems, Parameters, TransformVariables, Distributions

struct RCIVLogitQB{T,Stype}
  x::Matrix{T}
  z::Matrix{T}
  s::Matrix{Stype}
  N::Int64
  J::Int64
  ν::typeof(gausshermite(1))
  priors::typeof(defaultpriors)
  W::Matrix{T}
end
</code></pre>

<pre><code>Error: invalid redefinition of constant RCIVLogitQB
</code></pre>
<pre><code class="julia">defaultpriors = (β=Normal(0,20),
                 σ=truncated(Normal(0, 10), 0, Inf))
macro evalpriors(priors)
  first = true
  expr = :(1+1)
  for k in keys(defaultpriors)
    if first
      expr = :(logpdf($(priors).$k, $k))
      first = false
    else
      expr = :($expr + logpdf($(priors).$k, $k))
    end
  end
  return(esc(expr))
end

function (d::RCIVLogitQB)(θ)
  @unpack β, σ = θ
  @unpack x, z, s, N, J, ν, priors, W = d
  T = typeof(x[1,1]*β)
  # priors
  logp = @evalpriors(d.priors)
  # quasi-loglikelihood
  m = zeros(T, J*J)
  for i in 1:N
    ξ = delta(s[:,i], x[:,i], σ, ν) - x[:,i]*β
    m .+= vec(ξ*z[:,i]')
  end
  m ./= N
  logp += -0.5*N*m'*W*m #logpdf(MvNormal(zeros(length(m)), W), m)
  return(logp)
end

gh = gausshermite(3)
ν = (gh[1],gh[2]/sqrt(Base.π))
prob = RCIVLogitQB(data.x, data.z, max.(1,data.s[1:J,:])./M, N, J,ν , defaultpriors, diagm(ones(J*J)))
θ0 = (β=pm.β, σ=pm.σ )
prob(θ0)
t=as((β = asℝ, σ=asℝ₊))
x0 = inverse(t, θ0)
P = TransformedLogDensity(t, prob)
∇P = ADgradient(:ForwardDiff, P)
LogDensityProblems.logdensity_and_gradient(∇P, x0);
</code></pre>

<p>The chain here actually takes very little time to run, but we still
cache the results.</p>
<pre><code class="julia">warmup=default_warmup_stages(stepsize_search=nothing, doubling_stages=2)
results = DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, ∇P, 1000;
                                      initialization = (q = x0, ϵ=1e-5 ),
                                      reporter = LogProgressReport(nothing, 25, 15),
                                      warmup_stages =warmup)
JLD2.@save &quot;jmd/qb.jld2&quot; results
</code></pre>

<pre><code class="julia">JLD2.@load &quot;jmd/qb.jld2&quot; results
post = transform.(t,results.inference.chain)
p = post[1]
names = vcat([length(p[s])==1 ? String(s) : String.(s).*string.(1:length(p[s])) for s in keys(p)]...)
vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)'
chain = MCMCChains.Chains(reshape(vals, size(vals)..., 1), names)
describe(chain[[&quot;β&quot;,&quot;σ&quot;]])
plot(chain[[&quot;β&quot;,&quot;σ&quot;]])
</code></pre>

<p><img alt="" src="../figures/quasi_11_1.png" /></p>
<p>The quasi-posterior means look okay. The standard deviation and
quantiles are not consistent because we didn’t use an efficient
weighting matrix above. Let’s use one and repeat.</p>
<pre><code class="julia">σ = mean(chain[&quot;σ&quot;].value)
β = mean(chain[&quot;β&quot;].value)
m = zeros(J*J, N)
for i in 1:N
  ξ = delta(prob.s[:,i], prob.x[:,i], σ, ν) - prob.x[:,i]*β
  m[:,i] .= vec(ξ*prob.z[:,i]')
end
W = Symmetric(inv(cov(m')))

prob = RCIVLogitQB(data.x, data.z, max.(1,data.s[1:J,:])./M, N, J,ν , defaultpriors,W)
</code></pre>

<pre><code>Error: MethodError: no method matching RCIVLogitQB(::Array{Float64,2}, ::Ar
ray{Float64,2}, ::Array{Float64,2}, ::Int64, ::Int64, ::Tuple{Array{Float64
,1},Array{Float64,1}}, ::NamedTuple{(:β, :σ),Tuple{Normal{Float64},Truncate
d{Normal{Float64},Continuous,Float64}}}, ::Symmetric{Float64,Array{Float64,
2}})
Closest candidates are:
  RCIVLogitQB(::Array{T,2}, ::Array{T,2}, ::Array{Stype,2}, ::Int64, ::Int6
4, ::Tuple{Array{Float64,1},Array{Float64,1}}, ::NamedTuple{(:β, :σ),Tuple{
Normal{Float64},Truncated{Normal{Float64},Continuous,Float64}}}, !Matched::
Array{T,2}) where {T, Stype} at /home/paul/.julia/dev/Bayes101/docs/jmd/qb.
jl:67
</code></pre>
<pre><code class="julia">θ0 = (β=pm.β, σ=pm.σ )
prob(θ0)
P = TransformedLogDensity(t, prob)
∇P = ADgradient(:ForwardDiff, P)
LogDensityProblems.logdensity_and_gradient(∇P, x0);
</code></pre>

<pre><code class="julia">warmup=default_warmup_stages(stepsize_search=nothing, doubling_stages=2)
results = DynamicHMC.mcmc_keep_warmup(Random.GLOBAL_RNG, ∇P, 1000;initialization = (q = x0, ϵ=1e-5 ), reporter = LogProgressReport(nothing, 25, 15), warmup_stages =warmup)
JLD2.@save &quot;jmd/qbw.jld2&quot; results
</code></pre>

<pre><code class="julia">JLD2.@load &quot;jmd/qbw.jld2&quot; results
post = transform.(t,results.inference.chain)
p = post[1]
names = vcat([length(p[s])==1 ? String(s) : String.(s).*string.(1:length(p[s])) for s in keys(p)]...)
vals = hcat([vcat([length(v)==1 ? v : vec(v) for v in values(p)]...) for p in post]...)'
chain = MCMCChains.Chains(reshape(vals, size(vals)..., 1), names)
describe(chain[[&quot;β&quot;,&quot;σ&quot;]])
plot(chain[[&quot;β&quot;,&quot;σ&quot;]])
</code></pre>

<p><img alt="" src="../figures/quasi_14_1.png" /></p>
<p>These posterior standard deviations and quantiles can be used for
inference.</p>
<h2 id="about">About<a class="headerlink" href="#about" title="Permanent link">&para;</a></h2>
<p>This meant to accompany <a href="https://faculty.arts.ubc.ca/pschrimpf/565/11-bayesianEstimation.pdf">these slides on Bayesian methods in
IO.</a></p>
<p>This document was created using Weave.jl. The code is available in <a href="https://github.com/UBCECON567/Bayes101.jl">on
github</a>.</p>
<div class="references hanging-indent" id="refs">
<div id="ref-chernozhukov2003">
<p>Chernozhukov, Victor, and Han Hong. 2003. “An {Mcmc} Approach to
Classical Estimation.” <em>Journal of Econometrics</em> 115 (2): 293–346.
<a href="https://doi.org/http://dx.doi.org/10.1016/S0304-4076(03)00100-3">https://doi.org/http://dx.doi.org/10.1016/S0304-4076(03)00100-3</a>.</p>
</div>
<div id="ref-chung2015">
<p>Chung, Yeojin, Andrew Gelman, Sophia Rabe-Hesketh, Jingchen Liu, and
Vincent Dorie. 2015. “Weakly Informative Prior for Point Estimation of
Covariance Matrices in Hierarchical Models.” <em>Journal of Educational and
Behavioral Statistics</em> 40 (2): 136–57.
<a href="https://doi.org/10.3102/1076998615570945">https://doi.org/10.3102/1076998615570945</a>.</p>
</div>
<div id="ref-jiang2009">
<p>Jiang, R., P. Manchanda, and P. E. Rossi. 2009. “Bayesian Analysis of
Random Coefficient Logit Models Using Aggregate Data.” <em>Journal of
Econometrics</em> 149 (2): 136–48.
<a href="http://www.sciencedirect.com/science/article/pii/S0304407608002297">http://www.sciencedirect.com/science/article/pii/S0304407608002297</a>.</p>
</div>
</div></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Paul Schrimpf</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>
        <script src="../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
