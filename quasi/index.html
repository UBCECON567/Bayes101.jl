<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Paul Schrimpf">
        
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>Quasi-Bayesian - Bayes101.jl</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/rainbow.min.css">
        <link href="../assets/Documenter.css" rel="stylesheet">
        <link href="../assets/extra.css" rel="stylesheet">

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="..">Bayes101.jl</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown active">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Notes <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../ols/" class="dropdown-item">MCMC sampling methods</a>
</li>
                                    
<li>
    <a href="./" class="dropdown-item active">Quasi-Bayesian</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../license/" class="dropdown-item">License</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../ols/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../license/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/ubcecon567/Bayes101.jl/edit/master/docs/quasi.md" class="nav-link"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#introduction" class="nav-link">Introduction</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#random-coefficients-iv-logit" class="nav-link">Random Coefficients IV Logit</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#fully-specified-likelihood" class="nav-link">Fully specified likelihood</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#quasi-bayesian" class="nav-link">Quasi-Bayesian</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#about" class="nav-link">About</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p><a href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a></p>
<p>This work is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike
4.0 International
License</a></p>
<pre><code class="julia">using Plots, Distributions
Plots.pyplot()
</code></pre>

<pre><code>Plots.PyPlotBackend()
</code></pre>
<h1 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h1>
<p>Traditional bayesian methods require a fully specified statistical
model, so that a likelihood can be calculated. In particular, this
requires specifying the distribution of error terms. In IO, and
economics more broadly, many applied models avoid placing distributional
assumptions on error terms, and instead use GMM for estimation.</p>
<p>There are two ways to convert a GMM model into something suitable for
Bayesian methods. One is to just add distributional assumptions to the
error terms. (Jiang, Manchanda, and Rossi <a href="#ref-jiang2009">2009</a>)
applies this approach to a random coefficients demand model.</p>
<p>A second approach is the Quasi-Bayesian approach of (Chernozhukov and
Hong <a href="#ref-chernozhukov2003">2003</a>). In this approach, you simply use
the GMM objective function in place of the likelihood. We will look at
it in some detail.</p>
<h1 id="random-coefficients-iv-logit">Random Coefficients IV Logit<a class="headerlink" href="#random-coefficients-iv-logit" title="Permanent link">&para;</a></h1>
<p>As a working example, we’ll use a random coefficients multinomial logit
with endogeneity. To keep things simple for illustrustration, we’ll
assume that there’s a single endogenous variable, $x \in \mathbb{R}^J$,
and market shares are given by <script type="math/tex; mode=display">
s^*(\beta, \sigma, \xi; x) = \int \frac{e^{x_j\beta + \xi_j + x_j\sigma \nu}} 
{1 + \sum_{\ell} e^{x_\ell\beta + \xi_\ell + x_\ell\sigma \nu}} d\Phi(\nu)/\sigma
</script>
</p>
<p>The moment condition will be <script type="math/tex; mode=display">
0 = \Er[\xi | z ] 
</script>
</p>
<h2 id="fully-specified-likelihood">Fully specified likelihood<a class="headerlink" href="#fully-specified-likelihood" title="Permanent link">&para;</a></h2>
<p>One way to estimate the model is to fully specify a parametric
likleihood. In the simulated data, we know the correct likelihood, so
the results we get will be about as good as we can possibly hope to get.
Compared to GMM, the full likelihood approach here assumes that
(conditional on the instruments, $z$), the market demand shocks are
normally distributed,</p>
<p>
<script type="math/tex; mode=display">
 \xi \sim N(0, \Omega)
</script>
</p>
<p>The endogenous variable, $x$, has a normal first stage,</p>
<p>
<script type="math/tex; mode=display">
 x = z \Pi + \xi \Rho + u \;,\; u \sim N(0, \Sigma_x)
</script>
</p>
<p>and to keep the number of parameters small, we’ll impose $\Pi=\pi I_J$
and $\Rho=\rho I_J$.</p>
<p>Finally, we’ll assume the observed market shares $s_j$ come from $M$
draws from a Multinomial distribtion</p>
<p>
<script type="math/tex; mode=display">
M round(s) ~ Multinomial(M, s^*(\beta,\sigma,\xi)) 
</script>
</p>
<p>where $s^*()$ is the share function implied by the random coefficients
model. This approach removes the need to solve for $\xi$ as a function
of $s$ and the other parameters. However, it has the downsides of
needing to know $M$, and needing to sample $\xi$ along with the
parameters to generate the posterior.</p>
<p>We implement the above model in Turing.jl. This gives a convenient way
to both simulate the model and compute the posterior. However, it has
the downside of putting an extra layer of abstraction between us and the
core computations. This ends up costing us a bit of computation time,
and arguably making the posterior calculation harder to debug and
extend.</p>
<pre><code class="julia">using Turing, FastGaussQuadrature, LinearAlgebra, NNlib, JLD2

@model rcivlogit(x=missing, z=missing, s=missing, 
                 M=100, ν=gausshermite(12),
                 param=missing, N=10,J=1, ::Type{T}=Float64) where {T &lt;: Real} =
  begin
    if x === missing
      @assert s===missing &amp;&amp; z===missing
      x = Matrix{T}(undef, J, N)
      s = Matrix{Int64}(undef,J+1,N)
      z = randn(J,N)
    end
    J, N = size(x)
    ξ = Matrix{T}(undef, J, N)
    if !ismissing(param)
      β = pm.β
      σ = pm.σ
      Σx = pm.Σx
      π = pm.π
      ρ = pm.ρ
      Ω = pm.Ω
    else
      # Priors
      β ~ Normal(0, 20.0)
      σ ~ truncated(Normal(1.0, 10.0),0, Inf)
      ρ ~ Normal(0, 20.0)
      Σx ~ Wishart(J+2, diagm(ones(J))*1/(2*0.001))
      Ω ~ Wishart(J+2, diagm(ones(J))*1/(2*0.001))
      π ~ Normal(0.0, 20.0)
    end
    # The likelihood
    ξ .~ MvNormal(zeros(J), Symmetric(Ω))
    μ = zeros(typeof(x[1][1]*β),  J + 1 , length(ν[1]))
    for i in 1:N
      x[:,i] ~ MvNormal(π*z[:,i] + ρ*ξ[:,i], Symmetric(Σx))
      μ[1:J,:] .= x[:,i]*β + ξ[:,i] .+ x[:,i]*σ*sqrt(2)*ν[1]'
      μ = softmax(μ, dims=1)
      p = (μ*ν[2])/sqrt(Base.π)
      s[:,i] ~ Multinomial(M,p)
    end
    return(x=x, z=z, s=s, ξ=ξ)
  end

# some parameters for simulating data
J = 2
pm = (β=-1.0, σ=0.1, ρ=0.5, Σx=diagm(ones(J)), Ω=I+0.5*ones(J,J), π=1.0)
N = 20
M = 100
# simulate the data
data=rcivlogit(missing, missing ,missing, M, gausshermite(3), pm, N, J)()
</code></pre>

<pre><code>(x = [-0.9334967930273279 0.18014970868181757 … 1.8618306445187094 -1.80017
74234337438; 0.5895096155630419 -0.5018631782301344 … -4.164261780255877 0.
7764049179614305], z = [-0.41218446321553814 -0.6529617723859734 … 0.067543
84576170645 -1.0437788973484199; 1.2808960902170943 0.07124692216634719 … -
2.7210577300305805 0.2106318587616567], s = [37 40 … 0 60; 19 20 … 99 9; 44
 40 … 1 31], ξ = [-0.9206312572867548 0.7289132517526565 … 1.65120134387043
3 -1.0004380753509403; 0.26942178162504676 -1.1017208848247213 … 0.25508479
2470061 -1.1251567395426023])
</code></pre>
<p>Some remarks on the code</p>
<ul>
<li>
<p>When a Turing model is passed missing arguments, it sample them from
    the specified distributions. When the arguments are not missing,
    they’re treated as data and held fixed while calculating the
    posterior.</p>
</li>
<li>
<p>We use Gauss Hermite quadrature to integrate out the random
    coefficient. 3 integration points is not going to calculate the
    integral very accurately, but, since we use the same integration
    approach during estimation, the inaccuracy won’t matter.</p>
</li>
</ul>
<p>The Wishart prior distributions for the covariance matrices are not
entirely standard. The <strong>inverse</strong> Wishart distribution is the conjugate
prior for the covariance matrix of a Normal distribution, and is a
common choice. However, when using HMC for sampling, conjugate priors do
not matter. The modern view is that the inverse Wishart puts too much
weight on covariances with high correlation, and other priors. The
Wishart prior follows the advice of (Chung et al.
<a href="#ref-chung2015">2015</a>), and helps to avoid regions with degenerate
covariance matrices (which were causing numeric problems during the
tuning stages of NUTS with other priors, like the LKJ distribution).</p>
<h3 id="results">Results<a class="headerlink" href="#results" title="Permanent link">&para;</a></h3>
<p>We can simulate the posterior with the following code. It takes some
time to run.</p>
<pre><code class="julia">model = rcivlogit(data.x, data.z, data.s, M, gausshermite(3), missing, N,J)
chain = Turing.sample(model, NUTS(0.65), 1000, progress=true, verbose=true)
JLD2.@save &quot;jmd/turing.jld2&quot; chain model data
</code></pre>

<p>Let’s look at the posteriors. The chain also contains posteriors for all
$JN$ values of $\xi$, which are not going to be displayed.</p>
<pre><code class="julia">JLD2.@load &quot;jmd/turing.jld2&quot; chain model data
display(describe(chain[[keys(pm)...]]))
</code></pre>

<pre><code>2-element Array{ChainDataFrame,1}

Summary Statistics
  parameters     mean     std  naive_se    mcse       ess   r_hat
  ──────────  ───────  ──────  ────────  ──────  ────────  ──────
    Σx[1, 1]   3.0778  1.3840    0.0619  0.0960  120.1318  1.0019
    Σx[1, 2]   1.0670  0.8911    0.0399  0.0712  158.8332  1.0025
    Σx[2, 1]   1.0670  0.8911    0.0399  0.0712  158.8332  1.0025
    Σx[2, 2]   2.1036  0.9176    0.0410  0.0595  348.2472  1.0059
     Ω[1, 1]   2.4330  1.6947    0.0758  0.1013  153.8935  1.0009
     Ω[1, 2]   0.8539  1.1533    0.0516  0.0817  161.7063  1.0019
     Ω[2, 1]   0.8539  1.1533    0.0516  0.0817  161.7063  1.0019
     Ω[2, 2]   2.7162  1.2958    0.0579  0.0726  281.2821  0.9985
           β  -0.9214  0.2821    0.0126  0.0385   62.2979  1.0289
           π   1.0659  0.2592    0.0116  0.0070  292.6790  0.9984
           ρ   0.2482  0.3257    0.0146  0.0335  100.6957  1.0181
           σ   0.2897  0.1845    0.0083  0.0100  212.8711  0.9984

Quantiles
  parameters     2.5%    25.0%    50.0%    75.0%    97.5%
  ──────────  ───────  ───────  ───────  ───────  ───────
    Σx[1, 1]   0.9943   2.0881   2.8644   3.9272   6.2623
    Σx[1, 2]  -0.2738   0.4539   0.9735   1.5556   3.2447
    Σx[2, 1]  -0.2738   0.4539   0.9735   1.5556   3.2447
    Σx[2, 2]   0.9298   1.4544   1.8973   2.5971   4.2834
     Ω[1, 1]   0.7670   1.4292   1.9901   2.8824   6.4902
     Ω[1, 2]  -0.9247   0.1637   0.7029   1.2866   3.8990
     Ω[2, 1]  -0.9247   0.1637   0.7029   1.2866   3.8990
     Ω[2, 2]   1.1296   1.8870   2.3947   3.2030   5.7431
           β  -1.5187  -1.0873  -0.9218  -0.7284  -0.3709
           π   0.4887   0.9059   1.0715   1.2454   1.5612
           ρ  -0.4314   0.0055   0.2708   0.4913   0.8023
           σ   0.0165   0.1439   0.2621   0.4246   0.6918
</code></pre>
<pre><code class="julia">for k in keys(pm)
  display(plot(chain[k]))
end
</code></pre>

<p><img alt="" src="../figures/quasi_4_1.png" /> <img alt="" src="../figures/quasi_4_2.png" />
<img alt="" src="../figures/quasi_4_3.png" /> <img alt="" src="../figures/quasi_4_4.png" />
<img alt="" src="../figures/quasi_4_5.png" /> <img alt="" src="../figures/quasi_4_6.png" /></p>
<h3 id="dynamichmc-implementation">DynamicHMC Implementation<a class="headerlink" href="#dynamichmc-implementation" title="Permanent link">&para;</a></h3>
<h1 id="quasi-bayesian">Quasi-Bayesian<a class="headerlink" href="#quasi-bayesian" title="Permanent link">&para;</a></h1>
<p>Consider the following GMM setup. We have a data-depenendent function
$g_i(\theta)$ such that $\Er[g_i(\theta_0)] = 0$ . Let the usual GMM
estimator be</p>
<p>
<script type="math/tex; mode=display">
\hat{\theta}_n = \argmin_\theta n(\frac{1}{n} g_i(\theta) )' W (\frac{1}{n} g_i(\theta) )
</script>
</p>
<p>Assume the usual regularity conditions so that</p>
<p>
<script type="math/tex; mode=display">
    \sqrt{n}(\hat{\theta}_n - \theta_0) \to N\left(0, (D'WD)^{-1}(D'W\Omega W D)  (D'WD)^{-1} \right)
</script>
</p>
<p>where $D = D_\theta \Er[g_i(\theta_0)]$ and
$\frac{1}{\sqrt{n}} \sum  g_i(\theta_0) \to N(0, \Omega)$.</p>
<p>Quasi-Bayesian approaches are based on sampling from a quasi-posterior
that also converges to
$N\left(0, (D&rsquo;WD)^{-1}(D&rsquo;W\Omega W D)  (D&rsquo;WD)^{-1} \right)$ as
$n\to \infty$.</p>
<p>Let $\Sigma = (D&rsquo;WD)^{-1}(D&rsquo;W\Omega W D) (D&rsquo;WD)^{-1} \right)$. Note that
the log density of the asymptotic distribution is</p>
<p>
<script type="math/tex; mode=display">
  p_\infy(\theta) \propto -\frac{n}{2}(\theta - \theta_0)' \Sigma^{-1} (\theta - \theta_0)
</script>
</p>
<p>Compare that to minus the GMM objective function</p>
<p>
<script type="math/tex; mode=display">
p_n(\theta) \propto -n ...
</script>
</p>
<p>See <a href="https://ocw.mit.edu/courses/economics/14-385-nonlinear-econometric-analysis-fall-2007/lecture-notes/lecture10_11.pdf">these
notes</a>
for more information, or (Chernozhukov and Hong
<a href="#ref-chernozhukov2003">2003</a>) for complete details.</p>
<h2 id="about">About<a class="headerlink" href="#about" title="Permanent link">&para;</a></h2>
<p>This meant to accompany <a href="https://faculty.arts.ubc.ca/pschrimpf/565/11-bayesianEstimation.pdf">these slides on Bayesian methods in
IO.</a></p>
<p>This document was created using Weave.jl. The code is available in <a href="https://github.com/UBCECON567/Bayes101.jl">on
github</a>.</p>
<div class="references hanging-indent" id="refs">
<div id="ref-chernozhukov2003">
<p>Chernozhukov, Victor, and Han Hong. 2003. “An {Mcmc} Approach to
Classical Estimation.” <em>Journal of Econometrics</em> 115 (2): 293–346.
<a href="https://doi.org/http://dx.doi.org/10.1016/S0304-4076(03)00100-3">https://doi.org/http://dx.doi.org/10.1016/S0304-4076(03)00100-3</a>.</p>
</div>
<div id="ref-chung2015">
<p>Chung, Yeojin, Andrew Gelman, Sophia Rabe-Hesketh, Jingchen Liu, and
Vincent Dorie. 2015. “Weakly Informative Prior for Point Estimation of
Covariance Matrices in Hierarchical Models.” <em>Journal of Educational and
Behavioral Statistics</em> 40 (2): 136–57.
<a href="https://doi.org/10.3102/1076998615570945">https://doi.org/10.3102/1076998615570945</a>.</p>
</div>
<div id="ref-jiang2009">
<p>Jiang, R., P. Manchanda, and P. E. Rossi. 2009. “Bayesian Analysis of
Random Coefficient Logit Models Using Aggregate Data.” <em>Journal of
Econometrics</em> 149 (2): 136–48.
<a href="http://www.sciencedirect.com/science/article/pii/S0304407608002297">http://www.sciencedirect.com/science/article/pii/S0304407608002297</a>.</p>
</div>
</div></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Paul Schrimpf</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../assets/mathjaxhelper.js" defer></script>
        <script src="../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
